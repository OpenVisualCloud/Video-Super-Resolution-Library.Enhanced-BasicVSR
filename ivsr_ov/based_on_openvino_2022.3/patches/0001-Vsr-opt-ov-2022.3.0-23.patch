From a50834912ee27f3500237a3e720a5dcfd3f61d9c Mon Sep 17 00:00:00 2001
From: Jingxuan Dong <jingxuan.dong@intel.com>
Date: Wed, 8 Mar 2023 13:36:25 +0800
Subject: [PATCH 1/7] Vsr opt ov 2022.3.0 (#23)

* add custom op extension for mo

* add lk pot patch to fix memory issue while quantization

* add vsr_opt folder

* add flow_warp cl kernel and inference cpp api

* fix invalid input index in custom.cpp

* upload build.sh

* delete pdb in quantization.py
---
 build.sh                                      |  39 +
 flow_warp_cl_kernel/flow_warp.cl              |  90 +++
 flow_warp_cl_kernel/flow_warp.xml             |  15 +
 samples/cpp/basicvsr/CMakeLists.txt           |  25 +
 samples/cpp/basicvsr/main.cpp                 | 335 +++++++++
 src/core/template_extension/CMakeLists.txt    |   1 +
 .../custom_op/CMakeLists.txt                  |  22 +
 .../custom_op/extension.cpp                   |  23 +
 .../custom_op/flow_warp.cpp                   | 675 ++++++++++++++++++
 .../template_extension/custom_op/flow_warp.h  |   6 +
 .../custom_op/flow_warp_custom_op.cpp         |  75 ++
 .../custom_op/flow_warp_custom_op.h           |  29 +
 .../intel_gpu/src/plugin/ops/custom.cpp       |   8 +-
 tools/mo/openvino/tools/mo/convert_impl.py    |   1 +
 .../front/onnx/flow_warp_custom_op_ext.py     |  15 +
 .../ops/flow_warp_custom_op.py                |  22 +
 .../mo/openvino/tools/mo/utils/cli_parser.py  |   7 +
 .../openvino/tools/pot/engines/ie_engine.py   | 130 +++-
 vsr_opt/Custom op from Pytorch to OpenVINO.md | 559 +++++++++++++++
 vsr_opt/VSR Introduction.md                   |  77 ++
 vsr_opt/configs/basicvsr_x2_cuc.py            |  18 +
 vsr_opt/flow_warp_pytorch_op/flow_warp_op.cpp | 483 +++++++++++++
 vsr_opt/flow_warp_pytorch_op/setup.py         |  10 +
 vsr_opt/mmedit/__init__.py                    |  35 +
 vsr_opt/mmedit/models/__init__.py             |  16 +
 vsr_opt/mmedit/models/backbones/__init__.py   |   7 +
 .../models/backbones/sr_backbones/__init__.py |   8 +
 .../backbones/sr_backbones/basicvsr_net.py    | 433 +++++++++++
 vsr_opt/mmedit/models/base.py                 | 106 +++
 vsr_opt/mmedit/models/builder.py              |  61 ++
 vsr_opt/mmedit/models/common/__init__.py      |  11 +
 vsr_opt/mmedit/models/common/flow_warp.py     |  66 ++
 .../mmedit/models/common/sr_backbone_utils.py |  98 +++
 vsr_opt/mmedit/models/common/upsample.py      |  52 ++
 vsr_opt/mmedit/models/losses/__init__.py      |   6 +
 .../mmedit/models/losses/pixelwise_loss.py    | 222 ++++++
 vsr_opt/mmedit/models/losses/utils.py         | 116 +++
 vsr_opt/mmedit/models/registry.py             |   9 +
 vsr_opt/mmedit/models/restorers/__init__.py   |   8 +
 .../mmedit/models/restorers/basic_restorer.py | 212 ++++++
 vsr_opt/mmedit/models/restorers/basicvsr.py   | 225 ++++++
 vsr_opt/mmedit/version.py                     |  18 +
 vsr_opt/samples/basicvsr_inference_sample.py  | 207 ++++++
 vsr_opt/samples/patch_utils.py                | 116 +++
 vsr_opt/tools/pytorch2onnx.py                 | 116 +++
 vsr_opt/tools/quantization.py                 | 482 +++++++++++++
 vsr_opt/tools/requirements.txt                |   2 +
 47 files changed, 5275 insertions(+), 22 deletions(-)
 create mode 100755 build.sh
 create mode 100755 flow_warp_cl_kernel/flow_warp.cl
 create mode 100755 flow_warp_cl_kernel/flow_warp.xml
 create mode 100755 samples/cpp/basicvsr/CMakeLists.txt
 create mode 100755 samples/cpp/basicvsr/main.cpp
 create mode 100755 src/core/template_extension/custom_op/CMakeLists.txt
 create mode 100755 src/core/template_extension/custom_op/extension.cpp
 create mode 100755 src/core/template_extension/custom_op/flow_warp.cpp
 create mode 100755 src/core/template_extension/custom_op/flow_warp.h
 create mode 100755 src/core/template_extension/custom_op/flow_warp_custom_op.cpp
 create mode 100755 src/core/template_extension/custom_op/flow_warp_custom_op.h
 create mode 100755 tools/mo/openvino/tools/mo/custom_op_mo_extension/front/onnx/flow_warp_custom_op_ext.py
 create mode 100755 tools/mo/openvino/tools/mo/custom_op_mo_extension/ops/flow_warp_custom_op.py
 create mode 100644 vsr_opt/Custom op from Pytorch to OpenVINO.md
 create mode 100644 vsr_opt/VSR Introduction.md
 create mode 100755 vsr_opt/configs/basicvsr_x2_cuc.py
 create mode 100755 vsr_opt/flow_warp_pytorch_op/flow_warp_op.cpp
 create mode 100755 vsr_opt/flow_warp_pytorch_op/setup.py
 create mode 100755 vsr_opt/mmedit/__init__.py
 create mode 100755 vsr_opt/mmedit/models/__init__.py
 create mode 100755 vsr_opt/mmedit/models/backbones/__init__.py
 create mode 100755 vsr_opt/mmedit/models/backbones/sr_backbones/__init__.py
 create mode 100755 vsr_opt/mmedit/models/backbones/sr_backbones/basicvsr_net.py
 create mode 100755 vsr_opt/mmedit/models/base.py
 create mode 100755 vsr_opt/mmedit/models/builder.py
 create mode 100755 vsr_opt/mmedit/models/common/__init__.py
 create mode 100755 vsr_opt/mmedit/models/common/flow_warp.py
 create mode 100755 vsr_opt/mmedit/models/common/sr_backbone_utils.py
 create mode 100755 vsr_opt/mmedit/models/common/upsample.py
 create mode 100755 vsr_opt/mmedit/models/losses/__init__.py
 create mode 100755 vsr_opt/mmedit/models/losses/pixelwise_loss.py
 create mode 100755 vsr_opt/mmedit/models/losses/utils.py
 create mode 100755 vsr_opt/mmedit/models/registry.py
 create mode 100755 vsr_opt/mmedit/models/restorers/__init__.py
 create mode 100755 vsr_opt/mmedit/models/restorers/basic_restorer.py
 create mode 100755 vsr_opt/mmedit/models/restorers/basicvsr.py
 create mode 100755 vsr_opt/mmedit/version.py
 create mode 100755 vsr_opt/samples/basicvsr_inference_sample.py
 create mode 100755 vsr_opt/samples/patch_utils.py
 create mode 100755 vsr_opt/tools/pytorch2onnx.py
 create mode 100755 vsr_opt/tools/quantization.py
 create mode 100644 vsr_opt/tools/requirements.txt

diff --git a/build.sh b/build.sh
new file mode 100755
index 0000000000..36b2f5643d
--- /dev/null
+++ b/build.sh
@@ -0,0 +1,39 @@
+# Copyright (C) 2022 Intel Corporation
+#!/bin/sh
+#git submodule update --init --recursive
+apt-get install python3-dev
+pip install cython
+
+mkdir build
+cd build
+
+cmake \
+-DCMAKE_INSTALL_PREFIX=${PWD}/../install \
+-DENABLE_INTEL_CPU=ON \
+-DENABLE_CLDNN=ON \
+-DENABLE_INTEL_GPU=ON \
+-DENABLE_ONEDNN_FOR_GPU=OFF \
+-DENABLE_INTEL_GNA=OFF \
+-DENABLE_INTEL_MYRIAD_COMMON=OFF \
+-DENABLE_INTEL_MYRIAD=OFF \
+-DENABLE_PYTHON=ON \
+-DENABLE_OPENCV=ON \
+-DENABLE_SAMPLES=ON \
+-DENABLE_CPPLINT=OFF \
+-DTREAT_WARNING_AS_ERROR=OFF \
+-DENABLE_TESTS=OFF \
+-DENABLE_GAPI_TESTS=OFF \
+-DENABLE_BEH_TESTS=OFF \
+-DENABLE_FUNCTIONAL_TESTS=OFF \
+-DENABLE_OV_CORE_UNIT_TESTS=OFF \
+-DENABLE_OV_CORE_BACKEND_UNIT_TESTS=OFF \
+-DENABLE_DEBUG_CAPS=ON \
+-DENABLE_GPU_DEBUG_CAPS=ON \
+-DENABLE_CPU_DEBUG_CAPS=ON \
+-DCMAKE_BUILD_TYPE=$1 \
+..
+
+make -j`nproc`
+
+make install
+
diff --git a/flow_warp_cl_kernel/flow_warp.cl b/flow_warp_cl_kernel/flow_warp.cl
new file mode 100755
index 0000000000..fb537bac0b
--- /dev/null
+++ b/flow_warp_cl_kernel/flow_warp.cl
@@ -0,0 +1,90 @@
+// Copyright (C) 2022 Intel Corporation
+//OCL custom layer implementation for flow_warp
+//Author: Renzhi.Jiang@Intel.com
+//currently only support {padding:0, Bilinear interpolation, align corners:true}
+//input0 format: bfyx, input1:bfyx, outpyt format:bfyx
+
+// #pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#pragma OPENCL EXTENSION cl_intel_printf : enable
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)   \
+    CAT(prefix, _OFFSET) +                   \
+    (x)*CAT(prefix, _PITCHES[3]) +           \
+    (y)*CAT(prefix, _PITCHES[2]) +           \
+    (f)*CAT(prefix, _PITCHES[1]) +           \
+    (b)*CAT(prefix, _PITCHES[0])
+
+__kernel void flow_warp(
+    const __global INPUT0_TYPE* input0,
+    const __global INPUT1_TYPE* input1,
+    __global OUTPUT0_TYPE* output)
+{
+    uint x = get_global_id(0);
+    uint y = get_global_id(1);
+    
+#if OUTPUT0_BATCH_NUM == 1
+    uint f = get_global_id(2);
+    uint b = 0;
+#else
+    uint f = get_global_id(2) % OUTPUT0_DIMS[1];
+    uint b = get_global_id(2) / OUTPUT0_DIMS[1];
+#endif //OUTPUT_BATCH_NUM > 1
+
+    //get flow value
+    uint ind_x = GET_DATA_INDEX(INPUT1, b , y, x, 0);
+    uint ind_y = ind_x + 1;
+
+    
+    // get input pixel
+    INPUT1_TYPE flow_x = input1[ind_x] + x;
+    INPUT1_TYPE flow_y = input1[ind_y] + y;
+
+    // get input pixel neighborhood
+    int coord_x = floor(flow_x + 1e-4);
+    int coord_y = floor(flow_y + 1e-4);
+    int coord_x_next = coord_x + 1;
+    int coord_y_next = coord_y + 1;
+
+    float4 weights, x4, y4;
+    x4[0] = flow_x - coord_x; y4[0] = coord_y_next - flow_y;   //ne
+    x4[1] = coord_x_next - flow_x; y4[1] = coord_y_next - flow_y; // nw
+    x4[2] = flow_x - coord_x; y4[2] = flow_y - coord_y; // se
+    x4[3] = coord_x_next - flow_x; y4[3] = flow_y - coord_y;   // sw
+    weights = mad(x4, y4, 0);
+
+    OUTPUT0_TYPE res = 0.0f;
+
+    //TODO: more efficency way to do this ???
+    if (coord_y >=0 && coord_y < INPUT0_DIMS[2]) {
+        if (coord_x_next >= 0 && coord_x_next < INPUT0_DIMS[3]) { //ne
+            uint index = GET_DATA_INDEX(INPUT0, b, f, coord_y, coord_x_next);
+            res += input0[index] * weights[0];
+        }
+        if (coord_x >= 0 && coord_x < INPUT0_DIMS[3]) { //nw
+            uint index = GET_DATA_INDEX(INPUT0, b, f, coord_y, coord_x);
+            res += input0[index] * weights[1];
+        }
+    }
+
+    if (coord_y_next >=0 && coord_y_next < INPUT0_DIMS[2]) {
+        if (coord_x_next >=0 && coord_x_next < INPUT0_DIMS[3]) { //se
+            uint index = GET_DATA_INDEX(INPUT0, b, f, coord_y_next, coord_x_next);
+            res += input0[index] * weights[2];
+        }
+        if (coord_x >= 0 && coord_x < INPUT0_DIMS[3]) { //sw
+            uint index = GET_DATA_INDEX(INPUT0, b, f, coord_y_next, coord_x);
+            res += input0[index] * weights[3];
+        }
+    }
+    
+    uint out_indx = GET_DATA_INDEX(OUTPUT0, b, f, y, x);
+    output[out_indx] = res;
+}
+
+
+
+
+
diff --git a/flow_warp_cl_kernel/flow_warp.xml b/flow_warp_cl_kernel/flow_warp.xml
new file mode 100755
index 0000000000..cebd445a40
--- /dev/null
+++ b/flow_warp_cl_kernel/flow_warp.xml
@@ -0,0 +1,15 @@
+<!-- Copyright (C) 2022 Intel Corporation -->
+<!-- configuration file for flow warp kernel -->
+<CustomLayer name="flow_warp" type="SimpleGPU" version="1">
+  <Kernel entry="flow_warp">
+    <Source filename="flow_warp.cl"/>
+    <!-- <Define name="neg_slope" type="float" param="negative_slope" default="0.0"/> -->
+  </Kernel>
+  <Buffers>
+    <Tensor arg-index="0" type="input" port-index="0" format="BFYX"/>
+    <Tensor arg-index="1" type="input" port-index="1" format="BFYX"/>
+    <Tensor arg-index="2" type="output" port-index="0" format="BFYX"/>
+  </Buffers>
+  <CompilerOptions options="-cl-mad-enable"/>
+  <WorkSizes global="X,Y,B*F"/>
+</CustomLayer>
diff --git a/samples/cpp/basicvsr/CMakeLists.txt b/samples/cpp/basicvsr/CMakeLists.txt
new file mode 100755
index 0000000000..306d5736cd
--- /dev/null
+++ b/samples/cpp/basicvsr/CMakeLists.txt
@@ -0,0 +1,25 @@
+# Copyright (C) 2022 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+#
+
+set (TARGET_NAME "basicvsr_sample")
+
+ie_add_sample(NAME basicvsr_sample
+              SOURCES "${CMAKE_CURRENT_SOURCE_DIR}/main.cpp"
+              DEPENDENCIES format_reader ie_samples_utils)
+
+# Find OpenCV components if exist
+find_package(OpenCV COMPONENTS core imgproc imgcodecs QUIET)
+if(NOT OpenCV_FOUND)
+    message(WARNING "OpenCV is disabled or not found, ${TARGET_NAME} will be built without OpenCV support")
+else()
+    target_link_libraries(${TARGET_NAME} PRIVATE ${OpenCV_LIBRARIES} ie_samples_utils)
+    if(UNIX AND NOT APPLE)
+        # Workaround issue that rpath-link is missing for PRIVATE dependencies
+        # Fixed in cmake 3.16.0 https://gitlab.kitware.com/cmake/cmake/issues/19556
+        target_link_libraries(${TARGET_NAME} INTERFACE "-Wl,-rpath-link,${OpenCV_INSTALL_PATH}/lib")
+    endif()
+    target_compile_definitions(${TARGET_NAME} PRIVATE USE_OPENCV)
+endif()
+
+
diff --git a/samples/cpp/basicvsr/main.cpp b/samples/cpp/basicvsr/main.cpp
new file mode 100755
index 0000000000..000c871328
--- /dev/null
+++ b/samples/cpp/basicvsr/main.cpp
@@ -0,0 +1,335 @@
+// Copyright (C) 2022 Intel Corporation
+// main.cpp included
+#include <algorithm>
+#include <chrono>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+// clang-format off
+#include "openvino/openvino.hpp"
+
+#include "samples/args_helper.hpp"
+#include "samples/common.hpp"
+#include "samples/slog.hpp"
+
+#include "gna/gna_config.hpp"
+#include "gpu/gpu_config.hpp"
+
+#include "opencv2/opencv.hpp"
+#include <opencv2/core/utility.hpp>
+#include <sys/types.h>
+#include <dirent.h>
+
+const char *keys =
+    "{h                 || Print Help Message}"
+    "{model_path        || Need. Path of openvino ir model(.xml)}"
+    "{data_path         || Need. Dataset path }"
+    "{nif               |3| Need. Number of Input Frames}"
+    "{extension         || Optional. Extension (.so or .dll) path }"
+    "{device            |CPU| Optional. CPU or GPU }"
+    "{save_predictions  |true| Optional. Whether to save the results to save_path.}"
+    "{patch_evalution   |false| Optional. Whether to crop the original frames to smaller patches for evalute}"
+    "{save_path         |./outputs| Optional.Path to save predictions}"
+    "{cldnn_config      || Path of CLDNN config. }"
+   ;
+
+
+bool checkPath(std::string path){
+    DIR *pDir;
+    if(!(pDir = opendir(path.c_str())) || path[path.size() - 1] == '/')
+        return false;
+    return true;
+}
+
+// get file names in the directory 
+int getFileNames(std::string path,std::vector<std::string>& filenames)
+{
+    DIR *pDir;
+    struct dirent* ptr;
+    if(!(pDir = opendir(path.c_str())))
+        return -1;
+    while((ptr = readdir(pDir))!=0) {
+        if (strcmp(ptr->d_name, ".") != 0 && strcmp(ptr->d_name, "..") != 0)
+            filenames.push_back(ptr->d_name);
+    }
+    sort(filenames.begin(), filenames.end());
+    closedir(pDir);
+    return 0;
+}
+
+// input:   n images with BGR(RGB) 0-255 
+// output:  NCHW Matrix with RGB(BGR) 0-1
+cv::Mat blobFromImages(std::vector<cv::Mat>& images, double scaleFactor,
+                   bool swapRB)
+{
+    for (int i = 0; i < images.size(); i++) 
+        if(images[i].depth() == CV_8U) 
+            images[i].convertTo(images[i], CV_32F, scaleFactor);
+
+    size_t nimages = images.size();
+    cv::Mat image0 = images[0];
+    int nch = image0.channels();
+    int sz[] = { (int)nimages, nch, image0.rows, image0.cols };
+    cv::Mat blob;
+    blob.create(4, sz, CV_32F);
+    cv::Mat ch[3];
+
+    for(size_t i = 0; i < nimages; i++ )
+    {
+        const cv::Mat& image = images[i];
+        CV_Assert(image.depth() == CV_32F);
+        nch = image.channels();
+        CV_Assert(nch == 3);
+        CV_Assert(image.size() == image0.size());
+
+        for( int j = 0; j < nch; j++ )
+            ch[j] = cv::Mat(image.rows, image.cols, CV_32F, blob.ptr((int)i, j));
+        if(swapRB)
+            std::swap(ch[0], ch[2]);
+        cv::split(image, ch);
+    }
+    
+    return blob;
+}
+
+void imagesFromBlob(const cv::Mat& blob_, std::vector<cv::Mat>& images_)
+{
+    // A blob is a 4 dimensional matrix in floating point precision
+    // blob_[0] = batchSize = nbOfImages
+    // blob_[1] = nbOfChannels
+    // blob_[2] = height
+    // blob_[3] = width
+    // output : BGR HWC3 image lists
+    CV_Assert(blob_.depth() == CV_8U);
+    CV_Assert(blob_.dims == 4);
+    CV_Assert(blob_.size[1] == 3);
+
+    int sz[] = {blob_.size[2], blob_.size[3]};
+    cv::Mat vectorOfChannels[3];
+    for (int n = 0; n < blob_.size[0]; ++n)
+    {
+        for (int c = 0; c < 3; ++c)
+        {
+            cv::Mat curChannel(2, sz, CV_8U, (void *)blob_.ptr<uchar>(n, c));
+            vectorOfChannels[2 - c] = curChannel;   // BGR
+        }
+        cv::Mat curImage;
+        cv::merge(vectorOfChannels, 3, curImage);
+        images_.push_back(curImage);
+    }
+}
+
+
+std::vector<std::vector<int>> calculatePatchCoordinateList(int oriH, int oriW, int cropSize[], int blockSize[]){
+    int cropHeight = cropSize[0];
+    int cropWidth = cropSize[1];
+    int interHeight = (cropHeight * blockSize[0] - oriH) / (blockSize[0] - 1);
+    int lastFillHeight = (cropHeight * blockSize[0] - oriH) % (blockSize[0] - 1);
+    int interWidth = (cropWidth * blockSize[1] - oriW) / (blockSize[1] - 1);
+    int lastFillWidth = (cropWidth * blockSize[1] - oriW) % (blockSize[1] - 1);
+    std::vector<std::vector<int> > cropCoordinateList;
+    for(int i = 0; i < blockSize[0]; i ++){
+        for(int j = 0; j < blockSize[1]; j ++){
+            int x1 = (cropHeight - interHeight) * i;
+            int y1 = (cropWidth - interWidth) * j;
+            if(i == blockSize[0] - 1) x1 -= lastFillHeight;
+            if(j == blockSize[1] - 1) y1 -= lastFillWidth;
+            cropCoordinateList.push_back({x1, y1, x1 + cropHeight, y1 + cropWidth});
+        }
+    }
+    return cropCoordinateList;
+}
+
+// get LR patch list from blob NCHW matrix according to patchCoordinateList
+// nblocks : (1, nif, 3, 360, 640)
+std::vector<cv::Mat> getCroppedPatch(cv::Mat blobMat, std::vector<std::vector<int>> patchCoordinateList, int nif, int cropSize[], int blockSize[]){
+    std::vector<cv::Mat> cropedList;
+    for(int i = 0; i < blockSize[0]; i ++){
+        for(int j = 0; j < blockSize[1]; j ++){
+            int patchIdx = i * blockSize[1] + j;
+            std::vector<int> curPatchCoord = patchCoordinateList[patchIdx]; 
+            std::vector<cv::Range> cropRange = {cv::Range::all(), cv::Range::all(), cv::Range::all(), cv::Range(curPatchCoord[0], curPatchCoord[2]), cv::Range(curPatchCoord[1], curPatchCoord[3])};
+            cv::Mat curPatch(blobMat, cropRange);
+            cv::Mat copiedPatch = curPatch.clone();
+            cropedList.push_back(copiedPatch);
+        }
+    }   
+    return cropedList;
+}
+
+
+int main(int argc, char **argv){
+    // -------- Parsing and validation of input arguments --------
+    cv::CommandLineParser parser(argc, argv, keys);
+    std::string data_path = parser.get<std::string>("data_path");
+    int nif = parser.get<int>("nif");
+    std::string help = parser.get<std::string>("h");
+    std::string model_path = parser.get<std::string>("model_path");
+    std::string extension = parser.get<std::string>("extension");
+    std::string device = parser.get<std::string>("device");
+    bool save_predictions = parser.get<bool>("save_predictions");
+    bool patch_evalution = parser.get<bool>("patch_evalution");
+    std::string save_path = parser.get<std::string>("save_path");
+    std::string cldnn_config = parser.get<std::string>("cldnn_config");
+    if (!parser.check() || help != "")
+    {
+        std::cout << "Error in commandline!" << std::endl;
+        parser.printMessage();
+        parser.printErrors();
+        return -1;
+    }
+
+    // input path check 
+    if(!checkPath(data_path) || !checkPath(save_path)){
+        std::cout << "Invalid directory path!" << std::endl;
+        std::cout << "Directory NOT found or PATH ended with '/' " << std::endl;
+        std::cout << "Please confirm your path is existed and does NOT end with '/' " << std::endl;
+        return -1;
+    }
+
+    // global variables
+    int scaleFactor = 2;
+    int cropSize[] = {360, 640};
+
+    // -------- Step 1. Initialize OpenVINO Runtime Core --------
+    ov::Core core;
+
+    // add extension: flow_warp_op
+    if(extension != "") {
+        if(device == "CPU"){
+	        std::cout << "Running on CPU! " << std::endl;
+	        core.add_extension(extension);
+            slog::info << "libcustom_extension is loaded " << extension << slog::endl;
+        }
+        else if(device == "GPU"){
+            std::cout << "Running on GPU! " << std::endl; 
+	        core.add_extension(extension);
+            core.set_property("GPU", {{CONFIG_KEY(CONFIG_FILE), cldnn_config}});
+            core.set_property("GPU", {{CONFIG_KEY(GPU_THROUGHPUT_STREAMS), 1}});
+            slog::info << "GPU extensions is loaded " << extension << slog::endl;
+            slog::info << "Cldnn_config is set " << cldnn_config << slog::endl;
+        }
+    }
+    slog::info << "OpenVINO: " << ov::get_openvino_version() << slog::endl;
+
+    // -------- Step 2. Read model --------
+    slog::info << "Loading model files: " << model_path << slog::endl;
+    std::shared_ptr<ov::Model> input_model = core.read_model(model_path);
+    printInputAndOutputsInfo(*input_model);
+    auto inputs = (*input_model).inputs();
+    auto input = inputs[0];
+    const ov::Shape modelInputShape = input.get_shape();
+    // check NIF
+    if(nif != modelInputShape[1]){
+    	std::cout << "Invalid NIF value!" << std::endl;
+	    return -1;
+    }
+
+    // --------  Loading a model to the device --------
+    ov::CompiledModel compiled_model = core.compile_model(input_model, device);
+
+    // -------- Step 3. Set up input and initialize output
+    // 3.1 Get file names
+    std::cout << "Getting file names from : " << data_path << std::endl;
+    std::vector<std::string> filePathList;
+    getFileNames(data_path, filePathList);
+    if(filePathList.size() < nif) {
+        std::cout << "Not enough frames for an inference!" << std::endl;
+        std::cout << "At least NIF frames in the data_path are needed!" << std::endl;
+        return -1;
+    }
+
+    // 3.2 Read input images and preprocess with opencv
+    std::vector<cv::Mat> inMatList;
+    for(int i = 0; i < nif; i ++){
+        std::string filePath = data_path + "/" + filePathList[i];
+        std::cout << "Reading image: " << filePath << std::endl;
+        cv::Mat img = cv::imread(filePath, cv::IMREAD_COLOR);
+        //inMatList.push_back(img);
+        //small resolution to test
+        //cv::Mat small_img = cv::Mat::ones(360, 640, 21);
+        inMatList.push_back(img);
+    } 
+    // check images' shape
+    int oriHeight = inMatList[0].rows, oriWidth = inMatList[0].cols;
+    if(oriHeight < modelInputShape[3] || oriWidth < modelInputShape[4]) {
+        std::cout << "Shape of frame does NOT match with the model's input_shape '!" << std::endl;
+        return -1;
+    }
+
+    // refer to cv::dnn::blobFromImages
+    cv::Mat inputNCHW = blobFromImages(inMatList, 1.0/255, true);
+    int sz[] = { 1, nif, 3, oriHeight, oriWidth };
+    cv::Mat blobMat(5, sz, CV_32F, (float *)inputNCHW.data);
+    int outputSize[] = {1, nif, 3, oriHeight * scaleFactor, oriWidth * scaleFactor};
+	cv::Mat outputImg = cv::Mat::zeros(5, outputSize, CV_32F);
+    // -------- Step 4. Inference --------
+    if(!patch_evalution){
+        ov::element::Type input_type = ov::element::f32;
+        ov::Shape input_shape = {1, nif, 3, oriHeight, oriWidth};
+        ov::Tensor input_tensor = ov::Tensor(input_type, input_shape, (float *) blobMat.data);
+        ov::InferRequest infer_request = compiled_model.create_infer_request();
+        infer_request.set_input_tensor(input_tensor);
+        for (int i=0; i < 1; i++) infer_request.infer();
+        const ov::Tensor& output_tensor = infer_request.get_output_tensor();
+        cv::Mat outMat(5, outputSize, CV_32F, (float *)output_tensor.data());
+        outputImg += outMat;
+    }
+    else{   // patch inference
+        // crop patches
+        int blockSize[] = {oriHeight / cropSize[0] + 1, oriWidth / cropSize[1] + 1};
+        std::vector<std::vector<int>> patchCoordinateList = calculatePatchCoordinateList(oriHeight, oriWidth, cropSize, blockSize);
+        std::vector<cv::Mat> cropedList = getCroppedPatch(blobMat, patchCoordinateList, nif, cropSize, blockSize); 
+
+        outputImg = cv::Mat::zeros(5, outputSize, CV_32F);
+        cv::Mat addCount = cv::Mat::zeros(5, outputSize, CV_32F);
+
+        ov::element::Type input_type = ov::element::f32;
+        ov::Shape input_shape = {1, nif, 3, cropSize[0], cropSize[1]};
+        cv::Mat outputPatch;
+        for(int i = 0; i < blockSize[0]; i ++){
+            for(int j = 0; j < blockSize[1]; j ++){
+                int idx = i * blockSize[1] + j;
+                std::vector<int> coordinate = patchCoordinateList[idx]; 
+                ov::Tensor input_tensor = ov::Tensor(input_type, input_shape, (float *) cropedList[idx].data);
+                ov::InferRequest infer_request = compiled_model.create_infer_request();
+                infer_request.set_input_tensor(input_tensor);
+                infer_request.infer();
+                // -------- Process output --------
+                const ov::Tensor& output_tensor = infer_request.get_output_tensor();
+                int outsz[] = { 1, nif, 3, scaleFactor*cropSize[0], scaleFactor*cropSize[1] };
+                outputPatch = cv::Mat(5, outsz, CV_32F, (float *)output_tensor.data());
+
+                std::vector<cv::Range> curRange = {cv::Range::all(), cv::Range::all(), cv::Range::all(), cv::Range(coordinate[0] * scaleFactor, coordinate[2] * scaleFactor), cv::Range(coordinate[1] * scaleFactor, coordinate[3] * scaleFactor)};
+                cv::Mat outputROI(outputImg, curRange);
+                cv::Mat onesMat = cv::Mat::ones(5, outsz, CV_32F);
+                cv::Mat addCountROI(addCount, curRange);
+                outputROI += outputPatch;     
+                addCountROI += onesMat;
+            }
+        }
+        outputImg /= addCount;
+    }
+    // post process
+    outputImg.convertTo(outputImg, CV_8U, 255.0);
+    int nchwSize[] = {nif, 3, oriHeight * scaleFactor, oriWidth * scaleFactor};
+    cv::Mat outputNCHW(4, nchwSize, CV_8U, (void *)outputImg.data);
+    std::vector<cv::Mat> outMatList;
+    imagesFromBlob(outputNCHW, outMatList);
+
+    // save outputs
+    if(save_predictions){
+        if(save_path == ""){
+            std::cout << "save_path is needed!" << std::endl;
+            return -1;
+        }
+        for(int i = 0; i < nif; i ++){
+            std::string filePath = save_path + "/" + filePathList[i];
+            std::cout << "Saving image: " << filePath << std::endl;
+            cv::imwrite(filePath, outMatList[i]);
+        } 
+    }
+
+}
diff --git a/src/core/template_extension/CMakeLists.txt b/src/core/template_extension/CMakeLists.txt
index 8b1fa2e59a..faba13987a 100644
--- a/src/core/template_extension/CMakeLists.txt
+++ b/src/core/template_extension/CMakeLists.txt
@@ -3,6 +3,7 @@
 #
 add_subdirectory(old)
 add_subdirectory(new)
+add_subdirectory(custom_op)
 
 # Enable code style check
 file(GLOB_RECURSE template_extension_src "${CMAKE_CURRENT_SOURCE_DIR}/new/*.cpp" "${CMAKE_CURRENT_SOURCE_DIR}/new/*.hpp")
diff --git a/src/core/template_extension/custom_op/CMakeLists.txt b/src/core/template_extension/custom_op/CMakeLists.txt
new file mode 100755
index 0000000000..12b3039eb4
--- /dev/null
+++ b/src/core/template_extension/custom_op/CMakeLists.txt
@@ -0,0 +1,22 @@
+# Copyright (C) 2018-2022 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+#
+
+# [cmake:extension]
+set(CMAKE_CXX_STANDARD 11)
+
+set(TARGET_NAME "custom_extension")
+
+find_package(OpenVINO)
+
+set(SRC flow_warp_custom_op.cpp extension.cpp flow_warp.cpp)
+
+add_library(${TARGET_NAME} MODULE ${SRC})
+
+target_compile_definitions(${TARGET_NAME} PRIVATE IMPLEMENT_OPENVINO_EXTENSION_API)
+target_link_libraries(${TARGET_NAME} PRIVATE openvino::runtime)
+# [cmake:extension]
+
+# Enable code style check
+# file(GLOB_RECURSE template_extension_src "${CMAKE_CURRENT_SOURCE_DIR}/*.cpp" "${CMAKE_CURRENT_SOURCE_DIR}/*.hpp")
+# add_clang_format_target(${TARGET_NAME}_clang FOR_SOURCES ${template_extension_src})
diff --git a/src/core/template_extension/custom_op/extension.cpp b/src/core/template_extension/custom_op/extension.cpp
new file mode 100755
index 0000000000..2df045825c
--- /dev/null
+++ b/src/core/template_extension/custom_op/extension.cpp
@@ -0,0 +1,23 @@
+// Copyright (C) 2018-2022 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include <openvino/core/extension.hpp>
+#include <openvino/core/op_extension.hpp>
+#include <openvino/frontend/extension.hpp>
+
+#include "flow_warp_custom_op.h"
+
+// clang-format off
+//! [ov_extension:entry_point]
+OPENVINO_CREATE_EXTENSIONS(
+    std::vector<ov::Extension::Ptr>({
+
+        // Register operation itself, required to be read from IR
+        std::make_shared<ov::OpExtension<CustomExtension::FlowWarp>>(),
+
+        // Register operaton mapping, required when converted from framework model format
+        std::make_shared<ov::frontend::OpExtension<CustomExtension::FlowWarp>>()
+    }));
+//! [ov_extension:entry_point]
+// clang-format on
diff --git a/src/core/template_extension/custom_op/flow_warp.cpp b/src/core/template_extension/custom_op/flow_warp.cpp
new file mode 100755
index 0000000000..785dfbe1ce
--- /dev/null
+++ b/src/core/template_extension/custom_op/flow_warp.cpp
@@ -0,0 +1,675 @@
+// Copyright (C) 2022 Intel Corporation
+// #include <torch/script.h>
+#include <vector>
+#include <iostream>
+#include <cmath>
+#include "flow_warp.h"
+#define MIN(a, b) (((a) < (b)) ? (a) : (b))
+#define MAX(a, b) (((a) < (b)) ? (b) : (a))
+#define CLIP_COORDINATES(in, out, clip_limit) \
+  out = MIN((clip_limit - 1), MAX(in, 0))
+
+enum GridSamplerInterpolation { Bilinear = 0, Nearest = 1, Bicubic = 2 };
+enum GridSamplerPadding { Zeros = 0, Border = 1, Reflection = 2 };
+
+template <typename scalar_t>
+static inline scalar_t grid_sampler_unnormalize(scalar_t coord, int64_t size,
+                                                bool align_corners) {
+  if (align_corners) {
+    return ((coord + 1) / 2) * (size - 1);
+  } else {
+    return ((coord + 1) * size - 1) / 2;
+  }
+}
+
+// Clips coordinates to between 0 and clip_limit - 1
+template <typename scalar_t>
+static inline scalar_t clip_coordinates(scalar_t in, int64_t clip_limit) {
+  return std::min(static_cast<scalar_t>(clip_limit - 1),
+                  std::max(in, static_cast<scalar_t>(0)));
+}
+
+// Reflects coordinates until they fall between low and high (inclusive).
+// The bounds are passed as twice their value so that half-integer values
+// can be represented as ints.
+template <typename scalar_t>
+static inline scalar_t reflect_coordinates(scalar_t in, int64_t twice_low,
+                                           int64_t twice_high) {
+  if (twice_low == twice_high) {
+    return static_cast<scalar_t>(0);
+  }
+  scalar_t min = static_cast<scalar_t>(twice_low) / 2;
+  scalar_t span = static_cast<scalar_t>(twice_high - twice_low) / 2;
+  in = std::fabs(in - min);
+  // `fmod` returns same sign as `in`, which is positive after the `fabs` above.
+  scalar_t extra = std::fmod(in, span);
+  int flips = static_cast<int>(std::floor(in / span));
+  if (flips % 2 == 0) {
+    return extra + min;
+  } else {
+    return span - extra + min;
+  }
+}
+
+template <typename scalar_t>
+static inline scalar_t compute_coordinates(scalar_t coord, int64_t size,
+                                           int64_t padding_mode,
+                                           bool align_corners) {
+  if (padding_mode == GridSamplerPadding::Border) {
+    coord = clip_coordinates(coord, size);
+  } else if (padding_mode == GridSamplerPadding::Reflection) {
+    if (align_corners) {
+      coord = reflect_coordinates(coord, 0, 2 * (size - 1));
+    } else {
+      coord = reflect_coordinates(coord, -1, 2 * size - 1);
+    }
+    coord = clip_coordinates(coord, size);
+  }
+  return coord;
+}
+
+// Computes the pixel source index value for a grid coordinate
+template <typename scalar_t>
+static inline scalar_t grid_sampler_compute_source_index(scalar_t coord,
+                                                         int64_t size,
+                                                         int64_t padding_mode,
+                                                         bool align_corners) {
+  coord = grid_sampler_unnormalize(coord, size, align_corners);
+  coord = compute_coordinates(coord, size, padding_mode, align_corners);
+  return coord;
+}
+
+static inline bool within_bounds_2d(int64_t h, int64_t w, int64_t H,
+                                    int64_t W) {
+  return h >= 0 && h < H && w >= 0 && w < W;
+}
+
+template <typename scalar_t>
+static inline scalar_t get_value_bounded(const scalar_t *data, scalar_t x,
+                                         scalar_t y, int64_t W, int64_t H,
+                                         int64_t sW, int64_t sH,
+                                         int64_t padding_mode,
+                                         bool align_corners) {
+  x = compute_coordinates(x, W, padding_mode, align_corners);
+  y = compute_coordinates(y, H, padding_mode, align_corners);
+
+  int64_t ix = static_cast<int64_t>(x);
+  int64_t iy = static_cast<int64_t>(y);
+
+  if (within_bounds_2d(iy, ix, H, W)) {
+    return data[iy * sH + ix * sW];
+  }
+  return static_cast<scalar_t>(0);
+}
+
+template <typename scalar_t>
+static inline scalar_t cubic_convolution1(scalar_t x, scalar_t A) {
+  return ((A + 2) * x - (A + 3)) * x * x + 1;
+}
+
+template <typename scalar_t>
+static inline scalar_t cubic_convolution2(scalar_t x, scalar_t A) {
+  return ((A * x - 5 * A) * x + 8 * A) * x - 4 * A;
+}
+
+template <typename scalar_t>
+static inline void get_cubic_upsample_coefficients(scalar_t coeffs[4],
+                                                   scalar_t t) {
+  scalar_t A = -0.75;
+
+  scalar_t x1 = t;
+  coeffs[0] = cubic_convolution2<scalar_t>(x1 + 1.0, A);
+  coeffs[1] = cubic_convolution1<scalar_t>(x1, A);
+
+  // opposite coefficients
+  scalar_t x2 = 1.0 - t;
+  coeffs[2] = cubic_convolution1<scalar_t>(x2, A);
+  coeffs[3] = cubic_convolution2<scalar_t>(x2 + 1.0, A);
+}
+
+template <typename scalar_t>
+static inline scalar_t cubic_interp1d(scalar_t x0, scalar_t x1, scalar_t x2,
+                                      scalar_t x3, scalar_t t) {
+  scalar_t coeffs[4];
+  get_cubic_upsample_coefficients<scalar_t>(coeffs, t);
+
+  return x0 * coeffs[0] + x1 * coeffs[1] + x2 * coeffs[2] + x3 * coeffs[3];
+}
+
+#include<cassert>
+void flow_warp(const float*input_data,std::vector<size_t> input_dim,float*flow_data,std::vector<size_t> flow_dim,float*out_ptr){
+    int64_t N = input_dim[0];
+    int64_t C = input_dim[1];
+    int64_t inp_H =input_dim[2];
+    int64_t inp_W = input_dim[3];
+    int64_t out_H =flow_dim[1];
+    int64_t out_W = flow_dim[2];
+
+      // flow warp prehead
+    if(inp_H != out_H || inp_W != out_W){
+        std::cout << "The spatial sizes of input ("<<inp_H<<","<<inp_W<<")"<<" and flow ("<<out_H<<","<<out_W<<") are not the same."<<std::endl;
+        return ;
+    }
+
+    int64_t flow_sN = flow_dim[1] * flow_dim[2] * flow_dim[3];//flow_dims[1] * flow_dims[2] * flow_dims[3];
+    int64_t flow_sH = flow_dim[2] * flow_dim[3];//flow_dims[2] * flow_dims[3];
+    int64_t flow_sW = flow_dim[3]; //flow_dims[3];
+    int64_t flow_sCoor = 1;
+    
+    for (int64_t n = 0; n < N; ++n) {
+        float *flow_ptr_N = flow_data + n * flow_sN;
+        for (int64_t h = 0; h < out_H; ++h) {
+        for (int64_t w = 0; w < out_W; ++w) {
+            float *flow_ptr_NHW = flow_ptr_N + h * flow_sH + w * flow_sW;
+            float * flow_ptr_NHW_x = flow_ptr_NHW;
+            float * flow_ptr_NHW_y = &flow_ptr_NHW[flow_sCoor]; // flow_ptr_NHW + flow_sCoor;
+            *flow_ptr_NHW_x = 2.0 * (*flow_ptr_NHW_x + w) / MAX(out_W - 1, 1) - 1.0;
+            *flow_ptr_NHW_y = 2.0 *(*flow_ptr_NHW_y + h) / MAX(out_H - 1, 1) - 1.0;
+        }
+        }
+    }
+    const float * grid_data = flow_data;
+
+    // bilinear grid sample
+    const bool align_corners = true;
+    const int64_t padding_mode = 0;  // padding_mode: zeros
+    const int64_t interpolation_mode = 0; // interpolation_mode: bilinear
+    
+    int64_t grid_H = flow_dim[1],grid_W = flow_dim[2],grid_C = flow_dim[3];
+    int64_t out_C = C;
+    int64_t inp_sN = C * inp_H * inp_W;//input_dims[1] * input_dims[2] * input_dims[3];
+    int64_t inp_sC = inp_H * inp_W;//input_dims[2] * input_dims[3];
+    int64_t inp_sH = inp_W;//input_dims[3];
+    int64_t inp_sW = 1;
+    int64_t grid_sN = grid_H * grid_W * grid_C;//grid_dims[1] * grid_dims[2] * grid_dims[3];
+    int64_t grid_sH = grid_W * grid_C;//grid_dims[2] * grid_dims[3];
+    int64_t grid_sW = grid_C;//grid_dims[3];
+    int64_t grid_sCoor = 1;
+    int64_t out_sN = out_C * out_H * out_W;//output_dims[1] * output_dims[2] * output_dims[3];
+    int64_t out_sC = out_H * out_W;//output_dims[2] * output_dims[3];
+    int64_t out_sH = out_W;//output_dims[3];
+    int64_t out_sW = 1;
+
+        // loop over each output pixel
+    for (int64_t n = 0; n < N; ++n) {
+        const float *grid_ptr_N = grid_data + n * grid_sN;
+        const float *inp_ptr_N = input_data + n * inp_sN;
+        for (int64_t h = 0; h < out_H; ++h) {
+            for (int64_t w = 0; w < out_W; ++w) {
+                const float *grid_ptr_NHW = grid_ptr_N + h * grid_sH + w * grid_sW;
+                float x = *grid_ptr_NHW;
+                float y = grid_ptr_NHW[grid_sCoor];
+
+                float ix = grid_sampler_compute_source_index(x, inp_W, padding_mode,
+                                                            align_corners);
+                float iy = grid_sampler_compute_source_index(y, inp_H, padding_mode,
+                                                            align_corners);
+
+                if (interpolation_mode == GridSamplerInterpolation::Bilinear) {
+                    // get corner pixel values from (x, y)
+                    // for 4d, we use north-east-south-west
+                    int64_t ix_nw = static_cast<int64_t>(std::floor(ix + 1e-4));
+                    int64_t iy_nw = static_cast<int64_t>(std::floor(iy + 1e-4));
+
+                    int64_t ix_ne = ix_nw + 1;
+                    int64_t iy_ne = iy_nw;
+
+                    int64_t ix_sw = ix_nw;
+                    int64_t iy_sw = iy_nw + 1;
+
+                    int64_t ix_se = ix_nw + 1;
+                    int64_t iy_se = iy_nw + 1;
+
+                    // get surfaces to each neighbor:
+                    float nw = (ix_se - ix) * (iy_se - iy);
+                    float ne = (ix - ix_sw) * (iy_sw - iy);
+                    float sw = (ix_ne - ix) * (iy - iy_ne);
+                    float se = (ix - ix_nw) * (iy - iy_nw);
+
+		    // calculate bilinear weighted pixel value and set output pixel
+                    const float *inp_ptr_NC = inp_ptr_N;
+                    float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+                    for (int64_t c = 0; c < C; ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+                        auto res = static_cast<float>(0);
+                        if (within_bounds_2d(iy_nw, ix_nw, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_nw * inp_sH + ix_nw * inp_sW] * nw;
+                        }
+                        if (within_bounds_2d(iy_ne, ix_ne, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_ne * inp_sH + ix_ne * inp_sW] * ne;
+                        }
+                        if (within_bounds_2d(iy_sw, ix_sw, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_sw * inp_sH + ix_sw * inp_sW] * sw;
+                        }
+                        if (within_bounds_2d(iy_se, ix_se, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_se * inp_sH + ix_se * inp_sW] * se;
+                        }
+                        *out_ptr_NCHW = res;
+                    }
+                    } 
+                    else if (interpolation_mode == GridSamplerInterpolation::Nearest) {
+                        int64_t ix_nearest = static_cast<int64_t>(std::nearbyint(ix));
+                        int64_t iy_nearest = static_cast<int64_t>(std::nearbyint(iy));
+
+                        // assign nearest neighbor pixel value to output pixel
+                        float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+                        const float *inp_ptr_NC = inp_ptr_N;
+                        for (int64_t c = 0; c < C;
+                            ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+                            if (within_bounds_2d(iy_nearest, ix_nearest, inp_H, inp_W)) {
+                            *out_ptr_NCHW =
+                                inp_ptr_NC[iy_nearest * inp_sH + ix_nearest * inp_sW];
+                            } else {
+                            *out_ptr_NCHW = static_cast<float>(0);
+                            }
+                        }
+                        }
+                    else if (interpolation_mode == GridSamplerInterpolation::Bicubic) {
+                        // grid_sampler_compute_source_index will "clip the value" of idx
+                        // depends on the padding,
+                        // which would cause calculation to be wrong,
+                        // for example x = -0.1 -> ix = 0 for zero padding, but in bicubic ix
+                        // = floor(x) = -1
+                        // There would be more problem in reflection padding, since the -1 and
+                        // +1 direction is not fixed in boundary condition
+                        ix = grid_sampler_unnormalize(x, inp_W, align_corners);
+                        iy = grid_sampler_unnormalize(y, inp_H, align_corners);
+
+                        float ix_nw = std::floor(ix);
+                        float iy_nw = std::floor(iy);
+
+                        const float tx = ix - ix_nw;
+                        const float ty = iy - iy_nw;
+
+                        const float *inp_ptr_NC = inp_ptr_N;
+                        float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+                        for (int64_t c = 0; c < C;
+                            ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+                            float coefficients[4];
+
+                            // Interpolate 4 values in the x direction
+                            for (int64_t i = 0; i < 4; ++i) {
+                            coefficients[i] = cubic_interp1d<float>(
+                                get_value_bounded<float>(inp_ptr_NC, ix_nw - 1, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                get_value_bounded<float>(inp_ptr_NC, ix_nw + 0, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                get_value_bounded<float>(inp_ptr_NC, ix_nw + 1, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                get_value_bounded<float>(inp_ptr_NC, ix_nw + 2, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                tx);
+                            }
+
+                            // Interpolate in the y direction
+                            *out_ptr_NCHW =
+                                cubic_interp1d<float>(coefficients[0], coefficients[1],
+                                                    coefficients[2], coefficients[3], ty);
+                    }
+                }
+            }
+        }
+    }
+}
+
+
+void flow_warp_int8(const int8_t*input_data,std::vector<size_t> input_dim,int8_t*flow_data,std::vector<size_t> flow_dim,int8_t*out_ptr){
+    int8_t N = input_dim[0];
+    int8_t C = input_dim[1];
+    int8_t inp_H =input_dim[2];
+    int8_t inp_W = input_dim[3];
+    int8_t out_H =flow_dim[1];
+    int8_t out_W = flow_dim[2];
+
+
+      // flow warp prehead
+    if(inp_H != out_H || inp_W != out_W){
+        std::cout << "The spatial sizes of input ("<<inp_H<<","<<inp_W<<")"<<" and flow ("<<out_H<<","<<out_W<<") are not the same."<<std::endl;
+        return ;
+    }
+
+    int8_t flow_sN = flow_dim[1] * flow_dim[2] * flow_dim[3];//flow_dims[1] * flow_dims[2] * flow_dims[3];
+    int8_t flow_sH = flow_dim[2] * flow_dim[3];//flow_dims[2] * flow_dims[3];
+    int8_t flow_sW = flow_dim[3]; //flow_dims[3];
+    int8_t flow_sCoor = 1;
+    
+    for (int8_t n = 0; n < N; ++n) {
+        int8_t *flow_ptr_N = flow_data + n * flow_sN;
+        for (int8_t h = 0; h < out_H; ++h) {
+        for (int8_t w = 0; w < out_W; ++w) {
+            int8_t *flow_ptr_NHW = flow_ptr_N + h * flow_sH + w * flow_sW;
+            int8_t * flow_ptr_NHW_x = flow_ptr_NHW;
+            int8_t * flow_ptr_NHW_y = &flow_ptr_NHW[flow_sCoor]; // flow_ptr_NHW + flow_sCoor;
+            *flow_ptr_NHW_x = 2.0 * (*flow_ptr_NHW_x + w) / MAX(out_W - 1, 1) - 1.0;
+            *flow_ptr_NHW_y = 2.0 *(*flow_ptr_NHW_y + h) / MAX(out_H - 1, 1) - 1.0;
+        }
+        }
+    }
+    const int8_t * grid_data = flow_data;
+
+    // bilinear grid sample
+    const bool align_corners = true;
+    const int8_t padding_mode = 0;  // padding_mode: zeros
+    const int8_t interpolation_mode = 0; // interpolation_mode: bilinear
+    
+    int8_t grid_H = flow_dim[1],grid_W = flow_dim[2],grid_C = flow_dim[3];
+    int8_t out_C = C;
+    int8_t inp_sN = C * inp_H * inp_W;//input_dims[1] * input_dims[2] * input_dims[3];
+    int8_t inp_sC = inp_H * inp_W;//input_dims[2] * input_dims[3];
+    int8_t inp_sH = inp_W;//input_dims[3];
+    int8_t inp_sW = 1;
+    int8_t grid_sN = grid_H * grid_W * grid_C;//grid_dims[1] * grid_dims[2] * grid_dims[3];
+    int8_t grid_sH = grid_W * grid_C;//grid_dims[2] * grid_dims[3];
+    int8_t grid_sW = grid_C;//grid_dims[3];
+    int8_t grid_sCoor = 1;
+    int8_t out_sN = out_C * out_H * out_W;//output_dims[1] * output_dims[2] * output_dims[3];
+    int8_t out_sC = out_H * out_W;//output_dims[2] * output_dims[3];
+    int8_t out_sH = out_W;//output_dims[3];
+    int8_t out_sW = 1;
+
+    // loop over each output pixel
+    for (int8_t n = 0; n < N; ++n) {
+        const int8_t *grid_ptr_N = grid_data + n * grid_sN;
+        const int8_t *inp_ptr_N = input_data + n * inp_sN;
+        for (int8_t h = 0; h < out_H; ++h) {
+            for (int8_t w = 0; w < out_W; ++w) {
+                const int8_t *grid_ptr_NHW = grid_ptr_N + h * grid_sH + w * grid_sW;
+                int8_t x = *grid_ptr_NHW;
+                int8_t y = grid_ptr_NHW[grid_sCoor];
+
+                int8_t ix = grid_sampler_compute_source_index(x, inp_W, padding_mode,
+                                                            align_corners);
+                int8_t iy = grid_sampler_compute_source_index(y, inp_H, padding_mode,
+                                                            align_corners);
+
+                if (interpolation_mode == GridSamplerInterpolation::Bilinear) {
+                    // get corner pixel values from (x, y)
+                    // for 4d, we use north-east-south-west
+                    int8_t ix_nw = static_cast<int64_t>(std::floor(ix));
+                    int8_t iy_nw = static_cast<int64_t>(std::floor(iy));
+
+                    int8_t ix_ne = ix_nw + 1;
+                    int8_t iy_ne = iy_nw;
+
+                    int8_t ix_sw = ix_nw;
+                    int8_t iy_sw = iy_nw + 1;
+
+                    int8_t ix_se = ix_nw + 1;
+                    int8_t iy_se = iy_nw + 1;
+
+                    // get surfaces to each neighbor:
+                    int8_t nw = (ix_se - ix) * (iy_se - iy);
+                    int8_t ne = (ix - ix_sw) * (iy_sw - iy);
+                    int8_t sw = (ix_ne - ix) * (iy - iy_ne);
+                    int8_t se = (ix - ix_nw) * (iy - iy_nw);
+
+                    // calculate bilinear weighted pixel value and set output pixel
+                    const int8_t *inp_ptr_NC = inp_ptr_N;
+                    int8_t *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+                    for (int8_t c = 0; c < C; ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+                        auto res = static_cast<int8_t>(0);
+                        if (within_bounds_2d(iy_nw, ix_nw, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_nw * inp_sH + ix_nw * inp_sW] * nw;
+                        }
+                        if (within_bounds_2d(iy_ne, ix_ne, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_ne * inp_sH + ix_ne * inp_sW] * ne;
+                        }
+                        if (within_bounds_2d(iy_sw, ix_sw, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_sw * inp_sH + ix_sw * inp_sW] * sw;
+                        }
+                        if (within_bounds_2d(iy_se, ix_se, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_se * inp_sH + ix_se * inp_sW] * se;
+                        }
+                        *out_ptr_NCHW = res;
+                    }
+                    } 
+                    else if (interpolation_mode == GridSamplerInterpolation::Nearest) {
+                        int8_t ix_nearest = static_cast<int8_t>(std::nearbyint(ix));
+                        int8_t iy_nearest = static_cast<int8_t>(std::nearbyint(iy));
+
+                        // assign nearest neighbor pixel value to output pixel
+                        int8_t *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+                        const int8_t *inp_ptr_NC = inp_ptr_N;
+                        for (int8_t c = 0; c < C;
+                            ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+                            if (within_bounds_2d(iy_nearest, ix_nearest, inp_H, inp_W)) {
+                            *out_ptr_NCHW =
+                                inp_ptr_NC[iy_nearest * inp_sH + ix_nearest * inp_sW];
+                            } else {
+                            *out_ptr_NCHW = static_cast<int8_t>(0);
+                            }
+                        }
+                        }
+                    else if (interpolation_mode == GridSamplerInterpolation::Bicubic) {
+                        // grid_sampler_compute_source_index will "clip the value" of idx
+                        // depends on the padding,
+                        // which would cause calculation to be wrong,
+                        // for example x = -0.1 -> ix = 0 for zero padding, but in bicubic ix
+                        // = floor(x) = -1
+                        // There would be more problem in reflection padding, since the -1 and
+                        // +1 direction is not fixed in boundary condition
+                        ix = grid_sampler_unnormalize(x, inp_W, align_corners);
+                        iy = grid_sampler_unnormalize(y, inp_H, align_corners);
+
+                        int8_t ix_nw = std::floor(ix);
+                        int8_t iy_nw = std::floor(iy);
+
+                        const int8_t tx = ix - ix_nw;
+                        const int8_t ty = iy - iy_nw;
+
+                        const int8_t *inp_ptr_NC = inp_ptr_N;
+                        int8_t *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+                        for (int8_t c = 0; c < C;
+                            ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+                            int8_t coefficients[4];
+
+                            // Interpolate 4 values in the x direction
+                            for (int8_t i = 0; i < 4; ++i) {
+                            coefficients[i] = cubic_interp1d<int8_t>(
+                                get_value_bounded<int8_t>(inp_ptr_NC, ix_nw - 1, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                get_value_bounded<int8_t>(inp_ptr_NC, ix_nw + 0, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                get_value_bounded<int8_t>(inp_ptr_NC, ix_nw + 1, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                get_value_bounded<int8_t>(inp_ptr_NC, ix_nw + 2, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                tx);
+                            }
+
+                            // Interpolate in the y direction
+                            *out_ptr_NCHW =
+                                cubic_interp1d<int8_t>(coefficients[0], coefficients[1],
+                                                    coefficients[2], coefficients[3], ty);
+                    }
+                }
+            }
+        }
+    }
+}
+
+void flow_warp_uint8(const uint8_t*input_data,std::vector<size_t> input_dim,uint8_t*flow_data,std::vector<size_t> flow_dim,uint8_t*out_ptr){
+    uint8_t N = input_dim[0];
+    uint8_t C = input_dim[1];
+    uint8_t inp_H =input_dim[2];
+    uint8_t inp_W = input_dim[3];
+    uint8_t out_H =flow_dim[1];
+    uint8_t out_W = flow_dim[2];
+
+      // flow warp prehead
+    if(inp_H != out_H || inp_W != out_W){
+        std::cout << "The spatial sizes of input ("<<inp_H<<","<<inp_W<<")"<<" and flow ("<<out_H<<","<<out_W<<") are not the same."<<std::endl;
+        return ;
+    }
+
+    uint8_t flow_sN = flow_dim[1] * flow_dim[2] * flow_dim[3];//flow_dims[1] * flow_dims[2] * flow_dims[3];
+    uint8_t flow_sH = flow_dim[2] * flow_dim[3];//flow_dims[2] * flow_dims[3];
+    uint8_t flow_sW = flow_dim[3]; //flow_dims[3];
+    uint8_t flow_sCoor = 1;
+    
+    for (uint8_t n = 0; n < N; ++n) {
+        uint8_t *flow_ptr_N = flow_data + n * flow_sN;
+        for (uint8_t h = 0; h < out_H; ++h) {
+        for (uint8_t w = 0; w < out_W; ++w) {
+            uint8_t *flow_ptr_NHW = flow_ptr_N + h * flow_sH + w * flow_sW;
+            uint8_t * flow_ptr_NHW_x = flow_ptr_NHW;
+            uint8_t * flow_ptr_NHW_y = &flow_ptr_NHW[flow_sCoor]; // flow_ptr_NHW + flow_sCoor;
+            *flow_ptr_NHW_x = 2.0 * (*flow_ptr_NHW_x + w) / MAX(out_W - 1, 1) - 1.0;
+            *flow_ptr_NHW_y = 2.0 *(*flow_ptr_NHW_y + h) / MAX(out_H - 1, 1) - 1.0;
+        }
+        }
+    }
+    const uint8_t * grid_data = flow_data;
+
+    // bilinear grid sample
+    const bool align_corners = true;
+    const uint8_t padding_mode = 0;  // padding_mode: zeros
+    const uint8_t interpolation_mode = 0; // interpolation_mode: bilinear
+    
+    uint8_t grid_H = flow_dim[1],grid_W = flow_dim[2],grid_C = flow_dim[3];
+    uint8_t out_C = C;
+    uint8_t inp_sN = C * inp_H * inp_W;//input_dims[1] * input_dims[2] * input_dims[3];
+    uint8_t inp_sC = inp_H * inp_W;//input_dims[2] * input_dims[3];
+    uint8_t inp_sH = inp_W;//input_dims[3];
+    uint8_t inp_sW = 1;
+    uint8_t grid_sN = grid_H * grid_W * grid_C;//grid_dims[1] * grid_dims[2] * grid_dims[3];
+    uint8_t grid_sH = grid_W * grid_C;//grid_dims[2] * grid_dims[3];
+    uint8_t grid_sW = grid_C;//grid_dims[3];
+    uint8_t grid_sCoor = 1;
+    uint8_t out_sN = out_C * out_H * out_W;//output_dims[1] * output_dims[2] * output_dims[3];
+    uint8_t out_sC = out_H * out_W;//output_dims[2] * output_dims[3];
+    uint8_t out_sH = out_W;//output_dims[3];
+    uint8_t out_sW = 1;
+
+    // loop over each output pixel
+    for (uint8_t n = 0; n < N; ++n) {
+        const uint8_t *grid_ptr_N = grid_data + n * grid_sN;
+        const uint8_t *inp_ptr_N = input_data + n * inp_sN;
+        for (uint8_t h = 0; h < out_H; ++h) {
+            for (uint8_t w = 0; w < out_W; ++w) {
+                const uint8_t *grid_ptr_NHW = grid_ptr_N + h * grid_sH + w * grid_sW;
+                uint8_t x = *grid_ptr_NHW;
+                uint8_t y = grid_ptr_NHW[grid_sCoor];
+
+                uint8_t ix = grid_sampler_compute_source_index(x, inp_W, padding_mode,
+                                                            align_corners);
+                uint8_t iy = grid_sampler_compute_source_index(y, inp_H, padding_mode,
+                                                            align_corners);
+
+                if (interpolation_mode == GridSamplerInterpolation::Bilinear) {
+                    // get corner pixel values from (x, y)
+                    // for 4d, we use north-east-south-west
+                    uint8_t ix_nw = static_cast<uint8_t>(std::floor(ix));
+                    uint8_t iy_nw = static_cast<uint8_t>(std::floor(iy));
+
+                    uint8_t ix_ne = ix_nw + 1;
+                    uint8_t iy_ne = iy_nw;
+
+                    uint8_t ix_sw = ix_nw;
+                    uint8_t iy_sw = iy_nw + 1;
+
+                    uint8_t ix_se = ix_nw + 1;
+                    uint8_t iy_se = iy_nw + 1;
+
+                    // get surfaces to each neighbor:
+                    uint8_t nw = (ix_se - ix) * (iy_se - iy);
+                    uint8_t ne = (ix - ix_sw) * (iy_sw - iy);
+                    uint8_t sw = (ix_ne - ix) * (iy - iy_ne);
+                    uint8_t se = (ix - ix_nw) * (iy - iy_nw);
+
+                    // calculate bilinear weighted pixel value and set output pixel
+                    const uint8_t *inp_ptr_NC = inp_ptr_N;
+                    uint8_t *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+                    for (uint8_t c = 0; c < C; ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+                        auto res = static_cast<uint8_t>(0);
+                        if (within_bounds_2d(iy_nw, ix_nw, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_nw * inp_sH + ix_nw * inp_sW] * nw;
+                        }
+                        if (within_bounds_2d(iy_ne, ix_ne, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_ne * inp_sH + ix_ne * inp_sW] * ne;
+                        }
+                        if (within_bounds_2d(iy_sw, ix_sw, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_sw * inp_sH + ix_sw * inp_sW] * sw;
+                        }
+                        if (within_bounds_2d(iy_se, ix_se, inp_H, inp_W)) {
+                            res += inp_ptr_NC[iy_se * inp_sH + ix_se * inp_sW] * se;
+                        }
+                        *out_ptr_NCHW = res;
+                    }
+                    } 
+                    else if (interpolation_mode == GridSamplerInterpolation::Nearest) {
+                        uint8_t ix_nearest = static_cast<int64_t>(std::nearbyint(ix));
+                        uint8_t iy_nearest = static_cast<int64_t>(std::nearbyint(iy));
+
+                        // assign nearest neighbor pixel value to output pixel
+                        uint8_t *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+                        const uint8_t *inp_ptr_NC = inp_ptr_N;
+                        for (uint8_t c = 0; c < C;
+                            ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+                            if (within_bounds_2d(iy_nearest, ix_nearest, inp_H, inp_W)) {
+                            *out_ptr_NCHW =
+                                inp_ptr_NC[iy_nearest * inp_sH + ix_nearest * inp_sW];
+                            } else {
+                            *out_ptr_NCHW = static_cast<uint8_t>(0);
+                            }
+                        }
+                        }
+                    else if (interpolation_mode == GridSamplerInterpolation::Bicubic) {
+                        // grid_sampler_compute_source_index will "clip the value" of idx
+                        // depends on the padding,
+                        // which would cause calculation to be wrong,
+                        // for example x = -0.1 -> ix = 0 for zero padding, but in bicubic ix
+                        // = floor(x) = -1
+                        // There would be more problem in reflection padding, since the -1 and
+                        // +1 direction is not fixed in boundary condition
+                        ix = grid_sampler_unnormalize(x, inp_W, align_corners);
+                        iy = grid_sampler_unnormalize(y, inp_H, align_corners);
+
+                        uint8_t ix_nw = std::floor(ix);
+                        uint8_t iy_nw = std::floor(iy);
+
+                        const uint8_t tx = ix - ix_nw;
+                        const uint8_t ty = iy - iy_nw;
+
+                        const uint8_t *inp_ptr_NC = inp_ptr_N;
+                        uint8_t *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+                        for (uint8_t c = 0; c < C;
+                            ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+                            uint8_t coefficients[4];
+
+                            // Interpolate 4 values in the x direction
+                            for (uint8_t i = 0; i < 4; ++i) {
+                            coefficients[i] = cubic_interp1d<uint8_t>(
+                                get_value_bounded<uint8_t>(inp_ptr_NC, ix_nw - 1, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                get_value_bounded<uint8_t>(inp_ptr_NC, ix_nw + 0, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                get_value_bounded<uint8_t>(inp_ptr_NC, ix_nw + 1, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                get_value_bounded<uint8_t>(inp_ptr_NC, ix_nw + 2, iy_nw - 1 + i,
+                                                        inp_W, inp_H, inp_sW, inp_sH,
+                                                        padding_mode, align_corners),
+                                tx);
+                            }
+
+                            // Interpolate in the y direction
+                            *out_ptr_NCHW =
+                                cubic_interp1d<uint8_t>(coefficients[0], coefficients[1],
+                                                    coefficients[2], coefficients[3], ty);
+                    }
+                }
+            }
+        }
+    }
+}
+
diff --git a/src/core/template_extension/custom_op/flow_warp.h b/src/core/template_extension/custom_op/flow_warp.h
new file mode 100755
index 0000000000..c784f54b7f
--- /dev/null
+++ b/src/core/template_extension/custom_op/flow_warp.h
@@ -0,0 +1,6 @@
+// Copyright (C) 2022 Intel Corporation
+extern "C"{
+    void flow_warp(const float*input_data,std::vector<size_t> input_dim,float*flow_data,std::vector<size_t> flow_dim,float*output_ptr);
+    void flow_warp_int8(const int8_t*input_data,std::vector<size_t> input_dim,int8_t*flow_data,std::vector<size_t> flow_dim,int8_t*out_ptr);
+    void flow_warp_uint8(const uint8_t*input_data,std::vector<size_t> input_dim,uint8_t*flow_data,std::vector<size_t> flow_dim,uint8_t*out_ptr);
+}
\ No newline at end of file
diff --git a/src/core/template_extension/custom_op/flow_warp_custom_op.cpp b/src/core/template_extension/custom_op/flow_warp_custom_op.cpp
new file mode 100755
index 0000000000..59b15d4d8d
--- /dev/null
+++ b/src/core/template_extension/custom_op/flow_warp_custom_op.cpp
@@ -0,0 +1,75 @@
+// Copyright (C) 2018-2022 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include "flow_warp_custom_op.h"
+#include "flow_warp.h"
+using namespace CustomExtension;
+
+//! [op:ctor]
+FlowWarp::FlowWarp(const ov::Output<ov::Node>& input,const ov::Output<ov::Node>& flow) : Op({input,flow}) {
+    constructor_validate_and_infer_types();
+}
+//! [op:ctor]
+
+//! [op:validate]
+void FlowWarp::validate_and_infer_types() {
+    // Operation doesn't change shapes end element type
+    set_output_type(0, get_input_element_type(0), get_input_partial_shape(0));
+}
+//! [op:validate]
+
+//! [op:copy]
+std::shared_ptr<ov::Node> FlowWarp::clone_with_new_inputs(const ov::OutputVector& new_args) const {
+    OPENVINO_ASSERT(new_args.size() == 2, "Incorrect number of new arguments");
+
+    return std::make_shared<FlowWarp>(new_args.at(0),new_args.at(1));
+}
+//! [op:copy]
+
+//! [op:visit_attributes]
+bool FlowWarp::visit_attributes(ov::AttributeVisitor& visitor) {
+    return true;
+}
+//! [op:visit_attributes]
+
+//! [op:evaluate]
+bool FlowWarp::evaluate(ov::TensorVector& outputs, const ov::TensorVector& inputs) const {
+    auto in = inputs[0];
+    auto flow = inputs[1];
+    auto out = outputs[0];
+    ov::Tensor flow_new(flow);
+    ngraph::element::Type_t input_ty = get_input_element_type(0);
+    ngraph::element::Type_t flow_ty = get_input_element_type(1);
+    const float* input_ptr = in.data<float>();
+    float* flow_ptr = flow_new.data<float>();
+    float* output_ptr = out.data<float>();
+    switch (input_ty){
+        case ngraph::element::Type_t::i8:
+            flow_warp_int8(reinterpret_cast<const int8_t*>(input_ptr),in.get_shape(),reinterpret_cast<int8_t*>(flow_ptr),flow.get_shape(),reinterpret_cast<int8_t*>(output_ptr));
+            break;
+        case ngraph::element::Type_t::u8:
+            flow_warp_uint8(reinterpret_cast<const uint8_t*>(input_ptr),in.get_shape(),reinterpret_cast<uint8_t*>(flow_ptr),flow.get_shape(),reinterpret_cast<uint8_t*>(output_ptr));
+            break;
+        case ngraph::element::Type_t::f32:
+            flow_warp(input_ptr,in.get_shape(),flow_ptr,flow.get_shape(),output_ptr);
+            break;
+        default:
+            break;
+    }
+    return true;
+
+}
+
+bool FlowWarp::has_evaluate() const {
+    switch (get_input_element_type(0)) {
+    case ngraph::element::Type_t::i8:
+    case ngraph::element::Type_t::u8:
+    case ngraph::element::Type_t::f32:
+        return true;
+    default:
+        break;
+    }
+    return false;
+}
+//! [op:evaluate]
diff --git a/src/core/template_extension/custom_op/flow_warp_custom_op.h b/src/core/template_extension/custom_op/flow_warp_custom_op.h
new file mode 100755
index 0000000000..279cb7335c
--- /dev/null
+++ b/src/core/template_extension/custom_op/flow_warp_custom_op.h
@@ -0,0 +1,29 @@
+// Copyright (C) 2018-2022 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#pragma once
+
+//! [op:common_include]
+#include <openvino/op/op.hpp>
+//! [op:common_include]
+
+//! [op:header]
+namespace CustomExtension {
+
+class FlowWarp : public ov::op::Op {
+public:
+    OPENVINO_OP("flow_warp");
+
+    FlowWarp() = default;
+    FlowWarp(const ov::Output<ov::Node>& input,const ov::Output<ov::Node>& flow);
+    void validate_and_infer_types() override;
+    std::shared_ptr<ov::Node> clone_with_new_inputs(const ov::OutputVector& new_args) const override;
+    bool visit_attributes(ov::AttributeVisitor& visitor) override;
+
+    bool evaluate(ov::TensorVector& outputs, const ov::TensorVector& inputs) const override;
+    bool has_evaluate() const override;
+};
+//! [op:header]
+
+}  // namespace TemplateExtension
diff --git a/src/plugins/intel_gpu/src/plugin/ops/custom.cpp b/src/plugins/intel_gpu/src/plugin/ops/custom.cpp
index 08a4c089d6..ec8f7ed377 100644
--- a/src/plugins/intel_gpu/src/plugin/ops/custom.cpp
+++ b/src/plugins/intel_gpu/src/plugin/ops/custom.cpp
@@ -190,12 +190,14 @@ void CreateCustomOp(Program& p, const std::shared_ptr<ngraph::Node>& op, CustomL
     int featureDim = outputTensor.feature[0];
     int yDim = outputTensor.spatial[1];
     int xDim = outputTensor.spatial[0];
-    size_t iidx = customLayer->InputDimSourceIndex();
-
+    //djx fix Invalid input tensor for index
+    //size_t iidx = customLayer->InputDimSourceIndex();
+    int iidx = customLayer->InputDimSourceIndex();
     std::string genericLayerName = layer_type_name_ID(op);
     // if input index is greater than -1, take dimension from input
     if (iidx >= 0) {
-        if (iidx >= op->get_input_size())
+        //if (iidx >= op->get_input_size())
+        if (iidx >= int(op->get_input_size()))
             IE_THROW() << "Invalid input tensor for index: " << iidx;
         auto inputDims = op->get_input_shape(iidx);
 
diff --git a/tools/mo/openvino/tools/mo/convert_impl.py b/tools/mo/openvino/tools/mo/convert_impl.py
index b11818d1ab..63f75b564e 100644
--- a/tools/mo/openvino/tools/mo/convert_impl.py
+++ b/tools/mo/openvino/tools/mo/convert_impl.py
@@ -419,6 +419,7 @@ def emit_ir(graph: Graph, argv: argparse.Namespace, non_default_params: dict):
     # be destructed before fem object explicitly.
     def read_model(path_to_xml):
         fe = fem.load_by_framework(framework="ir")
+        fe.add_extension(argv.extension_for_ngraph_validation)
         function = fe.convert(fe.load(path_to_xml))
         return function
 
diff --git a/tools/mo/openvino/tools/mo/custom_op_mo_extension/front/onnx/flow_warp_custom_op_ext.py b/tools/mo/openvino/tools/mo/custom_op_mo_extension/front/onnx/flow_warp_custom_op_ext.py
new file mode 100755
index 0000000000..e91b57b9cd
--- /dev/null
+++ b/tools/mo/openvino/tools/mo/custom_op_mo_extension/front/onnx/flow_warp_custom_op_ext.py
@@ -0,0 +1,15 @@
+# Copyright (C) 2022 Intel Corporation
+from openvino.tools.mo.front.extractor import FrontExtractorOp
+from openvino.tools.mo.ops.op import Op
+from openvino.tools.mo.custom_op_mo_extension.ops.flow_warp_custom_op import FlowWarp
+class FlowWarpFrontExtractor(FrontExtractorOp):
+    op = 'flow_warp'
+    enabled = True
+
+    @classmethod
+    def extract(cls, node):
+        # Op.get_op_class_by_name('flow_warp').update_node_stat(node)
+        attrs = {}
+        FlowWarp.update_node_stat(node,attrs)
+
+        return cls.enabled
diff --git a/tools/mo/openvino/tools/mo/custom_op_mo_extension/ops/flow_warp_custom_op.py b/tools/mo/openvino/tools/mo/custom_op_mo_extension/ops/flow_warp_custom_op.py
new file mode 100755
index 0000000000..1baf086dd7
--- /dev/null
+++ b/tools/mo/openvino/tools/mo/custom_op_mo_extension/ops/flow_warp_custom_op.py
@@ -0,0 +1,22 @@
+# Copyright (C) 2022 Intel Corporation
+import numpy as np
+
+# from openvino.tools.mo.front.extractor import bool_to_str
+from openvino.tools.mo.graph.graph import Node, Graph
+from openvino.tools.mo.ops.op import Op
+class FlowWarp(Op):
+    op = 'flow_warp'
+    def __init__(self, graph, attrs):
+        mandatory_props = {
+            'type': self.op,  
+            'op': self.op,   
+            'infer': self.infer
+        }
+        super().__init__(graph, mandatory_props, attrs)
+    
+    @staticmethod
+    def infer(node):
+        node_name = node.soft_get('name', node.id)
+        input_shape = node.in_port(0).data.get_shape()
+        assert input_shape is not None, 'Input shape is None for node "{}"'.format(node_name)
+        node.out_port(0).data.set_shape(input_shape)
\ No newline at end of file
diff --git a/tools/mo/openvino/tools/mo/utils/cli_parser.py b/tools/mo/openvino/tools/mo/utils/cli_parser.py
index f1fe8e9723..447beec5e7 100644
--- a/tools/mo/openvino/tools/mo/utils/cli_parser.py
+++ b/tools/mo/openvino/tools/mo/utils/cli_parser.py
@@ -1047,6 +1047,13 @@ def get_common_cli_parser(parser: argparse.ArgumentParser = None):
                               default=[import_extensions.default_path()],
                               action=CanonicalizePathCheckExistenceAction,
                               type=readable_dirs_or_files_or_empty)
+    # add parse --extension_for_ngraph_validation
+    common_group.add_argument("--extension_for_ngraph_validation",   
+                            help="Path to libraries (.so or .dll) "
+                                "extension about custom operator for ngraph validation before generate the final IR",
+                            default=import_extensions.default_path(),
+                            action=CanonicalizePathCheckExistenceAction,
+                            type=readable_dirs_or_files_or_empty)
     common_group.add_argument("--batch", "-b",
                               type=check_positive,
                               default=None,
diff --git a/tools/pot/openvino/tools/pot/engines/ie_engine.py b/tools/pot/openvino/tools/pot/engines/ie_engine.py
index c75b943bb3..a8954bb701 100644
--- a/tools/pot/openvino/tools/pot/engines/ie_engine.py
+++ b/tools/pot/openvino/tools/pot/engines/ie_engine.py
@@ -101,25 +101,117 @@ class IEEngine(Engine):
             nodes_names_map = nodes_names_map[self._model.friendly_name]
             nodes_name = list(nodes_names_map.keys())
             cast_friendly_names(self._model.outputs)
-
-            outputs = self._add_outputs(list(nodes_names_map.values()))
-            add_tensor_names(outputs, nodes_name)
-
-            model_output_names = collect_model_outputs(self._model)
-
-            align_stat_names_with_results(model_output_names,
-                                          nodes_name,
-                                          node_to_result_names,
-                                          stats_layout,
-                                          stat_aliases)
-
-            # Creating statistics layout with IE-like names
-            stats_layout, stat_names_aliases = self._convert_stats_names(stats_layout)
-
-        self._predict(stats_layout=stats_layout,
-                      sampler=sampler,
-                      print_progress=print_progress,
-                      need_metrics_per_sample=metric_per_sample)
+            #add lk memory patch
+            def outputs_memory_need(outputs):
+                total_need = 0
+                max_need = 0
+                for output in outputs:
+                    node_memory_size = output.get_tensor().size
+                    max_need = max(max_need,node_memory_size)
+                    total_need += node_memory_size
+                return total_need,max_need
+            nodes_names_map_values = list(nodes_names_map.values())
+            outputs = self._add_outputs(nodes_names_map_values)
+            memory_needs,peek_need = outputs_memory_need(outputs)
+
+            import os
+            availiable_system_memory = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_AVPHYS_PAGES')
+
+            min_output_num = int(availiable_system_memory//peek_need)
+            if len(outputs) > min_output_num: 
+                subset_outputs = min_output_num
+                logger.info(f'Available system memory {availiable_system_memory} may not enough for memory needs {memory_needs}')
+                logger.info('Get in iteration mode:')
+                times = len(nodes_names_map_values) // subset_outputs
+                remain = len(nodes_names_map_values) % subset_outputs
+                for idx in range(times):
+                    logger.info(f'Iteraion No.{idx}/{times} for adding output and inference')
+                    self.set_model(model_with_stat_op)
+                    cast_friendly_names(self._model.outputs)
+
+                    subset_nodes_names_map_values = nodes_names_map_values[idx*subset_outputs:(idx+1)*subset_outputs]
+                    outputs = self._add_outputs(subset_nodes_names_map_values)
+
+                    subset_nodes_name = nodes_name[idx*subset_outputs:(idx+1)*subset_outputs]
+                    add_tensor_names(outputs, subset_nodes_name)
+
+                    model_output_names = collect_model_outputs(self._model)
+                    align_stat_names_with_results(model_output_names,
+                                            subset_nodes_name,
+                                            node_to_result_names,
+                                            stats_layout,
+                                            stat_aliases)
+
+                    # Creating statistics layout with IE-like names
+                    stats_layout, stat_names_aliases = self._convert_stats_names(stats_layout)
+                    self._predict(stats_layout=stats_layout,
+                        sampler=sampler,
+                        print_progress=print_progress,
+                        need_metrics_per_sample=metric_per_sample)
+                if remain:
+                    logger.info('In the last outputs to be added')
+                    self.set_model(model_with_stat_op)
+                    cast_friendly_names(self._model.outputs)
+
+                    subset_nodes_names_map_values = nodes_names_map_values[times*subset_outputs:len(nodes_names_map_values)]
+                    outputs = self._add_outputs(subset_nodes_names_map_values)
+                    subset_nodes_name = nodes_name[times*subset_outputs:len(nodes_names_map_values)]
+                    add_tensor_names(outputs, subset_nodes_name)
+
+                    model_output_names = collect_model_outputs(self._model)
+                    align_stat_names_with_results(model_output_names,
+                                            subset_nodes_name,
+                                            node_to_result_names,
+                                            stats_layout,
+                                            stat_aliases)
+
+                    # Creating statistics layout with IE-like names
+                    stats_layout, stat_names_aliases = self._convert_stats_names(stats_layout)
+                    self._predict(stats_layout=stats_layout,
+                        sampler=sampler,
+                        print_progress=print_progress,
+                        need_metrics_per_sample=metric_per_sample)
+                logger.info('Finish the outputs adding iteration')
+            else:
+                add_tensor_names(outputs, nodes_name)
+
+                model_output_names = collect_model_outputs(self._model)
+
+                align_stat_names_with_results(model_output_names,
+                                            nodes_name,
+                                            node_to_result_names,
+                                            stats_layout,
+                                            stat_aliases)
+                # Creating statistics layout with IE-like names
+                stats_layout, stat_names_aliases = self._convert_stats_names(stats_layout)
+                self._predict(stats_layout=stats_layout,
+                            sampler=sampler,
+                            print_progress=print_progress,
+                            need_metrics_per_sample=metric_per_sample)
+        else:
+            self._predict(stats_layout=stats_layout,
+                sampler=sampler,
+                print_progress=print_progress,
+                need_metrics_per_sample=metric_per_sample)
+        
+        #     outputs = self._add_outputs(list(nodes_names_map.values()))
+        #     add_tensor_names(outputs, nodes_name)
+
+        #     model_output_names = collect_model_outputs(self._model)
+
+        #     align_stat_names_with_results(model_output_names,
+        #                                   nodes_name,
+        #                                   node_to_result_names,
+        #                                   stats_layout,
+        #                                   stat_aliases)
+
+        #     # Creating statistics layout with IE-like names
+        #     stats_layout, stat_names_aliases = self._convert_stats_names(stats_layout)
+
+        # self._predict(stats_layout=stats_layout,
+        #               sampler=sampler,
+        #               print_progress=print_progress,
+        #               need_metrics_per_sample=metric_per_sample)
 
         # Process accumulated statistics
         # Replace IE-like statistics names with the original ones
diff --git a/vsr_opt/Custom op from Pytorch to OpenVINO.md b/vsr_opt/Custom op from Pytorch to OpenVINO.md
new file mode 100644
index 0000000000..c0f6a418e1
--- /dev/null
+++ b/vsr_opt/Custom op from Pytorch to OpenVINO.md	
@@ -0,0 +1,559 @@
+# Custom op  registration and implementation from Pytorch to OpenVINO
+
+Here provides a simple introduction about how to add a self-defined operation to Pytorch, ONNX and OpenVINO.
+
+## Create Pytorch Custom op
+
+There are three main steps to creating and applying a custom operation in Pytorch:
+
+1. Adding the custom operator implementation in C++ 
+
+2. Registering custom operator with TorchScript
+
+3. Using the custom operator in programs
+
+If you want to create a custom operator, you need to implement the operator and register the operator in TorchScript as a C++ extension.
+
+As an example, let's create a `flow_warp` custom operator and show the sample code.
+
+Implement the custom operator `flow_warp` and register the custom operator in TorchScript by `torch::RegisterOperators` in `flow_warp.cpp`.
+```c
+// flow_warp_op.cpp
+#include <torch/script.h>
+...
+torch::Tensor flow_warp(torch::Tensor X,torch::Tensor flow) {
+    // flow_warp operator implmentation
+    ...
+}
+// register flow_warp op with TorchScript compiler 
+static auto registry = torch::RegisterOperators("custom_op_namespace::flow_warp", &flow_warp);
+```
+Note that the first argument of `torch::RegisterOperators` includes the **namespace** and **name** of the operator to be registered, which separated by **::**. The next argument is a reference to your function.
+
+Once you finish the C++ implementation of custom operator, you can build it using `setuptools.Extension`. Create a `setup.py` script in the same directory where you have your C++ code. `CppExtension.BuildExtension` takes care of the required compiler flags, such as required include paths and flags required during C++/CUDA mixed compilation.
+
+
+```python
+# setup.py
+from setuptools import setup
+from torch.utils import cpp_extension
+
+setup(name='flow_warp',
+      ext_modules=[cpp_extension.CppExtension('flow_warp', ['flow_warp_op.cpp'])],
+      license='Apache License v2.0',
+      cmdclass={'build_ext': cpp_extension.BuildExtension})
+```
+Running the command `python setup.py build` from your source directory, you can build and install your extension. The shared object should be generated under build directory. In my example, the shared object is `./build/lib.linux-x86_64-3.9/flow_warp.cpython-39-x86_64-linux-gnu.so`
+
+Now, you can use the custom operator in your Pytorch code by loading the share object `torch.ops.load_library("./build/lib.linux-x86_64-3.9/flow_warp.cpython-39-x86_64-linux-gnu.so")` and referring to your custom operator: `torch.ops.<namespace_name>.<operator_name>` e.g. torch.ops.custom_op_namespace.flow_warp. The namespace and operator name is determind in `torch::RegisterOperators`
+
+```python
+# flow_warp_test.py
+import torch
+from torch.onnx import register_custom_op_symbolic
+class FlowWarpModel(torch.nn.Module):
+    def forward(self, x, flow):
+        # call custom operator
+        return torch.ops.custom_op_namespace.flow_warp(x, flow)
+
+if __name__ == '__main__':
+    x = torch.randn((1,1,5,5))
+    flow = torch.randn((1,5,5,2))
+    model = FlowWarpModel()
+    # load the share object
+    torch.ops.load_library("flow_warp_pytorch_op/build/lib.linux-x86_64-3.8/flow_warp.cpython-38-x86_64-linux-gnu.so")
+    output = model(x,flow)
+```
+
+## Expert the Pytorch custom op to ONNX
+
+You can export your custom operator using existing ONNX ops, or you can create custom ONNX ops to use. In both cases, you need to **add the symbolic method to the exporter**, and **register your custom symbolic** using `torch.onnx.register_custom_op_symbolic`.
+
+In our example, we intend to create the custom operator by our own. We also add the registeration script in the python file `flow_warp_test.py`
+
+```python
+# flow_warp_test.py
+# from torch.onnx.symbolic_helper import parse_args
+# @parse_args("v", "v")
+def my_flow_warp(g, input, flow):
+    return g.op("custom_op::flow_warp", input, flow)
+
+from torch.onnx import register_custom_op_symbolic
+register_custom_op_symbolic("custom_op_namespace::flow_warp", my_flow_warp, 11)
+```
+`g.op()` registers the custom operator symbolic in ONNX. The first argument is `<domain>::<custom_op_name>`.
+
+`register_custom_op_symbolic` function passes `<namespace_in_pytorch>::<custom_op_name>` in Pytorch to `<custom_op_domain_in_onnx>::<custom_op_name>` in ONNX. Note that the namespace/domain and custom operator name can be different in Pytorch and ONNX.
+
+Now, export the custom operator to ONNX using `torch.onnx.export`.
+```python
+# flow_warp_test.py
+import torch
+# from torch.onnx.symbolic_helper import parse_args
+# @parse_args("v", "v")
+def my_flow_warp(g, input, flow):
+    return g.op("custom_op::flow_warp", input, flow)
+
+from torch.onnx import register_custom_op_symbolic
+register_custom_op_symbolic("custom_op_namespace::flow_warp", my_flow_warp, 11)
+
+
+from torch.onnx import register_custom_op_symbolic
+class FlowWarpModel(torch.nn.Module):
+    def forward(self, x, flow):
+        return torch.ops.custom_op_namespace.flow_warp(x, flow)
+
+
+def exprt_to_onnx(model,inputs):
+    # export pytorch model to onnx model with custom operator
+    torch.onnx.export(model,inputs,'FlowWarp.onnx',opset_version=11,
+        input_names=["input", "flow"], 
+        output_names=["ouput"],custom_opsets={"custom_op": 11})
+    print('Successfully export FlowWarp.onnx')
+
+
+if __name__ == '__main__':
+    x = torch.randn((1,1,5,5))
+    flow = torch.randn((1,5,5,2))
+    model = FlowWarpModel()
+    # load the share object
+    torch.ops.load_library("flow_warp_pytorch_op/build/lib.linux-x86_64-3.8/flow_warp.cpython-38-x86_64-linux-gnu.so")
+    output = model(x,flow)
+    inputs = (x,flow)
+    # export the custom operator to onnx
+    export_to_onnx(model,inputs)
+```
+
+## Implement the custom op in ONNX runtime
+
+The next step is to implement this operator in ONNX Runtime and build it to a library. For this step, you need to have ONNX Runtime installed on your system.
+
+In order to implement the custom operator in ONNX runtime, you need to create a **custom operator object** and write it's **kernel implementations**, and **add it to your custom domain**. In our example we create a script named `flow_warp_op.h` to define the custom operation object and kernel definition.
+
+```c
+// flow_warp_op.h
+#ifndef ONNXRUNTIME_FLOWWARP_H
+#define ONNXRUNTIME_FLOWWARP_H
+
+#include <onnxruntime_cxx_api.h>
+
+// custom operator kernel define
+struct FlowWarpKernel {
+  FlowWarpKernel(OrtApi api, const OrtKernelInfo *info);
+  void Compute(OrtKernelContext *context);
+
+protected:
+  OrtApi api_;
+  Ort::CustomOpApi ort_;
+  const OrtKernelInfo *info_;
+  Ort::AllocatorWithDefaultOptions allocator_;
+};
+
+// custom operator object
+struct FlowWarpOp : Ort::CustomOpBase<FlowWarpOp, FlowWarpKernel> {
+  void *CreateKernel(OrtApi api, const OrtKernelInfo *info) const {
+    return new FlowWarpKernel(api, info);
+  };
+
+  const char *GetName() const { return "flow_warp"; };
+
+  size_t GetInputTypeCount() const { return 2; };
+  ONNXTensorElementDataType GetInputType(size_t /*index*/) const {
+    return ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT;
+  };
+
+  size_t GetOutputTypeCount() const { return 1; };
+  ONNXTensorElementDataType GetOutputType(size_t /*index*/) const {
+    return ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT;
+  };
+
+  const char *GetExecutionProviderType() const {
+    return "CPUExecutionProvider";
+  };
+};
+#endif
+```
+
+The implementation is write in `flow_warp_op.cc`.
+```c
+#include <iostream>
+#include "flow_warp_op.h"
+struct OrtTensorDimensions : std::vector<int64_t> {
+  OrtTensorDimensions(Ort::CustomOpApi ort, const OrtValue* value) {
+    OrtTensorTypeAndShapeInfo* info = ort.GetTensorTypeAndShape(value);
+    std::vector<int64_t>::operator=(ort.GetTensorShape(info));
+    ort.ReleaseTensorTypeAndShapeInfo(info);
+  }
+};
+
+FlowWarpKernel::FlowWarpKernel(OrtApi api, const OrtKernelInfo *info):api_(api),
+        ort_(api_),info_(info)
+{
+  allocator_ = Ort::AllocatorWithDefaultOptions();
+}
+
+void FlowWarpKernel::Compute(OrtKernelContext *context) {
+    // flow_warp operator implementation
+    ...
+}
+```
+Once you have the custom kernel and schema, you can add them to the domain using the C API. In our example, we create a register function in `register_op.h` and implement the function in source file `register_op.cpp`.
+
+```c
+// registor_op.h
+#ifndef ONNXRUNTIME_CUSTOM_OP_REGISTER_H
+#define ONNXRUNTIME_CUSTOM_OP_REGISTER_H
+#include <onnxruntime_c_api.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+OrtStatus *ORT_API_CALL RegisterCustomOps(OrtSessionOptions *options,
+                                          const OrtApiBase *api);
+
+#ifdef __cplusplus
+}
+#endif
+#endif  // ONNXRUNTIME_REGISTER_H
+```
+
+```c
+// register_op.cpp
+#include "flow_warp_op.h"
+#include <iostream>
+#include "onnxruntime_cxx_api.h"
+#include "register_op.h"
+
+FlowWarpOp c_FlowWarpOp;
+
+OrtStatus *ORT_API_CALL RegisterCustomOps(OrtSessionOptions *options,
+                                          const OrtApiBase *api) {
+    OrtCustomOpDomain *domain = nullptr;
+    const OrtApi *ortApi = api->GetApi(ORT_API_VERSION);
+    if (auto status = ortApi->CreateCustomOpDomain("custom_op", &domain)) {
+        return status;
+    }
+
+    // AddOrtCustomOpDomainToContainer(domain, ortApi);
+
+    if (auto status = ortApi->CustomOpDomain_Add(domain, &c_FlowWarpOp)) {
+        return status;
+    }
+
+    return ortApi->AddCustomOpDomain(options, domain); 
+  }
+
+```
+
+In the register function, you need to create a custom domain and add it to the api. This domain name is the same name provided in the symbolic method when exporting the model. Also this domain name is same with the domain name in ONNX. In our example, the domain name is `custom_op`. Note that the custom operator object should be a **global** variant, in case of scope failure.
+
+Now, you need to create a `CMakeLists.txt` to build the custom operator implmentation to a share library.
+
+Once you have the cmake file, create a build directory from the same location by command `mkdir build` and get in the directory by `cd build`. Execute the command `cmake ..` to configure the project and build it using `make` command.
+
+```cmake
+# CMakeLists.txt
+cmake_minimum_required(VERSION 3.10)
+project (FlowWarp_op)
+add_definitions(-std=c++11 -g)
+
+set(SOURCE flow_warp_op.cc register_op.cpp)
+set(HEADER flow_warp_op.h register_op.h)
+set(TEST_SOURCE "")
+add_library(FlowWarp_op SHARED ${SOURCE} ${HEADER} ${TEST_SOURCE})
+
+#Include path to header files for Custom Op
+include_directories("/home/media/xxx/basicvsr/custom_op/ort_custom_op/download/build/native/include/")
+
+#Linking dependencies for Custom Op
+find_library(ONNXRUNTIME_LIBRARY onnxruntime HINTS "/home/media/xxx/basicvsr/custom_op/ort_custom_op/download/runtimes/linux-x64/native")
+target_link_libraries(FlowWarp_op PUBLIC ${ONNXRUNTIME_LIBRARY})
+```
+
+Now that you have implemented the custom operator, you should be able to run your model and test it. Before running, you need to register the custom operator to onnxruntime sessions by the share library using `register_custom_ops_library`.
+
+```py
+def inference_on_onnx(x,flow):
+    ort_custom_op_path= "./flow_warp_onnxruntime_op/build/libFlowWarp_op.so"
+    onnx_file = 'FlowWarp.onnx'
+    session_options = ort.SessionOptions()
+    if ort_custom_op_path:
+        # register the custom operation
+        session_options.register_custom_ops_library(ort_custom_op_path)
+    
+    sess = ort.InferenceSession(onnx_file, session_options)
+    ort_result = sess.run(None, {
+        'input': x.detach().numpy(),
+        'flow': flow.detach().numpy()
+    })
+    return ort_result
+```
+The code fragment above can be added to the test file `flow_warp_test.py`.
+
+More details in [How to export Pytorch model with custom op to ONNX and run it in ONNX Runtime](https://github.com/onnx/tutorials/tree/master/PyTorchCustomOperator).
+
+
+## Expert ONNX custom operator to OpenVINO IR and implement for OpenVINO runtime
+
+As the OpenVINO offical documents [Custom Operation Support Overview](https://docs.openvino.ai/2021.4/openvino_docs_HOWTO_Custom_Layers_Guide.html#enabling-magnetic-resonance-image-reconstruction-model) informed, there are three steps to support inference of a model with custom operation(s):
+
+Step1: Add support for custom operation in the Model Optimizer
+
+Step2: Create custom operation as nGraph operation.
+
+Step3: Create the custom operation implementation for Inference Engine.
+
+
+
+### Add support for custom in the Model Optimizer
+
+You may refer to the [Model Optimizer Workflow](https://docs.openvino.ai/2020.1/_docs_HOWTO_Custom_Layers_Guide.html) and you can find that there are two keys to extend the custom layers in Model Optimizer:
+- Custom Layer Extractor
+- Custom Layer Operation
+
+In order to extend the custom operation in Model Optimizer, you need to make a directory in the openvino mo path `openvino/tools/mo/openvino/tools/mo` and define the custom layer extractor and operation. In our example, we make a directory `my_custom_op` and further make two sub-directories `front/onnx` and `ops`. We create `flow_warp_custom_op_ext.py` in `front/onnx` and `flow_warp_custom_op.py` in `ops`. [Example of Extractor and Operation](https://docs.openvino.ai/2021.4/openvino_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html#extensions)
+
+```py
+# flow_warp_custom_op_ext.py
+
+from mo.front.extractor import FrontExtractorOp
+from mo.ops.op import Op
+from mo.my_mo_extensions.ops.flow_warp_custom_op import FlowWarp
+class FlowWarpFrontExtractor(FrontExtractorOp):
+    op = 'flow_warp'
+    enabled = True
+
+    @classmethod
+    def extract(cls, node):
+        attrs = {}
+        FlowWarp.update_node_stat(node,attrs)
+
+        return cls.enabled
+```
+
+```py
+# flow_warp_custom_op.py
+
+import numpy as np
+from openvino.tools.mo.graph.graph import Node, Graph
+from openvino.tools.mo.ops.op import Op
+class FlowWarp(Op):
+    op = 'flow_warp'
+    def __init__(self, graph, attrs):
+        mandatory_props = {
+            'type': self.op,  
+            'op': self.op,   
+            'infer': self.infer
+        }
+        super().__init__(graph, mandatory_props, attrs)
+    
+    @staticmethod
+    def infer(node):
+        node_name = node.soft_get('name', node.id)
+        input_shape = node.in_port(0).data.get_shape()
+        assert input_shape is not None, 'Input shape is None for node "{}"'.format(node_name)
+        node.out_port(0).data.set_shape(input_shape)
+```
+Theoretically, you can transform the ONNX model to OpenVINO IR successfully by the custom layer extractor and operation. However, in the more recently release OpenVINO source code, there would be nGraph validation in the MO process. That means the MO process would produce an temporary IR and try to build the model graph to validate the IR is workable. The validation process would meet some mistakes, you can find the solution latter.
+
+Refer to [Model Optimizer Extensibility](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html#model-optimizer-extensions) for more information.
+
+### Create custom operation as nGraph operation
+There is the offical documents about create custom operation as nGraph operation
+[Custom nGraph Operations](https://docs.openvino.ai/2021.4/openvino_docs_IE_DG_Extensibility_DG_AddingNGraphOps.html#doxid-openvino-docs-i-e-d-g-extensibility-d-g-adding-n-graph-ops).
+
+Actually, you can find a example of custom operator extension in the OpenVINO source code in the path: `openvino/docs/template_extension/`. There are two kinds of template in the directory and the old one is the same as the documents described. However, the old one may not work in the new release (2022.1 in our example). In our example, we try the new one.
+
+You need to define your namespace and your custom operator object which should inherit from `ov::op::OP`. In our example, we create `flow_warp_custom_op.h` to define the `FlowWarp` operator. As for some override functions you may refer to [Custom nGraph Operations](https://docs.openvino.ai/2021.4/openvino_docs_IE_DG_Extensibility_DG_AddingNGraphOps.html#doxid-openvino-docs-i-e-d-g-extensibility-d-g-adding-n-graph-ops).
+
+```cpp
+// flow_warp_custom_op.h
+#pragma once
+
+//! [op:common_include]
+#include <openvino/op/op.hpp>
+//! [op:common_include]
+
+//! [op:header]
+namespace CustomExtension {
+
+class FlowWarp : public ov::op::Op {
+public:
+    OPENVINO_OP("flow_warp");
+
+    FlowWarp() = default;
+    FlowWarp(const ov::Output<ov::Node>& input,const ov::Output<ov::Node>& flow);
+    void validate_and_infer_types() override;
+    std::shared_ptr<ov::Node> clone_with_new_inputs(const ov::OutputVector& new_args) const override;
+    bool visit_attributes(ov::AttributeVisitor& visitor) override;
+
+    bool evaluate(ov::TensorVector& outputs, const ov::TensorVector& inputs) const override;
+    bool has_evaluate() const override;
+};
+//! [op:header]
+
+}  // namespace TemplateExtension
+
+```
+The implmentation is in the source file `flow_warp_custom_op.cpp`.
+```cpp
+//flow_warp_custom_op.cpp
+
+#include "flow_warp_custom_op.h"
+using namespace CustomExtension;
+
+//! [op:ctor]
+FlowWarp::FlowWarp(const ov::Output<ov::Node>& input,const ov::Output<ov::Node>& flow) : Op({input,flow}) {
+    constructor_validate_and_infer_types();
+}
+//! [op:ctor]
+
+//! [op:validate]
+void FlowWarp::validate_and_infer_types() {
+    // Operation doesn't change shapes end element type
+    set_output_type(0, get_input_element_type(0), get_input_partial_shape(0));
+}
+//! [op:validate]
+
+//! [op:copy]
+std::shared_ptr<ov::Node> FlowWarp::clone_with_new_inputs(const ov::OutputVector& new_args) const {
+    OPENVINO_ASSERT(new_args.size() == 2, "Incorrect number of new arguments");
+
+    return std::make_shared<FlowWarp>(new_args.at(0),new_args.at(1));
+}
+//! [op:copy]
+
+//! [op:visit_attributes]
+bool FlowWarp::visit_attributes(ov::AttributeVisitor& visitor) {
+    return true;
+}
+//! [op:visit_attributes]
+
+//! [op:evaluate]
+bool FlowWarp::evaluate(ov::TensorVector& outputs, const ov::TensorVector& inputs) const {
+    // flow_warp custom operator implementation
+    ...
+    return true;
+}
+
+bool FlowWarp::has_evaluate() const {
+    return true;
+}
+//! [op:evaluate]
+```
+Note that in the new custom extension method the `evaluate` function is the entrance for `CPU` kernel to perform the custom operator's operation. Therefore, the Step 3 "Create the custom operation for Inference Engine" can be merged into Step 2.
+
+After defining and implmenting the custom operator, the next step is to register the custom operation. You may register the custom operation by create the custom operation as an extension. In our example, we create a script file `extension.cpp` to register custom operation as an extension. In the script you need to define two share pointer on behalf of the operation itself and the operation mapping.
+
+```cpp
+// extension.cpp
+
+// Copyright (C) 2018-2022 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+
+#include <openvino/core/extension.hpp>
+#include <openvino/core/op_extension.hpp>
+#include <openvino/frontend/extension.hpp>
+
+#include "flow_warp_custom_op.h"
+
+// clang-format off
+//! [ov_extension:entry_point]
+OPENVINO_CREATE_EXTENSIONS(
+    std::vector<ov::Extension::Ptr>({
+
+        // Register operation itself, required to be read from IR
+        std::make_shared<ov::OpExtension<CustomExtension::FlowWarp>>(),
+
+        // Register operaton mapping, required when converted from framework model format
+        std::make_shared<ov::frontend::OpExtension<CustomExtension::FlowWarp>>()
+    }));
+//! [ov_extension:entry_point]
+// clang-format on
+
+```
+Now, you can build the custom operation souce code into a share library by `CMakeLists.txt`.
+```cmake
+# Copyright (C) 2018-2022 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
+# [cmake:extension]
+set(CMAKE_CXX_STANDARD 11)
+
+set(TARGET_NAME "custom_extension")
+
+find_package(OpenVINO)
+
+set(SRC flow_warp_custom_op.cpp extension.cpp flow_warp.cpp)
+
+add_library(${TARGET_NAME} MODULE ${SRC})
+
+target_compile_definitions(${TARGET_NAME} PRIVATE IMPLEMENT_OPENVINO_EXTENSION_API)
+target_link_libraries(${TARGET_NAME} PRIVATE openvino::runtime)
+```
+
+Since we have acquired the custom operation share library, we can add extension for nGraph validation in MO process. In our example, to address the nGraph validation problem, we make some modification in the `openvino/tools/mo/openvino/tools/mo/back/offline_transformations.py` by adding the custom extension to library path manually.
+```py
+# offline_transformations.py
+
+    def read_model(path_to_xml):
+        fe = fem.load_by_framework(framework="ir")
+        # add custom extension for nGraph validation 
+        fe.add_extension('xxx/openvino/bin/intel64/Release/lib/libcustom_extension.so')  # addin
+        function = fe.convert(fe.load(path_to_xml))
+        return function
+```
+
+### Create the custom operation implementation for Inference Engine
+
+To enable the custom operation on GPU, you need to provide the kernel code in OpenCL C and an XML configuration file that connects the kernel and its parameters to the parameters of the operation.
+
+```c
+//flow_warp.cl
+
+//OCL custom layer implementation for flow_warp
+__kernel void flow_warp(
+    const __global INPUT0_TYPE* input0,
+    const __global INPUT1_TYPE* input1,
+    __global OUTPUT0_TYPE* output)
+{
+    // flow_warp custom operator ocl implementation 
+    ...
+}
+
+```
+
+```xml
+<!-- flow_warp.xml -->
+
+<!-- configuration file for flow warp kernel -->
+<CustomLayer name="flow_warp" type="SimpleGPU" version="1">
+  <Kernel entry="flow_warp">
+    <Source filename="flow_warp.cl"/>
+    <!-- <Define name="neg_slope" type="float" param="negative_slope" default="0.0"/> -->
+  </Kernel>
+  <Buffers>
+    <Tensor arg-index="0" type="input" port-index="0" format="BFYX"/>
+    <Tensor arg-index="1" type="input" port-index="1" format="BFYX"/>
+    <Tensor arg-index="2" type="output" port-index="0" format="BFYX"/>
+  </Buffers>
+  <CompilerOptions options="-cl-mad-enable"/>
+  <WorkSizes global="X,Y,B*F"/>
+</CustomLayer>
+
+```
+After obtaining the IR format in success, you can use the custom operation in your scene by loading the custom operation share library as extension. You can add the code fragments below in your right position. For more details you may refer to [How to Implement Custom GPU Operations](https://docs.openvino.ai/2021.4/openvino_docs_IE_DG_Extensibility_DG_GPU_Kernel.html).
+
+```py
+from openvino.runtime import Core
+ie=Core()
+# for CPU extension
+ie.add_extension('xxx/openvino/bin/intel64/lib/libcustom_extension.so') 
+# for GPU extension
+ie.add_extension('xxx/openvino/bin/intel64/lib/libcustom_extension.so')
+ie.set_property('GPU', {'CONFIG_FILE': 'xxx/openvino/flow_warp_custom_op_gpu/flow_warp.xml'})
+```
+
+
+
diff --git a/vsr_opt/VSR Introduction.md b/vsr_opt/VSR Introduction.md
new file mode 100644
index 0000000000..2ae3256ff4
--- /dev/null
+++ b/vsr_opt/VSR Introduction.md	
@@ -0,0 +1,77 @@
+# VSR Introduction
+
+This tutorial demonstrates step-by-step instructions to perform Video Super Resolution (VSR) inference on a Pytorch model using OpenVINO's Inference Engine.
+
+## Pytorch to ONNX
+
+Firstly, the PyTorch model needs to be converted to ONNX, you may refer to [Convert PyTorch model to ONNX](https://docs.openvino.ai/latest/notebooks/102-pytorch-onnx-to-openvino-with-output.html#onnx-model-conversion) for details. There is an sample script `pytorch2onnx.py` implements the model conversion. You can perform the model conversion by command:
+```bash
+# Build custom op library
+cd <PATH_TO_THIS_PROJECT>/vsr_opt/flow_warp_pytorch_op
+python setup.py build
+```
+```bash
+export PYTHONPATH=$PYTHONPATH:<PATH_TO_THIS_PROJECT>/vsr_opt/
+cd <PATH_TO_THIS_PROJECT>/vsr_opt
+# To get a model w/o custom op
+python tools/pytorch2onnx.py --input_model <Pytorch model> --output_dir <Output dir to store ONNX model> --nif <Number of input frames> --width <Width of input frames> --height <Height of input frames> 
+# To get a model w/ custom op
+python tools/pytorch2onnx.py --input_model <Pytorch model> --output_dir <Output dir to store ONNX model> --nif <Number of input frames> --width <Width of input frames> --height <Height of input frames> --custom_op
+```
+
+## ONNX to OpenVINO IR
+
+Then the ONNX model needs to be further converted to OpenVINO Intermediate Representation (IR) formats. Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR.
+First, install new mo and pot python package.
+```bash
+mkdir <PATH_TO_THIS_PROJECT>/wheels
+cd <PATH_TO_THIS_PROJECT>/tools/mo && python3 setup.py build && python3 setup.py bdist_wheel --dist-dir=<PATH_TO_THIS_PROJECT>/wheels
+cd <PATH_TO_THIS_PROJECT>/tools/pot && python3 setup.py build && python3 setup.py bdist_wheel --dist-dir=<PATH_TO_THIS_PROJECT>/wheels
+sudo python3 -m pip install <PATH_TO_THIS_PROJECT>/wheels/*
+rm -r <PATH_TO_THIS_PROJECT>/wheels
+```
+You could uninstall mo and pot with:
+```bash
+sudo python3 -m pip uninstall <PATH_TO_THIS_PROJECT>/wheels/*
+```
+Then, run `mo.py` with the following command:
+```bash
+cd <PATH_TO_THIS_PROJECT>
+python3 tools/mo/openvino/tools/mo/mo.py --input_model <ONNX model> --extensions tools/mo/openvino/tools/mo/custom_op_mo_extension/ --extension_for_ngraph_validation bin/intel64/lib/libcustom_extension.so --output_dir <Output dir to store OpenVINO IR>
+```
+
+ Refer to [Convert model with Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information. 
+
+## OpenVINO IR Quantization
+
+Since the precision of original Pytorch model may be FP32 or FP16, in order to cut down the memory cost and accelerate the inference process you may need to perform model quantization. OpenVINO provides a Post-training Optimization Tool (POT) which supports the uniform integer quantization method. There is a sample script `quantization.py` to perform INT8 quantization . You may perform INT8 quantization by command:
+```bash
+python3 quantization.py --input_model <XML model to be quantized> --dataset_path <Path of calibration dataset> --sub_folder <Scenario sub-folder of calibration dataset> --nif <Number of input frames>  
+```
+
+
+Refer to [Post-training Optimization Tool](https://docs.openvino.ai/latest/pot_introduction.html) for more information.
+
+## OpenVINO IR Inference
+
+Last but not least, you can perform the inference by OpenVINO IR. There is a python sample code `basicvsr_inference_sample.py` implements the model inference. You can start the inference by command:
+
+```bash
+cd <PATH_TO_THIS_PROJECT>
+
+# Perform inference w/o patch evaluation
+python3 vsr_opt/samples/basicvsr_inference_sample.py --model <XML model> --input <Input frames> --number_input_frames 3 --device GPU --extensions bin/intel64/lib/libcustom_extension.so --path_to_cldnn_config flow_warp_custom_op/flow_warp.xml --save_path <Output dir to store predictions>
+
+# Perform inference w/ patch evaluation
+python3 vsr_opt/samples/basicvsr_inference_sample.py --model <XML model> --input <Input frames> --number_input_frames 3 --device GPU --extensions bin/intel64/lib/libcustom_extension.so --path_to_cldnn_config flow_warp_custom_op/flow_warp.xml --save_path <Output dir to store predictions> --patch_evaluation
+```
+
+Also, there is a C++ sample code. You can run `<PATH_TO_THIS_PROJECT>/bin/intel64/basicvsr -h` to get help messages and see the default settings of parameters. The followings are some examples to perform BasicVSR inference.
+
+```bash
+
+# Run the inference w/o patch evaluation on CPU
+<PATH_TO_THIS_PROJECT>/bin/intel64/basicvsr_sample -model_path=<IR model path(.xml)> -extension=<PATH_TO_THIS_PROJECT>/bin/intel64/lib/libcustom_extension.so -data_path=<Directory path including input frames> -nif=<Number of input frames> -save_predictions -save_path=<Directory path to save results> -cldnn_config=<PATH_TO_THIS_PROJECT>/flow_warp_custom_op/flow_warp.xml
+
+# Run the inference w/ patch evaluation on GPU
+<PATH_TO_THIS_PROJECT>/bin/intel64/basicvsr_sample -model_path=<IR model path(.xml)> -extension=<PATH_TO_THIS_PROJECT>/bin/intel64/lib/libcustom_extension.so -data_path=<Directory path including input frames> -nif=<Number of input frames> -patch_evalution -device=GPU -save_predictions -save_path=<Directory path to save results> -cldnn_config=<PATH_TO_THIS_PROJECT>/flow_warp_custom_op/flow_warp.xml
\ No newline at end of file
diff --git a/vsr_opt/configs/basicvsr_x2_cuc.py b/vsr_opt/configs/basicvsr_x2_cuc.py
new file mode 100755
index 0000000000..a2f5476240
--- /dev/null
+++ b/vsr_opt/configs/basicvsr_x2_cuc.py
@@ -0,0 +1,18 @@
+
+exp_name = 'basicvsr_x2_cuc'
+
+# model settings
+model = dict(
+    type='BasicVSR',
+    generator=dict(
+        type='BasicVSRNet',
+        scale=2,
+        mid_channels=64,
+        num_blocks=8,
+        spynet_pretrained='https://download.openmmlab.com/mmediting/restorers/'
+                          'basicvsr/spynet_20210409-c6c1bd09.pth',
+        custom_op=False),
+    pixel_loss=dict(type='CharbonnierLoss', loss_weight=1.0, reduction='mean'))
+# model training and testing settings
+train_cfg = dict(fix_iter=5000)
+test_cfg = dict(metrics=[], crop_border=0)
diff --git a/vsr_opt/flow_warp_pytorch_op/flow_warp_op.cpp b/vsr_opt/flow_warp_pytorch_op/flow_warp_op.cpp
new file mode 100755
index 0000000000..87e274945d
--- /dev/null
+++ b/vsr_opt/flow_warp_pytorch_op/flow_warp_op.cpp
@@ -0,0 +1,483 @@
+// Copyright (C) 2022 Intel Corporation
+#include <torch/script.h>
+#include <vector>
+#include <iostream>
+#include <cmath>
+#define MIN(a, b) (((a) < (b)) ? (a) : (b))
+#define MAX(a, b) (((a) < (b)) ? (b) : (a))
+#define CLIP_COORDINATES(in, out, clip_limit) \
+  out = MIN((clip_limit - 1), MAX(in, 0))
+
+enum GridSamplerInterpolation { Bilinear = 0, Nearest = 1, Bicubic = 2 };
+enum GridSamplerPadding { Zeros = 0, Border = 1, Reflection = 2 };
+
+template <typename scalar_t>
+static inline scalar_t grid_sampler_unnormalize(scalar_t coord, int64_t size,
+                                                bool align_corners) {
+  if (align_corners) {
+    return ((coord + 1) / 2) * (size - 1);
+  } else {
+    return ((coord + 1) * size - 1) / 2;
+  }
+}
+
+// Clips coordinates to between 0 and clip_limit - 1
+template <typename scalar_t>
+static inline scalar_t clip_coordinates(scalar_t in, int64_t clip_limit) {
+  return std::min(static_cast<scalar_t>(clip_limit - 1),
+                  std::max(in, static_cast<scalar_t>(0)));
+}
+
+// Reflects coordinates until they fall between low and high (inclusive).
+// The bounds are passed as twice their value so that half-integer values
+// can be represented as ints.
+template <typename scalar_t>
+static inline scalar_t reflect_coordinates(scalar_t in, int64_t twice_low,
+                                           int64_t twice_high) {
+  if (twice_low == twice_high) {
+    return static_cast<scalar_t>(0);
+  }
+  scalar_t min = static_cast<scalar_t>(twice_low) / 2;
+  scalar_t span = static_cast<scalar_t>(twice_high - twice_low) / 2;
+  in = std::fabs(in - min);
+  // `fmod` returns same sign as `in`, which is positive after the `fabs` above.
+  scalar_t extra = std::fmod(in, span);
+  int flips = static_cast<int>(std::floor(in / span));
+  if (flips % 2 == 0) {
+    return extra + min;
+  } else {
+    return span - extra + min;
+  }
+}
+
+template <typename scalar_t>
+static inline scalar_t compute_coordinates(scalar_t coord, int64_t size,
+                                           int64_t padding_mode,
+                                           bool align_corners) {
+  if (padding_mode == GridSamplerPadding::Border) {
+    coord = clip_coordinates(coord, size);
+  } else if (padding_mode == GridSamplerPadding::Reflection) {
+    if (align_corners) {
+      coord = reflect_coordinates(coord, 0, 2 * (size - 1));
+    } else {
+      coord = reflect_coordinates(coord, -1, 2 * size - 1);
+    }
+    coord = clip_coordinates(coord, size);
+  }
+  return coord;
+}
+
+// Computes the pixel source index value for a grid coordinate
+template <typename scalar_t>
+static inline scalar_t grid_sampler_compute_source_index(scalar_t coord,
+                                                         int64_t size,
+                                                         int64_t padding_mode,
+                                                         bool align_corners) {
+  coord = grid_sampler_unnormalize(coord, size, align_corners);
+  coord = compute_coordinates(coord, size, padding_mode, align_corners);
+  return coord;
+}
+
+static inline bool within_bounds_2d(int64_t h, int64_t w, int64_t H,
+                                    int64_t W) {
+  return h >= 0 && h < H && w >= 0 && w < W;
+}
+
+template <typename scalar_t>
+static inline scalar_t get_value_bounded(const scalar_t *data, scalar_t x,
+                                         scalar_t y, int64_t W, int64_t H,
+                                         int64_t sW, int64_t sH,
+                                         int64_t padding_mode,
+                                         bool align_corners) {
+  x = compute_coordinates(x, W, padding_mode, align_corners);
+  y = compute_coordinates(y, H, padding_mode, align_corners);
+
+  int64_t ix = static_cast<int64_t>(x);
+  int64_t iy = static_cast<int64_t>(y);
+
+  if (within_bounds_2d(iy, ix, H, W)) {
+    return data[iy * sH + ix * sW];
+  }
+  return static_cast<scalar_t>(0);
+}
+
+template <typename scalar_t>
+static inline scalar_t cubic_convolution1(scalar_t x, scalar_t A) {
+  return ((A + 2) * x - (A + 3)) * x * x + 1;
+}
+
+template <typename scalar_t>
+static inline scalar_t cubic_convolution2(scalar_t x, scalar_t A) {
+  return ((A * x - 5 * A) * x + 8 * A) * x - 4 * A;
+}
+
+template <typename scalar_t>
+static inline void get_cubic_upsample_coefficients(scalar_t coeffs[4],
+                                                   scalar_t t) {
+  scalar_t A = -0.75;
+
+  scalar_t x1 = t;
+  coeffs[0] = cubic_convolution2<scalar_t>(x1 + 1.0, A);
+  coeffs[1] = cubic_convolution1<scalar_t>(x1, A);
+
+  // opposite coefficients
+  scalar_t x2 = 1.0 - t;
+  coeffs[2] = cubic_convolution1<scalar_t>(x2, A);
+  coeffs[3] = cubic_convolution2<scalar_t>(x2 + 1.0, A);
+}
+
+template <typename scalar_t>
+static inline scalar_t cubic_interp1d(scalar_t x0, scalar_t x1, scalar_t x2,
+                                      scalar_t x3, scalar_t t) {
+  scalar_t coeffs[4];
+  get_cubic_upsample_coefficients<scalar_t>(coeffs, t);
+
+  return x0 * coeffs[0] + x1 * coeffs[1] + x2 * coeffs[2] + x3 * coeffs[3];
+}
+
+void bilinear_grid_sample(torch::Tensor X, torch::Tensor grid, torch::Tensor &output){
+    float* input_data = X.data_ptr<float>();
+    float* grid_data = grid.data_ptr<float>();
+    float* out_ptr = output.data_ptr<float>();
+    const bool align_corners = true;
+    const int64_t padding_mode = 0;  // padding_mode: zeros
+    const int64_t interpolation_mode = 0; // interpolation_mode: bilinear
+    int64_t N = X.size(0);
+    int64_t C = X.size(1);
+    int64_t inp_H = X.size(2);
+    int64_t inp_W = X.size(3);
+    int64_t /*grid_N = grid.size(0),*/grid_H = grid.size(1),grid_W = grid.size(2),grid_C = grid.size(3);
+    int64_t /*out_N = N,*/ out_C = C;
+    int64_t out_H = grid.size(1);
+    int64_t out_W =  grid.size(2);
+
+    int64_t inp_sN = C * inp_H * inp_W;//input_dims[1] * input_dims[2] * input_dims[3];
+    int64_t inp_sC = inp_H * inp_W;//input_dims[2] * input_dims[3];
+    int64_t inp_sH = inp_W;//input_dims[3];
+    int64_t inp_sW = 1;
+    int64_t grid_sN = grid_H * grid_W * grid_C;//grid_dims[1] * grid_dims[2] * grid_dims[3];
+    int64_t grid_sH = grid_W * grid_C;//grid_dims[2] * grid_dims[3];
+    int64_t grid_sW = grid_C;//grid_dims[3];
+    int64_t grid_sCoor = 1;
+    int64_t out_sN = out_C * out_H * out_W;//output_dims[1] * output_dims[2] * output_dims[3];
+    int64_t out_sC = out_H * out_W;//output_dims[2] * output_dims[3];
+    int64_t out_sH = out_W;//output_dims[3];
+    int64_t out_sW = 1;
+
+    // loop over each output pixel
+  for (int64_t n = 0; n < N; ++n) {
+    const float *grid_ptr_N = grid_data + n * grid_sN;
+    const float *inp_ptr_N = input_data + n * inp_sN;
+    for (int64_t h = 0; h < out_H; ++h) {
+      for (int64_t w = 0; w < out_W; ++w) {
+        const float *grid_ptr_NHW = grid_ptr_N + h * grid_sH + w * grid_sW;
+        float x = *grid_ptr_NHW;
+        float y = grid_ptr_NHW[grid_sCoor];
+
+        float ix = grid_sampler_compute_source_index(x, inp_W, padding_mode,
+                                                     align_corners);
+        float iy = grid_sampler_compute_source_index(y, inp_H, padding_mode,
+                                                     align_corners);
+
+        if (interpolation_mode == GridSamplerInterpolation::Bilinear) {
+          // get corner pixel values from (x, y)
+          // for 4d, we use north-east-south-west
+          int64_t ix_nw = static_cast<int64_t>(std::floor(ix));
+          int64_t iy_nw = static_cast<int64_t>(std::floor(iy));
+
+          int64_t ix_ne = ix_nw + 1;
+          int64_t iy_ne = iy_nw;
+
+          int64_t ix_sw = ix_nw;
+          int64_t iy_sw = iy_nw + 1;
+
+          int64_t ix_se = ix_nw + 1;
+          int64_t iy_se = iy_nw + 1;
+
+          // get surfaces to each neighbor:
+          float nw = (ix_se - ix) * (iy_se - iy);
+          float ne = (ix - ix_sw) * (iy_sw - iy);
+          float sw = (ix_ne - ix) * (iy - iy_ne);
+          float se = (ix - ix_nw) * (iy - iy_nw);
+
+          // calculate bilinear weighted pixel value and set output pixel
+          const float *inp_ptr_NC = inp_ptr_N;
+          float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+          for (int64_t c = 0; c < C;
+               ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+            auto res = static_cast<float>(0);
+            if (within_bounds_2d(iy_nw, ix_nw, inp_H, inp_W)) {
+              res += inp_ptr_NC[iy_nw * inp_sH + ix_nw * inp_sW] * nw;
+            }
+            if (within_bounds_2d(iy_ne, ix_ne, inp_H, inp_W)) {
+              res += inp_ptr_NC[iy_ne * inp_sH + ix_ne * inp_sW] * ne;
+            }
+            if (within_bounds_2d(iy_sw, ix_sw, inp_H, inp_W)) {
+              res += inp_ptr_NC[iy_sw * inp_sH + ix_sw * inp_sW] * sw;
+            }
+            if (within_bounds_2d(iy_se, ix_se, inp_H, inp_W)) {
+              res += inp_ptr_NC[iy_se * inp_sH + ix_se * inp_sW] * se;
+            }
+            *out_ptr_NCHW = res;
+          }
+        } else if (interpolation_mode == GridSamplerInterpolation::Nearest) {
+          int64_t ix_nearest = static_cast<int64_t>(std::nearbyint(ix));
+          int64_t iy_nearest = static_cast<int64_t>(std::nearbyint(iy));
+
+          // assign nearest neighbor pixel value to output pixel
+          float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+          const float *inp_ptr_NC = inp_ptr_N;
+          for (int64_t c = 0; c < C;
+               ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+            if (within_bounds_2d(iy_nearest, ix_nearest, inp_H, inp_W)) {
+              *out_ptr_NCHW =
+                  inp_ptr_NC[iy_nearest * inp_sH + ix_nearest * inp_sW];
+            } else {
+              *out_ptr_NCHW = static_cast<float>(0);
+            }
+          }
+        } else if (interpolation_mode == GridSamplerInterpolation::Bicubic) {
+          // grid_sampler_compute_source_index will "clip the value" of idx
+          // depends on the padding,
+          // which would cause calculation to be wrong,
+          // for example x = -0.1 -> ix = 0 for zero padding, but in bicubic ix
+          // = floor(x) = -1
+          // There would be more problem in reflection padding, since the -1 and
+          // +1 direction is not fixed in boundary condition
+          ix = grid_sampler_unnormalize(x, inp_W, align_corners);
+          iy = grid_sampler_unnormalize(y, inp_H, align_corners);
+
+          float ix_nw = std::floor(ix);
+          float iy_nw = std::floor(iy);
+
+          const float tx = ix - ix_nw;
+          const float ty = iy - iy_nw;
+
+          const float *inp_ptr_NC = inp_ptr_N;
+          float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+          for (int64_t c = 0; c < C;
+               ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+            float coefficients[4];
+
+            // Interpolate 4 values in the x direction
+            for (int64_t i = 0; i < 4; ++i) {
+              coefficients[i] = cubic_interp1d<float>(
+                  get_value_bounded<float>(inp_ptr_NC, ix_nw - 1, iy_nw - 1 + i,
+                                           inp_W, inp_H, inp_sW, inp_sH,
+                                           padding_mode, align_corners),
+                  get_value_bounded<float>(inp_ptr_NC, ix_nw + 0, iy_nw - 1 + i,
+                                           inp_W, inp_H, inp_sW, inp_sH,
+                                           padding_mode, align_corners),
+                  get_value_bounded<float>(inp_ptr_NC, ix_nw + 1, iy_nw - 1 + i,
+                                           inp_W, inp_H, inp_sW, inp_sH,
+                                           padding_mode, align_corners),
+                  get_value_bounded<float>(inp_ptr_NC, ix_nw + 2, iy_nw - 1 + i,
+                                           inp_W, inp_H, inp_sW, inp_sH,
+                                           padding_mode, align_corners),
+                  tx);
+            }
+
+            // Interpolate in the y direction
+            *out_ptr_NCHW =
+                cubic_interp1d<float>(coefficients[0], coefficients[1],
+                                      coefficients[2], coefficients[3], ty);
+          }
+        }
+      }
+    }
+  }
+}
+
+
+torch::Tensor flow_warp(torch::Tensor X,torch::Tensor flow) {
+
+  int64_t N = X.size(0);
+  int64_t C = X.size(1);
+  int64_t inp_H = X.size(2);
+  int64_t inp_W = X.size(3);
+  int64_t out_H = flow.size(1);
+  int64_t out_W = flow.size(2);
+  auto flow_new = torch::zeros({flow.size(0),flow.size(1),flow.size(2),flow.size(3)}, at::kFloat);
+  flow_new = flow_new + flow;
+  // auto flow_clone = torch::clone(flow); // torch::clone 
+  float* input_data = X.data_ptr<float>();
+  float* flow_data = flow_new.data_ptr<float>();
+  auto output = torch::zeros({N,C,inp_H,inp_W}, at::kFloat);
+  // flow warp prehead
+  if(inp_H != out_H || inp_W != out_W){
+    std::cout << "The spatial sizes of input ("<<inp_H<<","<<inp_W<<")"<<" and flow ("<<out_H<<","<<out_W<<") are not the same."<<std::endl;
+    return output;
+  }
+
+  int64_t flow_sN = flow.size(1) * flow.size(2) * flow.size(3);//flow_dims[1] * flow_dims[2] * flow_dims[3];
+  int64_t flow_sH = flow.size(2)*flow.size(3);//flow_dims[2] * flow_dims[3];
+  int64_t flow_sW = flow.size(3); //flow_dims[3];
+  int64_t flow_sCoor = 1;
+
+  for (int64_t n = 0; n < N; ++n) {
+    float *flow_ptr_N = flow_data + n * flow_sN;
+    for (int64_t h = 0; h < out_H; ++h) {
+      for (int64_t w = 0; w < out_W; ++w) {
+        float *flow_ptr_NHW = flow_ptr_N + h * flow_sH + w * flow_sW;
+        float * flow_ptr_NHW_x = flow_ptr_NHW;
+        float * flow_ptr_NHW_y = &flow_ptr_NHW[flow_sCoor]; // flow_ptr_NHW + flow_sCoor;
+        *flow_ptr_NHW_x = 2.0 * (*flow_ptr_NHW_x + w) / MAX(out_W - 1, 1) - 1.0;
+        *flow_ptr_NHW_y = 2.0 *(*flow_ptr_NHW_y + h) / MAX(out_H - 1, 1) - 1.0;
+      }
+      // std::cout<<std::endl;
+    }
+  }
+  
+  
+  
+  const float * grid_data = flow_data;
+
+  // bilinear grid sample
+  float* out_ptr = output.data_ptr<float>();
+  const bool align_corners = true;
+  const int64_t padding_mode = 0;  // padding_mode: zeros
+  const int64_t interpolation_mode = 0; // interpolation_mode: bilinear
+  
+  int64_t grid_H = flow.size(1),grid_W = flow.size(2),grid_C = flow.size(3);
+  int64_t out_C = C;
+  int64_t inp_sN = C * inp_H * inp_W;//input_dims[1] * input_dims[2] * input_dims[3];
+  int64_t inp_sC = inp_H * inp_W;//input_dims[2] * input_dims[3];
+  int64_t inp_sH = inp_W;//input_dims[3];
+  int64_t inp_sW = 1;
+  int64_t grid_sN = grid_H * grid_W * grid_C;//grid_dims[1] * grid_dims[2] * grid_dims[3];
+  int64_t grid_sH = grid_W * grid_C;//grid_dims[2] * grid_dims[3];
+  int64_t grid_sW = grid_C;//grid_dims[3];
+  int64_t grid_sCoor = 1;
+  int64_t out_sN = out_C * out_H * out_W;//output_dims[1] * output_dims[2] * output_dims[3];
+  int64_t out_sC = out_H * out_W;//output_dims[2] * output_dims[3];
+  int64_t out_sH = out_W;//output_dims[3];
+  int64_t out_sW = 1;
+
+    // loop over each output pixel
+  for (int64_t n = 0; n < N; ++n) {
+    const float *grid_ptr_N = grid_data + n * grid_sN;
+    const float *inp_ptr_N = input_data + n * inp_sN;
+    for (int64_t h = 0; h < out_H; ++h) {
+      for (int64_t w = 0; w < out_W; ++w) {
+        const float *grid_ptr_NHW = grid_ptr_N + h * grid_sH + w * grid_sW;
+        float x = *grid_ptr_NHW;
+        float y = grid_ptr_NHW[grid_sCoor];
+
+        float ix = grid_sampler_compute_source_index(x, inp_W, padding_mode,
+                                                     align_corners);
+        float iy = grid_sampler_compute_source_index(y, inp_H, padding_mode,
+                                                     align_corners);
+
+        if (interpolation_mode == GridSamplerInterpolation::Bilinear) {
+          // get corner pixel values from (x, y)
+          // for 4d, we use north-east-south-west
+          int64_t ix_nw = static_cast<int64_t>(std::floor(ix));
+          int64_t iy_nw = static_cast<int64_t>(std::floor(iy));
+
+          int64_t ix_ne = ix_nw + 1;
+          int64_t iy_ne = iy_nw;
+
+          int64_t ix_sw = ix_nw;
+          int64_t iy_sw = iy_nw + 1;
+
+          int64_t ix_se = ix_nw + 1;
+          int64_t iy_se = iy_nw + 1;
+
+          // get surfaces to each neighbor:
+          float nw = (ix_se - ix) * (iy_se - iy);
+          float ne = (ix - ix_sw) * (iy_sw - iy);
+          float sw = (ix_ne - ix) * (iy - iy_ne);
+          float se = (ix - ix_nw) * (iy - iy_nw);
+
+          // calculate bilinear weighted pixel value and set output pixel
+          const float *inp_ptr_NC = inp_ptr_N;
+          float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+          for (int64_t c = 0; c < C;
+               ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+            auto res = static_cast<float>(0);
+            if (within_bounds_2d(iy_nw, ix_nw, inp_H, inp_W)) {
+              res += inp_ptr_NC[iy_nw * inp_sH + ix_nw * inp_sW] * nw;
+            }
+            if (within_bounds_2d(iy_ne, ix_ne, inp_H, inp_W)) {
+              res += inp_ptr_NC[iy_ne * inp_sH + ix_ne * inp_sW] * ne;
+            }
+            if (within_bounds_2d(iy_sw, ix_sw, inp_H, inp_W)) {
+              res += inp_ptr_NC[iy_sw * inp_sH + ix_sw * inp_sW] * sw;
+            }
+            if (within_bounds_2d(iy_se, ix_se, inp_H, inp_W)) {
+              res += inp_ptr_NC[iy_se * inp_sH + ix_se * inp_sW] * se;
+            }
+            *out_ptr_NCHW = res;
+          }
+        } else if (interpolation_mode == GridSamplerInterpolation::Nearest) {
+          int64_t ix_nearest = static_cast<int64_t>(std::nearbyint(ix));
+          int64_t iy_nearest = static_cast<int64_t>(std::nearbyint(iy));
+
+          // assign nearest neighbor pixel value to output pixel
+          float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+          const float *inp_ptr_NC = inp_ptr_N;
+          for (int64_t c = 0; c < C;
+               ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+            if (within_bounds_2d(iy_nearest, ix_nearest, inp_H, inp_W)) {
+              *out_ptr_NCHW =
+                  inp_ptr_NC[iy_nearest * inp_sH + ix_nearest * inp_sW];
+            } else {
+              *out_ptr_NCHW = static_cast<float>(0);
+            }
+          }
+        } else if (interpolation_mode == GridSamplerInterpolation::Bicubic) {
+          // grid_sampler_compute_source_index will "clip the value" of idx
+          // depends on the padding,
+          // which would cause calculation to be wrong,
+          // for example x = -0.1 -> ix = 0 for zero padding, but in bicubic ix
+          // = floor(x) = -1
+          // There would be more problem in reflection padding, since the -1 and
+          // +1 direction is not fixed in boundary condition
+          ix = grid_sampler_unnormalize(x, inp_W, align_corners);
+          iy = grid_sampler_unnormalize(y, inp_H, align_corners);
+
+          float ix_nw = std::floor(ix);
+          float iy_nw = std::floor(iy);
+
+          const float tx = ix - ix_nw;
+          const float ty = iy - iy_nw;
+
+          const float *inp_ptr_NC = inp_ptr_N;
+          float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
+          for (int64_t c = 0; c < C;
+               ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
+            float coefficients[4];
+
+            // Interpolate 4 values in the x direction
+            for (int64_t i = 0; i < 4; ++i) {
+              coefficients[i] = cubic_interp1d<float>(
+                  get_value_bounded<float>(inp_ptr_NC, ix_nw - 1, iy_nw - 1 + i,
+                                           inp_W, inp_H, inp_sW, inp_sH,
+                                           padding_mode, align_corners),
+                  get_value_bounded<float>(inp_ptr_NC, ix_nw + 0, iy_nw - 1 + i,
+                                           inp_W, inp_H, inp_sW, inp_sH,
+                                           padding_mode, align_corners),
+                  get_value_bounded<float>(inp_ptr_NC, ix_nw + 1, iy_nw - 1 + i,
+                                           inp_W, inp_H, inp_sW, inp_sH,
+                                           padding_mode, align_corners),
+                  get_value_bounded<float>(inp_ptr_NC, ix_nw + 2, iy_nw - 1 + i,
+                                           inp_W, inp_H, inp_sW, inp_sH,
+                                           padding_mode, align_corners),
+                  tx);
+            }
+
+            // Interpolate in the y direction
+            *out_ptr_NCHW =
+                cubic_interp1d<float>(coefficients[0], coefficients[1],
+                                      coefficients[2], coefficients[3], ty);
+          }
+        }
+      }
+    }
+  }
+
+  return output;
+}
+static auto registry =
+  torch::RegisterOperators("custom_op_namespace::flow_warp", &flow_warp);
+
diff --git a/vsr_opt/flow_warp_pytorch_op/setup.py b/vsr_opt/flow_warp_pytorch_op/setup.py
new file mode 100755
index 0000000000..45b204273a
--- /dev/null
+++ b/vsr_opt/flow_warp_pytorch_op/setup.py
@@ -0,0 +1,10 @@
+# Copyright (C) 2022 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+
+from setuptools import setup
+from torch.utils import cpp_extension
+
+setup(name='flow_warp',
+      ext_modules=[cpp_extension.CppExtension('flow_warp', ['flow_warp_op.cpp'])],
+      license='Apache License v2.0',
+      cmdclass={'build_ext': cpp_extension.BuildExtension})
diff --git a/vsr_opt/mmedit/__init__.py b/vsr_opt/mmedit/__init__.py
new file mode 100755
index 0000000000..1cf1f392fe
--- /dev/null
+++ b/vsr_opt/mmedit/__init__.py
@@ -0,0 +1,35 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import mmcv
+
+from .version import __version__, version_info
+
+try:
+    from mmcv.utils import digit_version
+except ImportError:
+
+    def digit_version(version_str):
+        digit_ver = []
+        for x in version_str.split('.'):
+            if x.isdigit():
+                digit_ver.append(int(x))
+            elif x.find('rc') != -1:
+                patch_version = x.split('rc')
+                digit_ver.append(int(patch_version[0]) - 1)
+                digit_ver.append(int(patch_version[1]))
+        return digit_ver
+
+
+MMCV_MIN = '1.3.13'
+MMCV_MAX = '1.6'
+
+mmcv_min_version = digit_version(MMCV_MIN)
+mmcv_max_version = digit_version(MMCV_MAX)
+mmcv_version = digit_version(mmcv.__version__)
+
+
+assert (mmcv_min_version <= mmcv_version <= mmcv_max_version), \
+    f'mmcv=={mmcv.__version__} is used but incompatible. ' \
+    f'Please install mmcv-full>={mmcv_min_version}, <={mmcv_max_version}.'
+
+__all__ = ['__version__', 'version_info']
diff --git a/vsr_opt/mmedit/models/__init__.py b/vsr_opt/mmedit/models/__init__.py
new file mode 100755
index 0000000000..dad4e8f942
--- /dev/null
+++ b/vsr_opt/mmedit/models/__init__.py
@@ -0,0 +1,16 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+from .backbones import *  # noqa: F401, F403
+from .base import BaseModel
+from .builder import (build, build_backbone, build_component, build_loss,
+                      build_model)
+from .common import *  # noqa: F401, F403
+from .losses import *  # noqa: F401, F403
+from .registry import BACKBONES, COMPONENTS, LOSSES, MODELS
+from .restorers import BasicRestorer
+
+__all__ = [
+    'BaseModel', 'BasicRestorer', 'build',
+    'build_backbone', 'build_component', 'build_loss', 'build_model',
+    'BACKBONES', 'COMPONENTS', 'LOSSES', 'MODELS'
+]
diff --git a/vsr_opt/mmedit/models/backbones/__init__.py b/vsr_opt/mmedit/models/backbones/__init__.py
new file mode 100755
index 0000000000..b713ebe093
--- /dev/null
+++ b/vsr_opt/mmedit/models/backbones/__init__.py
@@ -0,0 +1,7 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+from .sr_backbones import (BasicVSRNet)
+
+__all__ = [
+    'BasicVSRNet'
+]
diff --git a/vsr_opt/mmedit/models/backbones/sr_backbones/__init__.py b/vsr_opt/mmedit/models/backbones/sr_backbones/__init__.py
new file mode 100755
index 0000000000..5d41810137
--- /dev/null
+++ b/vsr_opt/mmedit/models/backbones/sr_backbones/__init__.py
@@ -0,0 +1,8 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+from .basicvsr_net import BasicVSRNet
+
+
+__all__ = [
+    'BasicVSRNet',
+]
\ No newline at end of file
diff --git a/vsr_opt/mmedit/models/backbones/sr_backbones/basicvsr_net.py b/vsr_opt/mmedit/models/backbones/sr_backbones/basicvsr_net.py
new file mode 100755
index 0000000000..12917b8dc8
--- /dev/null
+++ b/vsr_opt/mmedit/models/backbones/sr_backbones/basicvsr_net.py
@@ -0,0 +1,433 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from mmcv.cnn import ConvModule
+from mmcv.runner import load_checkpoint
+
+from mmedit.models.common import (PixelShufflePack, ResidualBlockNoBN,
+                                  flow_warp, make_layer)
+from mmedit.models.registry import BACKBONES
+# from mmedit.utils import get_root_logger
+
+
+@BACKBONES.register_module()
+class BasicVSRNet(nn.Module):
+    """BasicVSR network structure for video super-resolution.
+
+    Support only x4 upsampling.
+    Paper:
+        BasicVSR: The Search for Essential Components in Video Super-Resolution
+        and Beyond, CVPR, 2021
+
+    Args:
+        mid_channels (int): Channel number of the intermediate features.
+            Default: 64.
+        num_blocks (int): Number of residual blocks in each propagation branch.
+            Default: 30.
+        spynet_pretrained (str): Pre-trained model path of SPyNet.
+            Default: None.
+    """
+
+    def __init__(self, scale=4, mid_channels=64, num_blocks=30, spynet_pretrained=None, custom_op=False):
+
+        super().__init__()
+
+        self.scale = scale
+        self.mid_channels = mid_channels
+        self.custom_op = custom_op  # whether to use custom op
+
+        # optical flow network for feature alignment
+        self.spynet = SPyNet(pretrained=spynet_pretrained, custom_op=custom_op)
+
+        # propagation branches
+        self.backward_resblocks = ResidualBlocksWithInputConv(
+            mid_channels + 3, mid_channels, num_blocks)
+        self.forward_resblocks = ResidualBlocksWithInputConv(
+            mid_channels + 3, mid_channels, num_blocks)
+
+        # upsample
+        self.fusion = nn.Conv2d(
+            mid_channels * 2, mid_channels, 1, 1, 0, bias=True)
+        self.upsample1 = PixelShufflePack(
+            mid_channels, mid_channels, 2, upsample_kernel=3)
+        self.upsample2 = PixelShufflePack(
+            mid_channels, 64, 2, upsample_kernel=3)
+        self.conv_hr = nn.Conv2d(64, 64, 3, 1, 1)
+        self.conv_last = nn.Conv2d(64, 3, 3, 1, 1)
+        self.img_upsample = nn.Upsample(
+            scale_factor=scale, mode='bilinear', align_corners=False)
+
+        # activation function
+        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)
+
+    def check_if_mirror_extended(self, lrs):
+        """Check whether the input is a mirror-extended sequence.
+
+        If mirror-extended, the i-th (i=0, ..., t-1) frame is equal to the
+        (t-1-i)-th frame.
+
+        Args:
+            lrs (tensor): Input LR images with shape (n, t, c, h, w)
+        """
+
+        self.is_mirror_extended = False
+        if lrs.size(1) % 2 == 0:
+            lrs_1, lrs_2 = torch.chunk(lrs, 2, dim=1)
+            if torch.norm(lrs_1 - lrs_2.flip(1)) == 0:
+                self.is_mirror_extended = True
+
+    def compute_flow(self, lrs):
+        """Compute optical flow using SPyNet for feature warping.
+
+        Note that if the input is an mirror-extended sequence, 'flows_forward'
+        is not needed, since it is equal to 'flows_backward.flip(1)'.
+
+        Args:
+            lrs (tensor): Input LR images with shape (n, t, c, h, w)
+
+        Return:
+            tuple(Tensor): Optical flow. 'flows_forward' corresponds to the
+                flows used for forward-time propagation (current to previous).
+                'flows_backward' corresponds to the flows used for
+                backward-time propagation (current to next).
+        """
+
+        n, t, c, h, w = lrs.size()
+        lrs_1 = lrs[:, :-1, :, :, :].reshape(-1, c, h, w)
+        lrs_2 = lrs[:, 1:, :, :, :].reshape(-1, c, h, w)
+
+        flows_backward = self.spynet(lrs_1, lrs_2).view(n, t - 1, 2, h, w)
+
+        if self.is_mirror_extended:  # flows_forward = flows_backward.flip(1)
+            flows_forward = None
+        else:
+            flows_forward = self.spynet(lrs_2, lrs_1).view(n, t - 1, 2, h, w)
+
+        return flows_forward, flows_backward
+
+    def forward(self, lrs):
+        """Forward function for BasicVSR.
+
+        Args:
+            lrs (Tensor): Input LR sequence with shape (n, t, c, h, w).
+
+        Returns:
+            Tensor: Output HR sequence with shape (n, t, c, 4h, 4w).
+        """
+
+        n, t, c, h, w = lrs.size()
+        assert h >= 64 and w >= 64, (
+            'The height and width of inputs should be at least 64, '
+            f'but got {h} and {w}.')
+
+        # check whether the input is an extended sequence
+        self.check_if_mirror_extended(lrs)
+
+        # compute optical flow
+        flows_forward, flows_backward = self.compute_flow(lrs)
+
+        # backward-time propgation
+        outputs = []
+        feat_prop = lrs.new_zeros(n, self.mid_channels, h, w)
+        for i in range(t - 1, -1, -1):
+            if i < t - 1:  # no warping required for the last timestep
+                flow = flows_backward[:, i, :, :, :]
+                feat_prop = flow_warp(feat_prop, flow.permute(0, 2, 3, 1), custom_op=self.custom_op)
+
+            feat_prop = torch.cat([lrs[:, i, :, :, :], feat_prop], dim=1)
+            feat_prop = self.backward_resblocks(feat_prop)
+
+            outputs.append(feat_prop)
+        outputs = outputs[::-1]
+
+        # forward-time propagation and upsampling
+        feat_prop = torch.zeros_like(feat_prop)
+        for i in range(0, t):
+            lr_curr = lrs[:, i, :, :, :]
+            if i > 0:  # no warping required for the first timestep
+                if flows_forward is not None:
+                    flow = flows_forward[:, i - 1, :, :, :]
+                else:
+                    flow = flows_backward[:, -i, :, :, :]
+                feat_prop = flow_warp(feat_prop, flow.permute(0, 2, 3, 1), custom_op=self.custom_op)
+
+            feat_prop = torch.cat([lr_curr, feat_prop], dim=1)
+            feat_prop = self.forward_resblocks(feat_prop)
+
+            # upsampling given the backward and forward features
+            out = torch.cat([outputs[i], feat_prop], dim=1)
+            out = self.lrelu(self.fusion(out))
+            if self.scale == 4:
+                out = self.lrelu(self.upsample1(out))
+            out = self.lrelu(self.upsample2(out))
+            out = self.lrelu(self.conv_hr(out))
+            out = self.conv_last(out)
+            base = self.img_upsample(lr_curr)
+            out += base
+            outputs[i] = out
+
+        return torch.stack(outputs, dim=1)
+
+    def init_weights(self, pretrained=None, strict=True):
+        """Init weights for models.
+
+        Args:
+            pretrained (str, optional): Path for pretrained weights. If given
+                None, pretrained weights will not be loaded. Defaults: None.
+            strict (boo, optional): Whether strictly load the pretrained model.
+                Defaults to True.
+        """
+        # if isinstance(pretrained, str):
+        #     logger = get_root_logger()
+        #     load_checkpoint(self, pretrained, strict=strict, logger=logger)
+        # elif pretrained is not None:
+        #     raise TypeError(f'"pretrained" must be a str or None. '
+        #                     f'But received {type(pretrained)}.')
+
+
+class ResidualBlocksWithInputConv(nn.Module):
+    """Residual blocks with a convolution in front.
+
+    Args:
+        in_channels (int): Number of input channels of the first conv.
+        out_channels (int): Number of channels of the residual blocks.
+            Default: 64.
+        num_blocks (int): Number of residual blocks. Default: 30.
+    """
+
+    def __init__(self, in_channels, out_channels=64, num_blocks=30):
+        super().__init__()
+
+        main = []
+
+        # a convolution used to match the channels of the residual blocks
+        main.append(nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=True))
+        main.append(nn.LeakyReLU(negative_slope=0.1, inplace=True))
+
+        # residual blocks
+        main.append(
+            make_layer(
+                ResidualBlockNoBN, num_blocks, mid_channels=out_channels))
+
+        self.main = nn.Sequential(*main)
+
+    def forward(self, feat):
+        """
+        Forward function for ResidualBlocksWithInputConv.
+
+        Args:
+            feat (Tensor): Input feature with shape (n, in_channels, h, w)
+
+        Returns:
+            Tensor: Output feature with shape (n, out_channels, h, w)
+        """
+        return self.main(feat)
+
+
+class SPyNet(nn.Module):
+    """SPyNet network structure.
+
+    The difference to the SPyNet in [tof.py] is that
+        1. more SPyNetBasicModule is used in this version, and
+        2. no batch normalization is used in this version.
+
+    Paper:
+        Optical Flow Estimation using a Spatial Pyramid Network, CVPR, 2017
+
+    Args:
+        pretrained (str): path for pre-trained SPyNet. Default: None.
+    """
+
+    def __init__(self, pretrained, custom_op=False):
+        super().__init__()
+
+        self.basic_module = nn.ModuleList(
+            [SPyNetBasicModule() for _ in range(6)])
+        self.custom_op = custom_op
+
+        # if isinstance(pretrained, str):
+        #     logger = get_root_logger()
+        #     load_checkpoint(self, pretrained, strict=True, logger=logger)
+        # elif pretrained is not None:
+        #     raise TypeError('[pretrained] should be str or None, '
+        #                     f'but got {type(pretrained)}.')
+
+        self.register_buffer(
+            'mean',
+            torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
+        self.register_buffer(
+            'std',
+            torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))
+
+    def compute_flow(self, ref, supp):
+        """Compute flow from ref to supp.
+
+        Note that in this function, the images are already resized to a
+        multiple of 32.
+
+        Args:
+            ref (Tensor): Reference image with shape of (n, 3, h, w).
+            supp (Tensor): Supporting image with shape of (n, 3, h, w).
+
+        Returns:
+            Tensor: Estimated optical flow: (n, 2, h, w).
+        """
+        n, _, h, w = ref.size()
+
+        # normalize the input images
+        ref = [(ref - self.mean) / self.std]
+        supp = [(supp - self.mean) / self.std]
+
+        # generate downsampled frames
+        for level in range(5):
+            ref.append(
+                F.avg_pool2d(
+                    input=ref[-1],
+                    kernel_size=2,
+                    stride=2,
+                    count_include_pad=False))
+            supp.append(
+                F.avg_pool2d(
+                    input=supp[-1],
+                    kernel_size=2,
+                    stride=2,
+                    count_include_pad=False))
+        ref = ref[::-1]
+        supp = supp[::-1]
+
+        # flow computation
+        flow = ref[0].new_zeros(n, 2, h // 32, w // 32)
+        for level in range(len(ref)):
+            if level == 0:
+                flow_up = flow
+            else:
+                flow_up = F.interpolate(
+                    input=flow,
+                    scale_factor=2,
+                    mode='bilinear',
+                    align_corners=True) * 2.0
+
+            # add the residue to the upsampled flow
+            flow = flow_up + self.basic_module[level](
+                torch.cat([
+                    ref[level],
+                    flow_warp(
+                        supp[level],
+                        flow_up.permute(0, 2, 3, 1),
+                        padding_mode='border',
+                        custom_op=self.custom_op), flow_up
+                ], 1))
+
+        return flow
+
+    def forward(self, ref, supp):
+        """Forward function of SPyNet.
+
+        This function computes the optical flow from ref to supp.
+
+        Args:
+            ref (Tensor): Reference image with shape of (n, 3, h, w).
+            supp (Tensor): Supporting image with shape of (n, 3, h, w).
+
+        Returns:
+            Tensor: Estimated optical flow: (n, 2, h, w).
+        """
+
+        # upsize to a multiple of 32
+        h, w = ref.shape[2:4]
+        w_up = w if (w % 32) == 0 else 32 * (w // 32 + 1)
+        h_up = h if (h % 32) == 0 else 32 * (h // 32 + 1)
+        ref = F.interpolate(
+            input=ref, size=(h_up, w_up), mode='bilinear', align_corners=False)
+        supp = F.interpolate(
+            input=supp,
+            size=(h_up, w_up),
+            mode='bilinear',
+            align_corners=False)
+
+        # compute flow, and resize back to the original resolution
+        flow = F.interpolate(
+            input=self.compute_flow(ref, supp),
+            size=(h, w),
+            mode='bilinear',
+            align_corners=False)
+
+        # adjust the flow values
+        if self.custom_op is False:
+            flow[:, 0, :, :] *= float(w) / float(w_up)
+            flow[:, 1, :, :] *= float(h) / float(h_up)
+        else:
+            w_scale = float(w) / float(w_up)
+            h_scale = float(h) / float(h_up)
+            flow_0 = flow[:, 0, :, :]*w_scale
+            flow_1 = flow[:, 1, :, :]*h_scale
+            flow = torch.stack((flow_0,flow_1),dim=1)
+
+        return flow
+
+
+class SPyNetBasicModule(nn.Module):
+    """Basic Module for SPyNet.
+
+    Paper:
+        Optical Flow Estimation using a Spatial Pyramid Network, CVPR, 2017
+    """
+
+    def __init__(self):
+        super().__init__()
+
+        self.basic_module = nn.Sequential(
+            ConvModule(
+                in_channels=8,
+                out_channels=32,
+                kernel_size=7,
+                stride=1,
+                padding=3,
+                norm_cfg=None,
+                act_cfg=dict(type='ReLU')),
+            ConvModule(
+                in_channels=32,
+                out_channels=64,
+                kernel_size=7,
+                stride=1,
+                padding=3,
+                norm_cfg=None,
+                act_cfg=dict(type='ReLU')),
+            ConvModule(
+                in_channels=64,
+                out_channels=32,
+                kernel_size=7,
+                stride=1,
+                padding=3,
+                norm_cfg=None,
+                act_cfg=dict(type='ReLU')),
+            ConvModule(
+                in_channels=32,
+                out_channels=16,
+                kernel_size=7,
+                stride=1,
+                padding=3,
+                norm_cfg=None,
+                act_cfg=dict(type='ReLU')),
+            ConvModule(
+                in_channels=16,
+                out_channels=2,
+                kernel_size=7,
+                stride=1,
+                padding=3,
+                norm_cfg=None,
+                act_cfg=None))
+
+    def forward(self, tensor_input):
+        """
+        Args:
+            tensor_input (Tensor): Input tensor with shape (b, 8, h, w).
+                8 channels contain:
+                [reference image (3), neighbor image (3), initial flow (2)].
+
+        Returns:
+            Tensor: Refined flow with shape (b, 2, h, w)
+        """
+        return self.basic_module(tensor_input)
diff --git a/vsr_opt/mmedit/models/base.py b/vsr_opt/mmedit/models/base.py
new file mode 100755
index 0000000000..590b13d034
--- /dev/null
+++ b/vsr_opt/mmedit/models/base.py
@@ -0,0 +1,106 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+from abc import ABCMeta, abstractmethod
+from collections import OrderedDict
+
+import torch
+import torch.nn as nn
+
+
+class BaseModel(nn.Module, metaclass=ABCMeta):
+    """Base model.
+
+    All models should subclass it.
+    All subclass should overwrite:
+
+        ``init_weights``, supporting to initialize models.
+
+        ``forward_train``, supporting to forward when training.
+
+        ``forward_test``, supporting to forward when testing.
+
+        ``train_step``, supporting to train one step when training.
+    """
+
+    @abstractmethod
+    def init_weights(self):
+        """Abstract method for initializing weight.
+
+        All subclass should overwrite it.
+        """
+
+    @abstractmethod
+    def forward_train(self, imgs, labels):
+        """Abstract method for training forward.
+
+        All subclass should overwrite it.
+        """
+
+    @abstractmethod
+    def forward_test(self, imgs):
+        """Abstract method for testing forward.
+
+        All subclass should overwrite it.
+        """
+
+    def forward(self, imgs, labels, test_mode, **kwargs):
+        """Forward function for base model.
+
+        Args:
+            imgs (Tensor): Input image(s).
+            labels (Tensor): Ground-truth label(s).
+            test_mode (bool): Whether in test mode.
+            kwargs (dict): Other arguments.
+
+        Returns:
+            Tensor: Forward results.
+        """
+
+        if test_mode:
+            return self.forward_test(imgs, **kwargs)
+
+        return self.forward_train(imgs, labels, **kwargs)
+
+    @abstractmethod
+    def train_step(self, data_batch, optimizer):
+        """Abstract method for one training step.
+
+        All subclass should overwrite it.
+        """
+
+    def val_step(self, data_batch, **kwargs):
+        """Abstract method for one validation step.
+
+        All subclass should overwrite it.
+        """
+        output = self.forward_test(**data_batch, **kwargs)
+        return output
+
+    def parse_losses(self, losses):
+        """Parse losses dict for different loss variants.
+
+        Args:
+            losses (dict): Loss dict.
+
+        Returns:
+            loss (float): Sum of the total loss.
+            log_vars (dict): loss dict for different variants.
+        """
+        log_vars = OrderedDict()
+        for loss_name, loss_value in losses.items():
+            if isinstance(loss_value, torch.Tensor):
+                log_vars[loss_name] = loss_value.mean()
+            elif isinstance(loss_value, list):
+                log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)
+            else:
+                raise TypeError(
+                    f'{loss_name} is not a tensor or list of tensors')
+
+        loss = sum(_value for _key, _value in log_vars.items()
+                   if 'loss' in _key)
+
+        log_vars['loss'] = loss
+        for name in log_vars:
+            log_vars[name] = log_vars[name].item()
+
+        return loss, log_vars
diff --git a/vsr_opt/mmedit/models/builder.py b/vsr_opt/mmedit/models/builder.py
new file mode 100755
index 0000000000..5961f4a2db
--- /dev/null
+++ b/vsr_opt/mmedit/models/builder.py
@@ -0,0 +1,61 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import torch.nn as nn
+from mmcv import build_from_cfg
+
+from .registry import BACKBONES, COMPONENTS, LOSSES, MODELS
+
+
+def build(cfg, registry, default_args=None):
+    """Build module function.
+
+    Args:
+        cfg (dict): Configuration for building modules.
+        registry (obj): ``registry`` object.
+        default_args (dict, optional): Default arguments. Defaults to None.
+    """
+    if isinstance(cfg, list):
+        modules = [
+            build_from_cfg(cfg_, registry, default_args) for cfg_ in cfg
+        ]
+        return nn.Sequential(*modules)
+
+    return build_from_cfg(cfg, registry, default_args)
+
+
+def build_backbone(cfg):
+    """Build backbone.
+
+    Args:
+        cfg (dict): Configuration for building backbone.
+    """
+    return build(cfg, BACKBONES)
+
+
+def build_component(cfg):
+    """Build component.
+
+    Args:
+        cfg (dict): Configuration for building component.
+    """
+    return build(cfg, COMPONENTS)
+
+
+def build_loss(cfg):
+    """Build loss.
+
+    Args:
+        cfg (dict): Configuration for building loss.
+    """
+    return build(cfg, LOSSES)
+
+
+def build_model(cfg, train_cfg=None, test_cfg=None):
+    """Build model.
+
+    Args:
+        cfg (dict): Configuration for building model.
+        train_cfg (dict): Training configuration. Default: None.
+        test_cfg (dict): Testing configuration. Default: None.
+    """
+    return build(cfg, MODELS, dict(train_cfg=train_cfg, test_cfg=test_cfg))
diff --git a/vsr_opt/mmedit/models/common/__init__.py b/vsr_opt/mmedit/models/common/__init__.py
new file mode 100755
index 0000000000..457f1c1aae
--- /dev/null
+++ b/vsr_opt/mmedit/models/common/__init__.py
@@ -0,0 +1,11 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+from .flow_warp import flow_warp
+from .sr_backbone_utils import (ResidualBlockNoBN, default_init_weights,
+                                make_layer)
+from .upsample import PixelShufflePack
+
+__all__ = [
+    'PixelShufflePack', 'default_init_weights',
+    'ResidualBlockNoBN', 'make_layer', 'flow_warp'
+]
diff --git a/vsr_opt/mmedit/models/common/flow_warp.py b/vsr_opt/mmedit/models/common/flow_warp.py
new file mode 100755
index 0000000000..57b99d0468
--- /dev/null
+++ b/vsr_opt/mmedit/models/common/flow_warp.py
@@ -0,0 +1,66 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import torch
+import torch.nn.functional as F
+from mmcv.ops.point_sample import bilinear_grid_sample
+
+
+def flow_warp(x,
+              flow,
+              interpolation='bilinear',
+              padding_mode='zeros',
+              align_corners=True,
+              custom_op=False):
+    """Warp an image or a feature map with optical flow.
+
+    Args:
+        x (Tensor): Tensor with size (n, c, h, w).
+        flow (Tensor): Tensor with size (n, h, w, 2). The last dimension is
+            a two-channel, denoting the width and height relative offsets.
+            Note that the values are not normalized to [-1, 1].
+        interpolation (str): Interpolation mode: 'nearest' or 'bilinear'.
+            Default: 'bilinear'.
+        padding_mode (str): Padding mode: 'zeros' or 'border' or 'reflection'.
+            Default: 'zeros'.
+        align_corners (bool): Whether align corners. Default: True.
+
+    Returns:
+        Tensor: Warped image or feature map.
+    """
+    if custom_op:
+        torch.ops.load_library(
+            "./flow_warp_pytorch_op/build/lib.linux-x86_64-cpython-39/flow_warp.cpython-39-x86_64-linux-gnu.so")
+        register_custom_op()
+        result = torch.ops.custom_op_namespace.flow_warp(x, flow)
+        return result
+
+    if x.size()[-2:] != flow.size()[1:3]:
+        raise ValueError(f'The spatial sizes of input ({x.size()[-2:]}) and '
+                         f'flow ({flow.size()[1:3]}) are not the same.')
+    _, _, h, w = x.size()
+    # create mesh grid
+    grid_y, grid_x = torch.meshgrid(torch.arange(0, h), torch.arange(0, w))
+    grid = torch.stack((grid_x, grid_y), 2).type_as(x)  # (h, w, 2)
+    grid.requires_grad = False
+
+    grid_flow = grid + flow
+    # scale grid_flow to [-1,1]
+    grid_flow_x = 2.0 * grid_flow[:, :, :, 0] / max(w - 1, 1) - 1.0
+    grid_flow_y = 2.0 * grid_flow[:, :, :, 1] / max(h - 1, 1) - 1.0
+    grid_flow = torch.stack((grid_flow_x, grid_flow_y), dim=3)
+    output = bilinear_grid_sample(x, grid_flow, align_corners=align_corners)
+    # output = F.grid_sample(
+    #     x,
+    #     grid_flow,
+    #     mode=interpolation,
+    #     padding_mode=padding_mode,
+    #     align_corners=align_corners)
+    return output
+
+
+def register_custom_op():
+    def my_flow_warp(g, input, flow):
+        return g.op("custom_op::flow_warp", input, flow)
+
+    from torch.onnx import register_custom_op_symbolic
+    register_custom_op_symbolic("custom_op_namespace::flow_warp", my_flow_warp, 11)
diff --git a/vsr_opt/mmedit/models/common/sr_backbone_utils.py b/vsr_opt/mmedit/models/common/sr_backbone_utils.py
new file mode 100755
index 0000000000..276b3e478b
--- /dev/null
+++ b/vsr_opt/mmedit/models/common/sr_backbone_utils.py
@@ -0,0 +1,98 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import torch.nn as nn
+from mmcv.cnn import constant_init, kaiming_init
+from mmcv.utils.parrots_wrapper import _BatchNorm
+
+
+def default_init_weights(module, scale=1):
+    """Initialize network weights.
+
+    Args:
+        modules (nn.Module): Modules to be initialized.
+        scale (float): Scale initialized weights, especially for residual
+            blocks.
+    """
+    for m in module.modules():
+        if isinstance(m, nn.Conv2d):
+            kaiming_init(m, a=0, mode='fan_in', bias=0)
+            m.weight.data *= scale
+        elif isinstance(m, nn.Linear):
+            kaiming_init(m, a=0, mode='fan_in', bias=0)
+            m.weight.data *= scale
+        elif isinstance(m, _BatchNorm):
+            constant_init(m.weight, val=1, bias=0)
+
+
+def make_layer(block, num_blocks, **kwarg):
+    """Make layers by stacking the same blocks.
+
+    Args:
+        block (nn.module): nn.module class for basic block.
+        num_blocks (int): number of blocks.
+
+    Returns:
+        nn.Sequential: Stacked blocks in nn.Sequential.
+    """
+    layers = []
+    for _ in range(num_blocks):
+        layers.append(block(**kwarg))
+    return nn.Sequential(*layers)
+
+
+class ResidualBlockNoBN(nn.Module):
+    """Residual block without BN.
+
+    It has a style of:
+
+    ::
+
+        ---Conv-ReLU-Conv-+-
+         |________________|
+
+    Args:
+        mid_channels (int): Channel number of intermediate features.
+            Default: 64.
+        res_scale (float): Used to scale the residual before addition.
+            Default: 1.0.
+    """
+
+    def __init__(self, mid_channels=64, res_scale=1.0):
+        super().__init__()
+        self.res_scale = res_scale
+        self.conv1 = nn.Conv2d(mid_channels, mid_channels, 3, 1, 1, bias=True)
+        self.conv2 = nn.Conv2d(mid_channels, mid_channels, 3, 1, 1, bias=True)
+
+        self.relu = nn.ReLU(inplace=True)
+
+        # if res_scale < 1.0, use the default initialization, as in EDSR.
+        # if res_scale = 1.0, use scaled kaiming_init, as in MSRResNet.
+        if res_scale == 1.0:
+            self.init_weights()
+
+    def init_weights(self):
+        """Initialize weights for ResidualBlockNoBN.
+
+        Initialization methods like `kaiming_init` are for VGG-style
+        modules. For modules with residual paths, using smaller std is
+        better for stability and performance. We empirically use 0.1.
+        See more details in "ESRGAN: Enhanced Super-Resolution Generative
+        Adversarial Networks"
+        """
+
+        for m in [self.conv1, self.conv2]:
+            default_init_weights(m, 0.1)
+
+    def forward(self, x):
+        """Forward function.
+
+        Args:
+            x (Tensor): Input tensor with shape (n, c, h, w).
+
+        Returns:
+            Tensor: Forward results.
+        """
+
+        identity = x
+        out = self.conv2(self.relu(self.conv1(x)))
+        return identity + out * self.res_scale
diff --git a/vsr_opt/mmedit/models/common/upsample.py b/vsr_opt/mmedit/models/common/upsample.py
new file mode 100755
index 0000000000..9f17723216
--- /dev/null
+++ b/vsr_opt/mmedit/models/common/upsample.py
@@ -0,0 +1,52 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import torch.nn as nn
+import torch.nn.functional as F
+
+from .sr_backbone_utils import default_init_weights
+
+
+class PixelShufflePack(nn.Module):
+    """ Pixel Shuffle upsample layer.
+
+    Args:
+        in_channels (int): Number of input channels.
+        out_channels (int): Number of output channels.
+        scale_factor (int): Upsample ratio.
+        upsample_kernel (int): Kernel size of Conv layer to expand channels.
+
+    Returns:
+        Upsampled feature map.
+    """
+
+    def __init__(self, in_channels, out_channels, scale_factor,
+                 upsample_kernel):
+        super().__init__()
+        self.in_channels = in_channels
+        self.out_channels = out_channels
+        self.scale_factor = scale_factor
+        self.upsample_kernel = upsample_kernel
+        self.upsample_conv = nn.Conv2d(
+            self.in_channels,
+            self.out_channels * scale_factor * scale_factor,
+            self.upsample_kernel,
+            padding=(self.upsample_kernel - 1) // 2)
+        self.init_weights()
+
+    def init_weights(self):
+        """Initialize weights for PixelShufflePack.
+        """
+        default_init_weights(self, 1)
+
+    def forward(self, x):
+        """Forward function for PixelShufflePack.
+
+        Args:
+            x (Tensor): Input tensor with shape (n, c, h, w).
+
+        Returns:
+            Tensor: Forward results.
+        """
+        x = self.upsample_conv(x)
+        x = F.pixel_shuffle(x, self.scale_factor)
+        return x
diff --git a/vsr_opt/mmedit/models/losses/__init__.py b/vsr_opt/mmedit/models/losses/__init__.py
new file mode 100755
index 0000000000..77a4758dee
--- /dev/null
+++ b/vsr_opt/mmedit/models/losses/__init__.py
@@ -0,0 +1,6 @@
+# Copyright (C) 2022 Intel Corporation
+from .pixelwise_loss import CharbonnierLoss
+
+__all__ = [
+    'CharbonnierLoss',
+]
diff --git a/vsr_opt/mmedit/models/losses/pixelwise_loss.py b/vsr_opt/mmedit/models/losses/pixelwise_loss.py
new file mode 100755
index 0000000000..189b848707
--- /dev/null
+++ b/vsr_opt/mmedit/models/losses/pixelwise_loss.py
@@ -0,0 +1,222 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from ..registry import LOSSES
+from .utils import masked_loss
+
+_reduction_modes = ['none', 'mean', 'sum']
+
+
+@masked_loss
+def l1_loss(pred, target):
+    """L1 loss.
+
+    Args:
+        pred (Tensor): Prediction Tensor with shape (n, c, h, w).
+        target ([type]): Target Tensor with shape (n, c, h, w).
+
+    Returns:
+        Tensor: Calculated L1 loss.
+    """
+    return F.l1_loss(pred, target, reduction='none')
+
+
+@masked_loss
+def mse_loss(pred, target):
+    """MSE loss.
+
+    Args:
+        pred (Tensor): Prediction Tensor with shape (n, c, h, w).
+        target ([type]): Target Tensor with shape (n, c, h, w).
+
+    Returns:
+        Tensor: Calculated MSE loss.
+    """
+    return F.mse_loss(pred, target, reduction='none')
+
+
+@masked_loss
+def charbonnier_loss(pred, target, eps=1e-12):
+    """Charbonnier loss.
+
+    Args:
+        pred (Tensor): Prediction Tensor with shape (n, c, h, w).
+        target ([type]): Target Tensor with shape (n, c, h, w).
+
+    Returns:
+        Tensor: Calculated Charbonnier loss.
+    """
+    return torch.sqrt((pred - target)**2 + eps)
+
+
+@LOSSES.register_module()
+class L1Loss(nn.Module):
+    """L1 (mean absolute error, MAE) loss.
+
+    Args:
+        loss_weight (float): Loss weight for L1 loss. Default: 1.0.
+        reduction (str): Specifies the reduction to apply to the output.
+            Supported choices are 'none' | 'mean' | 'sum'. Default: 'mean'.
+        sample_wise (bool): Whether calculate the loss sample-wise. This
+            argument only takes effect when `reduction` is 'mean' and `weight`
+            (argument of `forward()`) is not None. It will first reduce loss
+            with 'mean' per-sample, and then it means over all the samples.
+            Default: False.
+    """
+
+    def __init__(self, loss_weight=1.0, reduction='mean', sample_wise=False):
+        super().__init__()
+        if reduction not in ['none', 'mean', 'sum']:
+            raise ValueError(f'Unsupported reduction mode: {reduction}. '
+                             f'Supported ones are: {_reduction_modes}')
+
+        self.loss_weight = loss_weight
+        self.reduction = reduction
+        self.sample_wise = sample_wise
+
+    def forward(self, pred, target, weight=None, **kwargs):
+        """Forward Function.
+
+        Args:
+            pred (Tensor): of shape (N, C, H, W). Predicted tensor.
+            target (Tensor): of shape (N, C, H, W). Ground truth tensor.
+            weight (Tensor, optional): of shape (N, C, H, W). Element-wise
+                weights. Default: None.
+        """
+        return self.loss_weight * l1_loss(
+            pred,
+            target,
+            weight,
+            reduction=self.reduction,
+            sample_wise=self.sample_wise)
+
+
+@LOSSES.register_module()
+class MSELoss(nn.Module):
+    """MSE (L2) loss.
+
+    Args:
+        loss_weight (float): Loss weight for MSE loss. Default: 1.0.
+        reduction (str): Specifies the reduction to apply to the output.
+            Supported choices are 'none' | 'mean' | 'sum'. Default: 'mean'.
+        sample_wise (bool): Whether calculate the loss sample-wise. This
+            argument only takes effect when `reduction` is 'mean' and `weight`
+            (argument of `forward()`) is not None. It will first reduces loss
+            with 'mean' per-sample, and then it means over all the samples.
+            Default: False.
+    """
+
+    def __init__(self, loss_weight=1.0, reduction='mean', sample_wise=False):
+        super().__init__()
+        if reduction not in ['none', 'mean', 'sum']:
+            raise ValueError(f'Unsupported reduction mode: {reduction}. '
+                             f'Supported ones are: {_reduction_modes}')
+
+        self.loss_weight = loss_weight
+        self.reduction = reduction
+        self.sample_wise = sample_wise
+
+    def forward(self, pred, target, weight=None, **kwargs):
+        """Forward Function.
+
+        Args:
+            pred (Tensor): of shape (N, C, H, W). Predicted tensor.
+            target (Tensor): of shape (N, C, H, W). Ground truth tensor.
+            weight (Tensor, optional): of shape (N, C, H, W). Element-wise
+                weights. Default: None.
+        """
+        return self.loss_weight * mse_loss(
+            pred,
+            target,
+            weight,
+            reduction=self.reduction,
+            sample_wise=self.sample_wise)
+
+
+@LOSSES.register_module()
+class CharbonnierLoss(nn.Module):
+    """Charbonnier loss (one variant of Robust L1Loss, a differentiable
+    variant of L1Loss).
+
+    Described in "Deep Laplacian Pyramid Networks for Fast and Accurate
+        Super-Resolution".
+
+    Args:
+        loss_weight (float): Loss weight for L1 loss. Default: 1.0.
+        reduction (str): Specifies the reduction to apply to the output.
+            Supported choices are 'none' | 'mean' | 'sum'. Default: 'mean'.
+        sample_wise (bool): Whether calculate the loss sample-wise. This
+            argument only takes effect when `reduction` is 'mean' and `weight`
+            (argument of `forward()`) is not None. It will first reduces loss
+            with 'mean' per-sample, and then it means over all the samples.
+            Default: False.
+        eps (float): A value used to control the curvature near zero.
+            Default: 1e-12.
+    """
+
+    def __init__(self,
+                 loss_weight=1.0,
+                 reduction='mean',
+                 sample_wise=False,
+                 eps=1e-12):
+        super().__init__()
+        if reduction not in ['none', 'mean', 'sum']:
+            raise ValueError(f'Unsupported reduction mode: {reduction}. '
+                             f'Supported ones are: {_reduction_modes}')
+
+        self.loss_weight = loss_weight
+        self.reduction = reduction
+        self.sample_wise = sample_wise
+        self.eps = eps
+
+    def forward(self, pred, target, weight=None, **kwargs):
+        """Forward Function.
+
+        Args:
+            pred (Tensor): of shape (N, C, H, W). Predicted tensor.
+            target (Tensor): of shape (N, C, H, W). Ground truth tensor.
+            weight (Tensor, optional): of shape (N, C, H, W). Element-wise
+                weights. Default: None.
+        """
+        return self.loss_weight * charbonnier_loss(
+            pred,
+            target,
+            weight,
+            eps=self.eps,
+            reduction=self.reduction,
+            sample_wise=self.sample_wise)
+
+
+@LOSSES.register_module()
+class MaskedTVLoss(L1Loss):
+    """Masked TV loss.
+
+        Args:
+            loss_weight (float, optional): Loss weight. Defaults to 1.0.
+    """
+
+    def __init__(self, loss_weight=1.0):
+        super().__init__(loss_weight=loss_weight)
+
+    def forward(self, pred, mask=None):
+        """Forward function.
+
+        Args:
+            pred (torch.Tensor): Tensor with shape of (n, c, h, w).
+            mask (torch.Tensor, optional): Tensor with shape of (n, 1, h, w).
+                Defaults to None.
+
+        Returns:
+            [type]: [description]
+        """
+        y_diff = super().forward(
+            pred[:, :, :-1, :], pred[:, :, 1:, :], weight=mask[:, :, :-1, :])
+        x_diff = super().forward(
+            pred[:, :, :, :-1], pred[:, :, :, 1:], weight=mask[:, :, :, :-1])
+
+        loss = x_diff + y_diff
+
+        return loss
diff --git a/vsr_opt/mmedit/models/losses/utils.py b/vsr_opt/mmedit/models/losses/utils.py
new file mode 100755
index 0000000000..0182ef6c8f
--- /dev/null
+++ b/vsr_opt/mmedit/models/losses/utils.py
@@ -0,0 +1,116 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import functools
+
+import torch.nn.functional as F
+
+
+def reduce_loss(loss, reduction):
+    """Reduce loss as specified.
+
+    Args:
+        loss (Tensor): Elementwise loss tensor.
+        reduction (str): Options are "none", "mean" and "sum".
+
+    Returns:
+        Tensor: Reduced loss tensor.
+    """
+    reduction_enum = F._Reduction.get_enum(reduction)
+    # none: 0, elementwise_mean:1, sum: 2
+    if reduction_enum == 0:
+        return loss
+    if reduction_enum == 1:
+        return loss.mean()
+
+    return loss.sum()
+
+
+def mask_reduce_loss(loss, weight=None, reduction='mean', sample_wise=False):
+    """Apply element-wise weight and reduce loss.
+
+    Args:
+        loss (Tensor): Element-wise loss.
+        weight (Tensor): Element-wise weights. Default: None.
+        reduction (str): Same as built-in losses of PyTorch. Options are
+            "none", "mean" and "sum". Default: 'mean'.
+        sample_wise (bool): Whether calculate the loss sample-wise. This
+            argument only takes effect when `reduction` is 'mean' and `weight`
+            (argument of `forward()`) is not None. It will first reduces loss
+            with 'mean' per-sample, and then it means over all the samples.
+            Default: False.
+
+    Returns:
+        Tensor: Processed loss values.
+    """
+    # if weight is specified, apply element-wise weight
+    if weight is not None:
+        assert weight.dim() == loss.dim()
+        assert weight.size(1) == 1 or weight.size(1) == loss.size(1)
+        loss = loss * weight
+
+    # if weight is not specified or reduction is sum, just reduce the loss
+    if weight is None or reduction == 'sum':
+        loss = reduce_loss(loss, reduction)
+    # if reduction is mean, then compute mean over masked region
+    elif reduction == 'mean':
+        # expand weight from N1HW to NCHW
+        if weight.size(1) == 1:
+            weight = weight.expand_as(loss)
+        # small value to prevent division by zero
+        eps = 1e-12
+
+        # perform sample-wise mean
+        if sample_wise:
+            weight = weight.sum(dim=[1, 2, 3], keepdim=True)  # NCHW to N111
+            loss = (loss / (weight + eps)).sum() / weight.size(0)
+        # perform pixel-wise mean
+        else:
+            loss = loss.sum() / (weight.sum() + eps)
+
+    return loss
+
+
+def masked_loss(loss_func):
+    """Create a masked version of a given loss function.
+
+    To use this decorator, the loss function must have the signature like
+    `loss_func(pred, target, **kwargs)`. The function only needs to compute
+    element-wise loss without any reduction. This decorator will add weight
+    and reduction arguments to the function. The decorated function will have
+    the signature like `loss_func(pred, target, weight=None, reduction='mean',
+    avg_factor=None, **kwargs)`.
+
+    :Example:
+
+    >>> import torch
+    >>> @masked_loss
+    >>> def l1_loss(pred, target):
+    >>>     return (pred - target).abs()
+
+    >>> pred = torch.Tensor([0, 2, 3])
+    >>> target = torch.Tensor([1, 1, 1])
+    >>> weight = torch.Tensor([1, 0, 1])
+
+    >>> l1_loss(pred, target)
+    tensor(1.3333)
+    >>> l1_loss(pred, target, weight)
+    tensor(1.5000)
+    >>> l1_loss(pred, target, reduction='none')
+    tensor([1., 1., 2.])
+    >>> l1_loss(pred, target, weight, reduction='sum')
+    tensor(3.)
+    """
+
+    @functools.wraps(loss_func)
+    def wrapper(pred,
+                target,
+                weight=None,
+                reduction='mean',
+                sample_wise=False,
+                **kwargs):
+        # get element-wise loss
+        loss = loss_func(pred, target, **kwargs)
+        loss = mask_reduce_loss(loss, weight, reduction, sample_wise)
+        return loss
+
+    return wrapper
diff --git a/vsr_opt/mmedit/models/registry.py b/vsr_opt/mmedit/models/registry.py
new file mode 100755
index 0000000000..4f3e5aab7c
--- /dev/null
+++ b/vsr_opt/mmedit/models/registry.py
@@ -0,0 +1,9 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+from mmcv.cnn import MODELS as MMCV_MODELS
+from mmcv.utils import Registry
+
+MODELS = Registry('model', parent=MMCV_MODELS)
+BACKBONES = MODELS
+COMPONENTS = MODELS
+LOSSES = MODELS
diff --git a/vsr_opt/mmedit/models/restorers/__init__.py b/vsr_opt/mmedit/models/restorers/__init__.py
new file mode 100755
index 0000000000..c4f38daf03
--- /dev/null
+++ b/vsr_opt/mmedit/models/restorers/__init__.py
@@ -0,0 +1,8 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+from .basic_restorer import BasicRestorer
+from .basicvsr import BasicVSR
+
+__all__ = [
+    'BasicRestorer', 'BasicVSR',
+]
diff --git a/vsr_opt/mmedit/models/restorers/basic_restorer.py b/vsr_opt/mmedit/models/restorers/basic_restorer.py
new file mode 100755
index 0000000000..a3e99b5e78
--- /dev/null
+++ b/vsr_opt/mmedit/models/restorers/basic_restorer.py
@@ -0,0 +1,212 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import numbers
+import os.path as osp
+
+import mmcv
+from mmcv.runner import auto_fp16
+
+# from mmedit.core import psnr, ssim, tensor2img
+from ..base import BaseModel
+from ..builder import build_backbone, build_loss
+from ..registry import MODELS
+
+
+@MODELS.register_module()
+class BasicRestorer(BaseModel):
+    """Basic model for image restoration.
+
+    It must contain a generator that takes an image as inputs and outputs a
+    restored image. It also has a pixel-wise loss for training.
+
+    The subclasses should overwrite the function `forward_train`,
+    `forward_test` and `train_step`.
+
+    Args:
+        generator (dict): Config for the generator structure.
+        pixel_loss (dict): Config for pixel-wise loss.
+        train_cfg (dict): Config for training. Default: None.
+        test_cfg (dict): Config for testing. Default: None.
+        pretrained (str): Path for pretrained model. Default: None.
+    """
+    # allowed_metrics = {'PSNR': psnr, 'SSIM': ssim}
+    allowed_metrics = {}
+
+    def __init__(self,
+                 generator,
+                 pixel_loss,
+                 train_cfg=None,
+                 test_cfg=None,
+                 pretrained=None):
+        super().__init__()
+
+        self.train_cfg = train_cfg
+        self.test_cfg = test_cfg
+
+        # support fp16
+        self.fp16_enabled = False
+
+        # generator
+        self.generator = build_backbone(generator)
+        self.init_weights(pretrained)
+
+        # loss
+        self.pixel_loss = build_loss(pixel_loss)
+
+    def init_weights(self, pretrained=None):
+        """Init weights for models.
+
+        Args:
+            pretrained (str, optional): Path for pretrained weights. If given
+                None, pretrained weights will not be loaded. Defaults to None.
+        """
+        self.generator.init_weights(pretrained)
+
+    @auto_fp16(apply_to=('lq', ))
+    def forward(self, lq, gt=None, test_mode=False, **kwargs):
+        """Forward function.
+
+        Args:
+            lq (Tensor): Input lq images.
+            gt (Tensor): Ground-truth image. Default: None.
+            test_mode (bool): Whether in test mode or not. Default: False.
+            kwargs (dict): Other arguments.
+        """
+
+        if test_mode:
+            return self.forward_test(lq, gt, **kwargs)
+
+        return self.forward_train(lq, gt)
+
+    def forward_train(self, lq, gt):
+        """Training forward function.
+
+        Args:
+            lq (Tensor): LQ Tensor with shape (n, c, h, w).
+            gt (Tensor): GT Tensor with shape (n, c, h, w).
+
+        Returns:
+            Tensor: Output tensor.
+        """
+        losses = dict()
+        output = self.generator(lq)
+        loss_pix = self.pixel_loss(output, gt)
+        losses['loss_pix'] = loss_pix
+        outputs = dict(
+            losses=losses,
+            num_samples=len(gt.data),
+            results=dict(lq=lq.cpu(), gt=gt.cpu(), output=output.cpu()))
+        return outputs
+
+    def evaluate(self, output, gt):
+        """Evaluation function.
+
+        Args:
+            output (Tensor): Model output with shape (n, c, h, w).
+            gt (Tensor): GT Tensor with shape (n, c, h, w).
+
+        Returns:
+            dict: Evaluation results.
+        """
+        crop_border = self.test_cfg.crop_border
+
+        # output = tensor2img(output)
+        # gt = tensor2img(gt)
+
+        eval_result = dict()
+        for metric in self.test_cfg.metrics:
+            eval_result[metric] = self.allowed_metrics[metric](output, gt,
+                                                               crop_border)
+        return eval_result
+
+    def forward_test(self,
+                     lq,
+                     gt=None,
+                     meta=None,
+                     save_image=False,
+                     save_path=None,
+                     iteration=None):
+        """Testing forward function.
+
+        Args:
+            lq (Tensor): LQ Tensor with shape (n, c, h, w).
+            gt (Tensor): GT Tensor with shape (n, c, h, w). Default: None.
+            save_image (bool): Whether to save image. Default: False.
+            save_path (str): Path to save image. Default: None.
+            iteration (int): Iteration for the saving image name.
+                Default: None.
+
+        Returns:
+            dict: Output results.
+        """
+        output = self.generator(lq)
+        if self.test_cfg is not None and self.test_cfg.get('metrics', None):
+            assert gt is not None, (
+                'evaluation with metrics must have gt images.')
+            results = dict(eval_result=self.evaluate(output, gt))
+        else:
+            results = dict(lq=lq.cpu(), output=output.cpu())
+            if gt is not None:
+                results['gt'] = gt.cpu()
+
+        # # save image
+        # if save_image:
+        #     lq_path = meta[0]['lq_path']
+        #     folder_name = osp.splitext(osp.basename(lq_path))[0]
+        #     if isinstance(iteration, numbers.Number):
+        #         save_path = osp.join(save_path, folder_name,
+        #                              f'{folder_name}-{iteration + 1:06d}.png')
+        #     elif iteration is None:
+        #         save_path = osp.join(save_path, f'{folder_name}.png')
+        #     else:
+        #         raise ValueError('iteration should be number or None, '
+        #                          f'but got {type(iteration)}')
+        #     mmcv.imwrite(tensor2img(output), save_path)
+
+        return results
+
+    def forward_dummy(self, img):
+        """Used for computing network FLOPs.
+
+        Args:
+            img (Tensor): Input image.
+
+        Returns:
+            Tensor: Output image.
+        """
+        out = self.generator(img)
+        return out
+
+    def train_step(self, data_batch, optimizer):
+        """Train step.
+
+        Args:
+            data_batch (dict): A batch of data.
+            optimizer (obj): Optimizer.
+
+        Returns:
+            dict: Returned output.
+        """
+        outputs = self(**data_batch, test_mode=False)
+        loss, log_vars = self.parse_losses(outputs.pop('losses'))
+
+        # optimize
+        optimizer['generator'].zero_grad()
+        loss.backward()
+        optimizer['generator'].step()
+
+        outputs.update({'log_vars': log_vars})
+        return outputs
+
+    def val_step(self, data_batch, **kwargs):
+        """Validation step.
+
+        Args:
+            data_batch (dict): A batch of data.
+            kwargs (dict): Other arguments for ``val_step``.
+
+        Returns:
+            dict: Returned output.
+        """
+        output = self.forward_test(**data_batch, **kwargs)
+        return output
diff --git a/vsr_opt/mmedit/models/restorers/basicvsr.py b/vsr_opt/mmedit/models/restorers/basicvsr.py
new file mode 100755
index 0000000000..accdb04871
--- /dev/null
+++ b/vsr_opt/mmedit/models/restorers/basicvsr.py
@@ -0,0 +1,225 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+import numbers
+import os.path as osp
+
+import mmcv
+import numpy as np
+import torch
+
+# from mmedit.core import tensor2img
+from ..registry import MODELS
+from .basic_restorer import BasicRestorer
+
+
+@MODELS.register_module()
+class BasicVSR(BasicRestorer):
+    """BasicVSR model for video super-resolution.
+
+    Note that this model is used for IconVSR.
+
+    Paper:
+        BasicVSR: The Search for Essential Components in Video Super-Resolution
+        and Beyond, CVPR, 2021
+
+    Args:
+        generator (dict): Config for the generator structure.
+        pixel_loss (dict): Config for pixel-wise loss.
+        ensemble (dict): Config for ensemble. Default: None.
+        train_cfg (dict): Config for training. Default: None.
+        test_cfg (dict): Config for testing. Default: None.
+        pretrained (str): Path for pretrained model. Default: None.
+    """
+
+    def __init__(self,
+                 generator,
+                 pixel_loss,
+                 ensemble=None,
+                 train_cfg=None,
+                 test_cfg=None,
+                 pretrained=None):
+        super().__init__(generator, pixel_loss, train_cfg, test_cfg,
+                         pretrained)
+
+        # fix pre-trained networks
+        self.fix_iter = train_cfg.get('fix_iter', 0) if train_cfg else 0
+        self.is_weight_fixed = False
+
+        # count training steps
+        self.register_buffer('step_counter', torch.zeros(1))
+
+        # # ensemble
+        # self.forward_ensemble = None
+        # if ensemble is not None:
+        #     if ensemble['type'] == 'SpatialTemporalEnsemble':
+        #         from mmedit.models.common.ensemble import \
+        #             SpatialTemporalEnsemble
+        #         is_temporal = ensemble.get('is_temporal_ensemble', False)
+        #         self.forward_ensemble = SpatialTemporalEnsemble(is_temporal)
+        #     else:
+        #         raise NotImplementedError(
+        #             'Currently support only '
+        #             '"SpatialTemporalEnsemble", but got type '
+        #             f'[{ensemble["type"]}]')
+
+    def check_if_mirror_extended(self, lrs):
+        """Check whether the input is a mirror-extended sequence.
+
+        If mirror-extended, the i-th (i=0, ..., t-1) frame is equal to the
+        (t-1-i)-th frame.
+
+        Args:
+            lrs (tensor): Input LR images with shape (n, t, c, h, w)
+        """
+
+        is_mirror_extended = False
+        if lrs.size(1) % 2 == 0:
+            lrs_1, lrs_2 = torch.chunk(lrs, 2, dim=1)
+            if torch.norm(lrs_1 - lrs_2.flip(1)) == 0:
+                is_mirror_extended = True
+
+        return is_mirror_extended
+
+    def train_step(self, data_batch, optimizer):
+        """Train step.
+
+        Args:
+            data_batch (dict): A batch of data.
+            optimizer (obj): Optimizer.
+
+        Returns:
+            dict: Returned output.
+        """
+        # fix SPyNet and EDVR at the beginning
+        if self.step_counter < self.fix_iter:
+            if not self.is_weight_fixed:
+                self.is_weight_fixed = True
+                for k, v in self.generator.named_parameters():
+                    if 'spynet' in k or 'edvr' in k:
+                        v.requires_grad_(False)
+        elif self.step_counter == self.fix_iter:
+            # train all the parameters
+            self.generator.requires_grad_(True)
+
+        outputs = self(**data_batch, test_mode=False)
+        loss, log_vars = self.parse_losses(outputs.pop('losses'))
+
+        # optimize
+        optimizer['generator'].zero_grad()
+        loss.backward()
+        optimizer['generator'].step()
+
+        self.step_counter += 1
+
+        outputs.update({'log_vars': log_vars})
+        return outputs
+
+    def evaluate(self, output, gt):
+        """Evaluation function.
+
+        If the output contains multiple frames, we compute the metric
+        one by one and take an average.
+
+        Args:
+            output (Tensor): Model output with shape (n, t, c, h, w).
+            gt (Tensor): GT Tensor with shape (n, t, c, h, w).
+
+        Returns:
+            dict: Evaluation results.
+        """
+        crop_border = self.test_cfg.crop_border
+        convert_to = self.test_cfg.get('convert_to', None)
+
+        eval_result = dict()
+        # for metric in self.test_cfg.metrics:
+        #     if output.ndim == 5:  # a sequence: (n, t, c, h, w)
+        #         avg = []
+        #         for i in range(0, output.size(1)):
+        #             output_i = tensor2img(output[:, i, :, :, :])
+        #             gt_i = tensor2img(gt[:, i, :, :, :])
+        #             avg.append(self.allowed_metrics[metric](
+        #                 output_i, gt_i, crop_border, convert_to=convert_to))
+        #         eval_result[metric] = np.mean(avg)
+        #     elif output.ndim == 4:  # an image: (n, c, t, w), for Vimeo-90K-T
+        #         output_img = tensor2img(output)
+        #         gt_img = tensor2img(gt)
+        #         value = self.allowed_metrics[metric](
+        #             output_img, gt_img, crop_border, convert_to=convert_to)
+        #         eval_result[metric] = value
+
+        return eval_result
+
+    def forward_test(self,
+                     lq,
+                     gt=None,
+                     meta=None,
+                     save_image=False,
+                     save_path=None,
+                     iteration=None):
+        """Testing forward function.
+
+        Args:
+            lq (Tensor): LQ Tensor with shape (n, t, c, h, w).
+            gt (Tensor): GT Tensor with shape (n, t, c, h, w). Default: None.
+            save_image (bool): Whether to save image. Default: False.
+            save_path (str): Path to save image. Default: None.
+            iteration (int): Iteration for the saving image name.
+                Default: None.
+
+        Returns:
+            dict: Output results.
+        """
+        with torch.no_grad():
+            if self.forward_ensemble is not None:
+                output = self.forward_ensemble(lq, self.generator)
+            else:
+                output = self.generator(lq)
+
+        # If the GT is an image (i.e. the center frame), the output sequence is
+        # turned to an image.
+        if gt is not None and gt.ndim == 4:
+            t = output.size(1)
+            if self.check_if_mirror_extended(lq):  # with mirror extension
+                output = 0.5 * (output[:, t // 4] + output[:, -1 - t // 4])
+            else:  # without mirror extension
+                output = output[:, t // 2]
+
+        if self.test_cfg is not None and self.test_cfg.get('metrics', None):
+            assert gt is not None, (
+                'evaluation with metrics must have gt images.')
+            results = dict(eval_result=self.evaluate(output, gt))
+        else:
+            results = dict(lq=lq.cpu(), output=output.cpu())
+            if gt is not None:
+                results['gt'] = gt.cpu()
+
+        # # save image
+        # if save_image:
+        #     if output.ndim == 4:  # an image, key = 000001/0000 (Vimeo-90K)
+        #         img_name = meta[0]['key'].replace('/', '_')
+        #         if isinstance(iteration, numbers.Number):
+        #             save_path = osp.join(
+        #                 save_path, f'{img_name}-{iteration + 1:06d}.png')
+        #         elif iteration is None:
+        #             save_path = osp.join(save_path, f'{img_name}.png')
+        #         else:
+        #             raise ValueError('iteration should be number or None, '
+        #                              f'but got {type(iteration)}')
+        #         mmcv.imwrite(tensor2img(output), save_path)
+        #     elif output.ndim == 5:  # a sequence, key = 000
+        #         folder_name = meta[0]['key'].split('/')[0]
+        #         for i in range(0, output.size(1)):
+        #             if isinstance(iteration, numbers.Number):
+        #                 save_path_i = osp.join(
+        #                     save_path, folder_name,
+        #                     f'{i:08d}-{iteration + 1:06d}.png')
+        #             elif iteration is None:
+        #                 save_path_i = osp.join(save_path, folder_name,
+        #                                        f'{i:08d}.png')
+        #             else:
+        #                 raise ValueError('iteration should be number or None, '
+        #                                  f'but got {type(iteration)}')
+        #             mmcv.imwrite(
+        #                 tensor2img(output[:, i, :, :, :]), save_path_i)
+
+        return results
diff --git a/vsr_opt/mmedit/version.py b/vsr_opt/mmedit/version.py
new file mode 100755
index 0000000000..275ef8bf5d
--- /dev/null
+++ b/vsr_opt/mmedit/version.py
@@ -0,0 +1,18 @@
+# Copyright (c) Open-MMLab. All rights reserved.
+# Copyright (C) 2022 Intel Corporation
+__version__ = '0.14.0'
+
+
+def parse_version_info(version_str):
+    ver_info = []
+    for x in version_str.split('.'):
+        if x.isdigit():
+            ver_info.append(int(x))
+        elif x.find('rc') != -1:
+            patch_version = x.split('rc')
+            ver_info.append(int(patch_version[0]))
+            ver_info.append(f'rc{patch_version[1]}')
+    return tuple(ver_info)
+
+
+version_info = parse_version_info(__version__)
diff --git a/vsr_opt/samples/basicvsr_inference_sample.py b/vsr_opt/samples/basicvsr_inference_sample.py
new file mode 100755
index 0000000000..4883ded4e7
--- /dev/null
+++ b/vsr_opt/samples/basicvsr_inference_sample.py
@@ -0,0 +1,207 @@
+# Copyright (C) 2022 Intel Corporation
+#!/usr/bin/env python
+# coding=utf-8
+import argparse
+import os
+import math
+
+import torch
+import numpy as np
+import cv2
+from math import ceil
+from torchvision.utils import make_grid
+from openvino.runtime import Core
+from patch_utils import restore_random_crop_from_patch,crop_images_to_random_patch
+import logging 
+logging.basicConfig(level = logging.INFO)
+logger = logging.getLogger(__name__)
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Model Inference')
+    parser.add_argument(
+        '--model',
+        '-m',
+        type=str,
+        help='Need. Openvino IR model for inference')
+    parser.add_argument(
+        '--input',
+        '-i',
+        type=str,
+        help='Need. Input data for inference, multi-input please separate by comma') 
+    parser.add_argument(
+        '--number_input_frames',
+        '-nif',
+        type=int,
+        help='Need. Number of input frames. Please keep in line with the model')
+    parser.add_argument(
+        '--device',
+        '-d',
+        default='CPU',
+        type=str,
+        help='Optional. Device to performance inference, default CPU') 
+    parser.add_argument(
+        '--save_path',
+        '-s',
+        default='./',
+        type=str,
+        help='Optional. Path to save inference predictions')
+    parser.add_argument(
+        '--patch_evaluation', 
+        '-pe',
+        action='store_true', 
+        help='Optional. Evaluate by image patches. Supported patch size is [360,640],'
+            ' make sure model input consistent with the supported patch size')
+    parser.add_argument(
+        '--extensions',
+        '-e',
+        type=str,
+        help='Optional. Required for CPU custom layers. '
+                'Absolute path to a shared library with the kernels implementations.')
+    parser.add_argument(
+        '--path_to_cldnn_config',
+        '-pcc',
+        type=str,
+        help='Optional. Required for GPU custom kernels. Absolute path to an .xml file with the '
+                    'kernels description.')
+    args = parser.parse_args()
+    return args
+
+
+def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):
+    """Convert torch Tensors into image numpy arrays.
+
+    Note that the image channel in input tensors should be RGB order. This
+    function will convert it to cv2 convention, i.e., (H x W x C) with BGR
+    order.
+
+    Args:
+        tensor (Tensor | list[Tensor]): Input tensors.
+        out_type (numpy type): Output types. If ``np.uint8``, transform outputs
+            to uint8 type with range [0, 255]; otherwise, float type with
+            range [0, 1]. Default: ``np.uint8``.
+        min_max (tuple): min and max values for clamp.
+
+    Returns:
+        (Tensor | list[Tensor]): 3D ndarray of shape (H x W x C) or 2D ndarray
+        of shape (H x W).
+    """
+    if not (torch.is_tensor(tensor) or
+            (isinstance(tensor, list)
+             and all(torch.is_tensor(t) for t in tensor))):
+        raise TypeError(
+            f'tensor or list of tensors expected, got {type(tensor)}')
+
+    if torch.is_tensor(tensor):
+        tensor = [tensor]
+    result = []
+    for _tensor in tensor:
+        _tensor = _tensor.squeeze(0).squeeze(0)
+        _tensor = _tensor.float().detach().cpu().clamp_(*min_max)
+        _tensor = (_tensor - min_max[0]) / (min_max[1] - min_max[0])
+        n_dim = _tensor.dim()
+        if n_dim == 4:
+            img_np = make_grid(
+                _tensor, nrow=int(math.sqrt(_tensor.size(0))),
+                normalize=False).numpy()
+            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
+        elif n_dim == 3:
+            img_np = _tensor.numpy()
+            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
+        elif n_dim == 2:
+            img_np = _tensor.numpy()
+        else:
+            raise ValueError('Only support 4D, 3D or 2D tensor. '
+                             f'But received with dimension: {n_dim}')
+        if out_type == np.uint8:
+            img_np = (img_np * 255.0).round()
+
+        img_np.astype(out_type)
+        result.append(img_np)
+    result = result[0] if len(result) == 1 else result
+    return result
+
+
+def read_input_data(path,nif):
+    input_path_list = path.split(',')
+    if len(input_path_list) < nif:
+        raise ValueError(f'BasicVSR inference needs {nif} input frames. But received {len(input_patch_list)}')
+    if len(input_path_list) > nif:
+        logger.warning('Too much input data. Only previous "{}" images will be used for inference'.format(nif))
+        input_path_list = input_path_list[:nif]
+    input_list = []
+    file_name_list = []
+    for input_path in input_path_list:
+        file_name_list.append(input_path.split('/')[-1])
+        image = cv2.cvtColor(cv2.imread(input_path), cv2.COLOR_BGR2RGB)
+        input_image = np.transpose(image, (2, 0, 1))
+        input_list.append(input_image)
+    input_array = np.array(input_list)
+    input_array = np.expand_dims(input_array,0)
+    input_array = input_array.astype(np.float32)/255.
+    return input_array,file_name_list
+
+
+def save_predictions(predictions,save_path,file_name_list):
+    for id in range(len(file_name_list)):
+        pred = predictions[:,id,:,:,:]
+        name = file_name_list[id]
+        output = torch.from_numpy(pred)
+        output_img = tensor2img(output)
+        cv2.imwrite(os.path.join(save_path,name),output_img)
+    logger.info('Succeed in saving inference results.')
+
+
+def basicvsr_inference(args):
+    ie = Core()
+    # CPU extension
+    if args.device == 'CPU' and args.extensions:
+        ie.add_extension(args.extensions) 
+        logger.info(f'CPU extensions is loaded {args.extensions}')
+    # GPU extension
+    if args.device == 'GPU' and args.extensions and args.path_to_cldnn_config:
+        ie.add_extension(args.extensions)
+        ie.set_property('GPU', {'CONFIG_FILE': args.path_to_cldnn_config})
+        logger.info(f'GPU extensions is loaded {args.path_to_cldnn_config}')
+        
+    input_data,data_names = read_input_data(args.input,args.number_input_frames)
+    input_data = np.ones((1,3,3,360,640))
+    _,t,c,h,w = input_data.shape
+
+    ir_model=ie.read_model(model=args.model)
+    ir_compiled_model = ie.compile_model(model=ir_model,device_name=args.device)
+    ir_input_layer = next(iter(ir_compiled_model.inputs))
+    ir_output_layer = next(iter(ir_compiled_model.outputs))
+    _,input_t,input_c,input_h,input_w = ir_input_layer.shape
+    _,target_t,target_c,target_h,target_w = ir_output_layer.shape
+
+    logger.info(f'Start inference on {args.model}:')
+    if args.patch_evaluation:
+        if h < 360 or w < 640:
+            raise ValueError(f'Patch generation failure. Input data size [{h,w}] smaller than patch size [{360,640}]') 
+        if input_h != 360 or input_w != 640:
+            raise ValueError(f'Patch evaluation failure. Model input shape [{input_c,input_h}] is incompatible with patch size [360,640]') 
+        blocks = [ceil(h/360),ceil(w/640)]
+        input_patch_list = crop_images_to_random_patch(input_data,[360,640], blocks)
+        pred_patch_list = []
+        for idx in range(len(input_patch_list)):
+            input_patch = input_patch_list[idx]
+            pred_patch = ir_compiled_model(inputs=[input_patch])[ir_output_layer]
+            pred_patch_list.append(pred_patch)
+        scale = target_h // input_h
+        restore_size = [input_data.shape[-2]*scale,input_data.shape[-1]*scale]
+        pred_size = pred_patch_list[0].shape[-2:]
+        preds = restore_random_crop_from_patch(pred_patch_list, restore_size, pred_size, blocks)
+    else:
+        for (x,y) in zip(ir_input_layer.shape,input_data.shape):
+            if x != y :
+                raise Exception(f'Model input {ir_input_layer.shape} and given input data {input_data.shape} are incompatible')
+        for i in range(100):
+            preds = ir_compiled_model(inputs=[input_data])[ir_output_layer]
+    
+    save_predictions(preds,args.save_path,data_names)
+    logger.info(f'BasicVSR inference on {args.input} has finished.')
+
+
+if __name__=='__main__':
+    args = parse_args()
+    basicvsr_inference(args)
\ No newline at end of file
diff --git a/vsr_opt/samples/patch_utils.py b/vsr_opt/samples/patch_utils.py
new file mode 100755
index 0000000000..b5f81549a7
--- /dev/null
+++ b/vsr_opt/samples/patch_utils.py
@@ -0,0 +1,116 @@
+# Copyright (C) 2022 Intel Corporation
+import numpy as np
+
+def caulate_random_crop_coordinate(ori_size, crop_size, blocks):
+    [width, height] = ori_size   
+    [crop_width,crop_height] = crop_size
+    inter_width = (crop_width * blocks[0] - width) // (blocks[0] - 1)
+    last_fill_width = (crop_width * blocks[0] - width) % (blocks[0] - 1)
+    inter_height = (crop_height * blocks[1] - height) // (blocks[1] - 1)
+    last_fill_height = (crop_height * blocks[1] - height) % (blocks[1] - 1)
+    crop_coordinate_list = []
+    for i_ in range(blocks[0]):
+        for j_ in range(blocks[1]):
+            x1 = (crop_width - inter_width) * i_
+            y1 = (crop_height - inter_height) * j_
+            if i_ == blocks[0] - 1:
+                x1 = x1 - last_fill_width
+            if j_ == blocks[1] - 1:
+                y1 = y1 - last_fill_height
+            crop_coordinate_list.append((x1, y1, x1 + crop_width, y1 + crop_height))
+    return crop_coordinate_list,blocks
+
+
+def caulate_crop_coordinate_padding(ori_size: list, crop_size: int, overlapp_size: list):
+    [width, height] = ori_size 
+    [overlapp_w, overlapp_h] = overlapp_size
+    block_w = (width - crop_size) / (crop_size - overlapp_w)
+    if block_w % 1 != 0:
+        block_w = int(block_w) + 2
+    else:
+        block_w = int(block_w) + 1
+    block_h = (height - crop_size) / (crop_size - overlapp_h)
+    if block_h % 1 != 0:
+        block_h = int(block_h) + 2
+    else:
+        block_h = int(block_h) + 1
+
+    if (height) <= crop_size:
+        block_h = 1
+    crop_coordinate_list = []
+    for i_ in range(block_w):
+        for j_ in range(block_h):
+            x1 = (crop_size - overlapp_w) * i_
+            y1 = (crop_size - overlapp_h) * j_
+            if (height) <= crop_size:
+                crop_coordinate_list.append((x1, 0, x1 + crop_size, height))
+            else:
+                crop_coordinate_list.append((x1, y1, x1 + crop_size, y1 + crop_size))
+
+    return crop_coordinate_list, [block_w, block_h]
+
+
+def crop_images_to_random_patch(images,crop_size, blocks, overlapp_size=[0, 0], scale=1):
+    b,n,c,h,w = images.shape
+    ori_size = [h,w]
+    scale_size = list(int(a / scale) for a in ori_size)
+    if overlapp_size[0] == 0:
+        crop_coordinate_list, cal_blocks = caulate_random_crop_coordinate(scale_size, crop_size, blocks)
+    else:
+        crop_coordinate_list, cal_blocks = caulate_crop_coordinate_padding(scale_size, crop_size, overlapp_size)
+    crop_coordinate_list = [tuple(coo * scale for coo in corps) for corps in crop_coordinate_list]
+    def crop_img(image):
+        croped_list =[]
+        for idx, coordinate in enumerate(crop_coordinate_list):
+            cropped_img = image[:,:,:,coordinate[0]:coordinate[2],coordinate[1]:coordinate[3]]
+            _,_,_,c_h,c_w = cropped_img.shape
+            if c_h < crop_size[0] or c_w < crop_size[1]:
+                cropped = np.zeros((b,n,c,crop_size[0],crop_size[1]))
+                cropped[:,:,:,:c_h,:c_w] = cropped[:,:,:,:c_h,:c_w] + cropped_img
+                cropped_img = cropped
+            croped_list.append(cropped_img)
+
+        return croped_list
+    
+    crop_list = crop_img(images)
+    return crop_list
+
+
+def restore_random_crop_from_patch(patch_list, ori_size: list, crop_size: int, blocks, padding=0, scales=1):
+    if padding == 0:
+        crop_coordinate_list,cal_blocks = caulate_random_crop_coordinate(ori_size, crop_size, blocks)
+    else:
+        padding = [padding,padding] 
+        crop_coordinate_list,cal_blocks = caulate_crop_coordinate_padding(ori_size, crop_size, padding)
+    crop_coordinate_list = [tuple(coo * scales for coo in corps) for corps in crop_coordinate_list]
+
+    def restore_crop_img(img_restore_list):
+        b,n,c,_,_= img_restore_list[0].shape
+        restore_size_h = ori_size[0] * scales  
+        restore_size_w = ori_size[1] * scales  
+        output_img = np.zeros((b,n,c,restore_size_h, restore_size_w), dtype=np.float32)
+        add_nums_np = np.zeros((b,n,c,restore_size_h, restore_size_w), dtype=np.float32)
+        crop_img_size = crop_size * scales
+        for img_idx, img_crop in enumerate(img_restore_list):
+            teplate_add = np.ones((b,n,c,crop_img_size[0], crop_img_size[1]), dtype=np.float32)
+            coordinate = crop_coordinate_list[img_idx]
+            if coordinate[3] > restore_size_w:
+                img_crop = img_crop[:,:,:, :,:-(coordinate[3] - restore_size_w)]
+                teplate_add = teplate_add[:,:,:,:,:-(coordinate[3] - restore_size_w)]
+            if coordinate[2] > restore_size_h:
+                img_crop = img_crop[:,:,:,:-(coordinate[2] - restore_size_h),:]
+                teplate_add = teplate_add[:,:,:,:-(coordinate[2] - restore_size_h),:]
+
+            output_img[:,:,:,coordinate[0]:coordinate[2], coordinate[1]:coordinate[3]] += img_crop
+            add_nums_np[:,:,:,coordinate[0]:coordinate[2], coordinate[1]:coordinate[3]] += teplate_add
+
+        np.place(add_nums_np,add_nums_np==0,[1])
+        img_result = np.divide(output_img, add_nums_np)
+        return img_result
+
+    restored_results = restore_crop_img(patch_list)
+    return restored_results
+
+
+    
+    
\ No newline at end of file
diff --git a/vsr_opt/tools/pytorch2onnx.py b/vsr_opt/tools/pytorch2onnx.py
new file mode 100755
index 0000000000..db71e2db41
--- /dev/null
+++ b/vsr_opt/tools/pytorch2onnx.py
@@ -0,0 +1,116 @@
+# Copyright (C) 2022 Intel Corporation
+import torch
+import argparse
+import os
+from mmedit.models import build_model
+import mmcv
+from mmcv.runner import load_checkpoint
+from mmcv.onnx import register_extra_symbolics
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Pytorch model to ONNX model')
+    parser.add_argument(
+        '--input_model',
+        default=None,
+        type=str,
+        help='Pytorch model file')
+    parser.add_argument(
+        '--output_dir',
+        default='./',
+        type=str,
+        help='Path to store ONNX model file')
+    parser.add_argument('--nif', default=3, type=int, help='Number of input frames')
+    parser.add_argument('--width', default=1920, type=int, help='Width of a frame')
+    parser.add_argument('--height', default=1080, type=int, help='Height of a frame')
+    parser.add_argument('--custom_op', action='store_true', help='Whether to use custom op')
+    args = parser.parse_args()
+    return args
+
+
+def pytorch2onnx(model,
+                 input_shape=[1, 3, 3, 1080, 1920],
+                 opset_version=12,
+                 show=False,
+                 output_file='tmp.onnx',
+                 dynamic_export=False):
+    """Export Pytorch model to ONNX model and verify the outputs are same
+    between Pytorch and ONNX.
+
+    Args:
+        model (nn.Module): Pytorch model we want to export.
+        input (dict): We need to use this input to execute the model.
+        opset_version (int): The onnx op version. Default: 11.
+        show (bool): Whether print the computation graph. Default: False.
+        output_file (string): The path to where we store the output ONNX model.
+            Default: `tmp.onnx`.
+        verify (bool): Whether compare the outputs between Pytorch and ONNX.
+            Default: False.
+    """
+    model.cpu().eval()
+
+    data = torch.randn(input_shape)
+    model.forward = model.forward_dummy
+    register_extra_symbolics(opset_version)
+    dynamic_axes = None
+    if dynamic_export:
+        dynamic_axes = {
+            'input': {
+                0: 'batch',
+                2: 'height',
+                3: 'width'
+            },
+            'output': {
+                0: 'batch',
+                2: 'height',
+                3: 'width'
+            }
+        }
+
+    script_model = torch.jit.trace(model, data)
+    freeze_model = torch.jit.freeze(script_model, preserved_attrs=["training"])
+    with torch.no_grad():
+        torch.onnx.export(
+            freeze_model,
+            data,
+            output_file,
+            input_names=['input'],
+            output_names=['output'],
+            export_params=True,
+            keep_initializers_as_inputs=False,
+            verbose=show,
+            opset_version=opset_version,
+            dynamic_axes=dynamic_axes,
+            do_constant_folding=True
+        )
+    print(f'Successfully exported ONNX model: {output_file}')
+
+
+def main(args):
+    model_name = args.input_model.split('/')[-1].split('.')[0]
+    suffix = ''
+    if args.custom_op is True:
+        suffix = '_custom_op'
+    output_model_path = os.path.join(args.output_dir, f'{model_name}{suffix}_1_{args.nif}_3_{args.height}_{args.width}.onnx')
+
+    # build the model
+    config = mmcv.Config.fromfile('./configs/basicvsr_x2_cuc.py')
+    config.model.pretrained = None
+    config.model.generator.custom_op = args.custom_op
+    model = build_model(config.model, test_cfg=config.test_cfg)
+    checkpoint = load_checkpoint(model, args.input_model, map_location='cpu')
+    nif = args.nif
+    channel_num = 3
+    width = args.width
+    height = args.height
+    input_shape = [1, nif, channel_num, height, width]
+    # convert model to onnx file
+    pytorch2onnx(
+        model,
+        input_shape,
+        output_file=output_model_path)
+
+
+if __name__ == '__main__':
+    args = parse_args()
+    main(args)
diff --git a/vsr_opt/tools/quantization.py b/vsr_opt/tools/quantization.py
new file mode 100755
index 0000000000..ef056142cb
--- /dev/null
+++ b/vsr_opt/tools/quantization.py
@@ -0,0 +1,482 @@
+# Copyright (C) 2022 Intel Corporation
+import os
+import torch
+import numpy as np
+import argparse
+
+from torchvision.utils import make_grid
+from openvino.tools.pot import IEEngine
+from openvino.tools.pot import load_model,save_model
+from openvino.tools.pot import compress_model_weights
+from openvino.tools.pot import create_pipeline
+from openvino.tools.pot import DataLoader
+from openvino.tools.pot import Metric
+import math
+import cv2
+import math
+
+from openvino.tools.pot.utils.logger import init_logger,get_logger
+init_logger(level='INFO')
+logger = get_logger(__name__)
+
+def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):
+    """Convert torch Tensors into image numpy arrays. 
+    
+    Args:
+        tensor (Tensor | list[Tensor]): Input tensors.
+        out_type (numpy type): Output types. If ``np.uint8``, transform outputs
+            to uint8 type with range [0, 255]; otherwise, float type with
+            range [0, 1]. Default: ``np.uint8``.
+        min_max (tuple): min and max values for clamp.
+
+    Returns:
+        (Tensor | list[Tensor]): 3D ndarray of shape (H x W x C) or 2D ndarray
+        of shape (H x W).
+    """
+    if not (torch.is_tensor(tensor) or
+            (isinstance(tensor, list)
+             and all(torch.is_tensor(t) for t in tensor))):
+        raise TypeError(
+            f'tensor or list of tensors expected, got {type(tensor)}')
+
+    if torch.is_tensor(tensor):
+        tensor = [tensor]
+    result = []
+    for _tensor in tensor:
+        _tensor = _tensor.squeeze(0).squeeze(0)
+        _tensor = _tensor.float().detach().cpu().clamp_(*min_max)
+        _tensor = (_tensor - min_max[0]) / (min_max[1] - min_max[0])
+        n_dim = _tensor.dim()
+        if n_dim == 4:
+            img_np = make_grid(
+                _tensor, nrow=int(math.sqrt(_tensor.size(0))),
+                normalize=False).numpy()
+            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
+        elif n_dim == 3:
+            img_np = _tensor.numpy()
+            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
+        elif n_dim == 2:
+            img_np = _tensor.numpy()
+        else:
+            raise ValueError('Only support 4D, 3D or 2D tensor. '
+                             f'But received with dimension: {n_dim}')
+        if out_type == np.uint8:
+            img_np = (img_np * 255.0).round()
+        img_np.astype(out_type)
+        result.append(img_np)
+    result = result[0] if len(result) == 1 else result
+    return result
+
+def psnr(img1, img2, crop_border=0, input_order='HWC', convert_to=None):
+    """Calculate PSNR (Peak Signal-to-Noise Ratio).
+
+    Ref: https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio
+
+    Args:
+        img1 (ndarray): Images with range [0, 255].
+        img2 (ndarray): Images with range [0, 255].
+        crop_border (int): Cropped pixels in each edges of an image. These
+            pixels are not involved in the PSNR calculation. Default: 0.
+        input_order (str): Whether the input order is 'HWC' or 'CHW'.
+            Default: 'HWC'.
+        convert_to (str): Whether to convert the images to other color models.
+            If None, the images are not altered. When computing for 'Y',
+            the images are assumed to be in BGR order. Options are 'Y' and
+            None. Default: None.
+
+    Returns:
+        float: psnr result.
+    """
+
+    assert img1.shape == img2.shape, (
+        f'Image shapes are differnet: {img1.shape}, {img2.shape}.')
+    if input_order not in ['HWC', 'CHW']:
+        raise ValueError(
+            f'Wrong input_order {input_order}. Supported input_orders are '
+            '"HWC" and "CHW"')
+    img1 = reorder_image(img1, input_order=input_order)
+    img2 = reorder_image(img2, input_order=input_order)
+
+    img1, img2 = img1.astype(np.float32), img2.astype(np.float32)
+    if isinstance(convert_to, str) and convert_to.lower() == 'y':
+        img1 = cv.cvtColor(	img1 / 255,cv2.COLOR_BGR2YCrCb) * 255.
+        img2 = cv.cvtColor(	img2 / 255,cv2.COLOR_BGR2YCrCb) * 255.
+    elif convert_to is not None:
+        raise ValueError('Wrong color model. Supported values are '
+                         '"Y" and None.')
+
+    if crop_border != 0:
+        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, None]
+        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, None]
+
+    mse_value = np.mean((img1 - img2)**2)
+    if mse_value == 0:
+        return float(10e5)
+    return 20. * np.log10(255. / np.sqrt(mse_value))
+
+def _ssim(img1, img2):
+    """Calculate SSIM (structural similarity) for one channel images.
+
+    Args:
+        img1, img2 (ndarray): Images with range [0, 255] with order 'HWC'.
+
+    Returns:
+        float: ssim result.
+    """
+
+    C1 = (0.01 * 255)**2
+    C2 = (0.03 * 255)**2
+
+    img1 = img1.astype(np.float64)
+    img2 = img2.astype(np.float64)
+    kernel = cv2.getGaussianKernel(11, 1.5)
+    window = np.outer(kernel, kernel.transpose())
+
+    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]
+    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]
+    mu1_sq = mu1**2
+    mu2_sq = mu2**2
+    mu1_mu2 = mu1 * mu2
+    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq
+    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq
+    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2
+
+    ssim_map = ((2 * mu1_mu2 + C1) *
+                (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *
+                                       (sigma1_sq + sigma2_sq + C2))
+    return ssim_map.mean()
+
+
+def ssim(img1, img2, crop_border=0, input_order='HWC', convert_to=None):
+    """Calculate SSIM (structural similarity).
+
+    Args:
+        img1 (ndarray): Images with range [0, 255].
+        img2 (ndarray): Images with range [0, 255].
+        crop_border (int): Cropped pixels in each edges of an image. These
+            pixels are not involved in the SSIM calculation. Default: 0.
+        input_order (str): Whether the input order is 'HWC' or 'CHW'.
+            Default: 'HWC'.
+        convert_to (str): Whether to convert the images to other color models.
+            If None, the images are not altered. When computing for 'Y',
+            the images are assumed to be in BGR order. Options are 'Y' and
+            None. Default: None.
+
+    Returns:
+        float: ssim result.
+    """
+
+    assert img1.shape == img2.shape, (
+        f'Image shapes are differnet: {img1.shape}, {img2.shape}.')
+    if input_order not in ['HWC', 'CHW']:
+        raise ValueError(
+            f'Wrong input_order {input_order}. Supported input_orders are '
+            '"HWC" and "CHW"')
+    img1 = reorder_image(img1, input_order=input_order)
+    img2 = reorder_image(img2, input_order=input_order)
+
+    if isinstance(convert_to, str) and convert_to.lower() == 'y':
+        img1, img2 = img1.astype(np.float32), img2.astype(np.float32)
+        img1 = cv.cvtColor(	img1 / 255,cv2.COLOR_BGR2YCrCb) * 255.
+        img2 = cv.cvtColor(	img2 / 255,cv2.COLOR_BGR2YCrCb) * 255.
+        img2 = cv2.COLOR_BGR2YCrCb()
+        img1 = np.expand_dims(img1, axis=2)
+        img2 = np.expand_dims(img2, axis=2)
+    elif convert_to is not None:
+        raise ValueError('Wrong color model. Supported values are '
+                         '"Y" and None')
+
+    if crop_border != 0:
+        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, None]
+        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, None]
+
+    ssims = []
+    for i in range(img1.shape[2]):
+        ssims.append(_ssim(img1[..., i], img2[..., i]))
+    return np.array(ssims).mean()
+
+
+class QuantizationLoader(DataLoader):
+    """ Loads images from a folder """
+    def __init__(self,dataset_path,LR_or_HR=['LR','HR'],sub_folder='',num_input_frames =3,need_gt=True):
+        self._lq_files=[]
+        self._gt_files=[]
+        self._input_frames = num_input_frames
+        self.sub_folder = sub_folder.split(',')
+        if 'LR' in LR_or_HR:
+            folder_path = os.path.join(dataset_path,'LR')
+            self._lq_files = self.get_file_path(folder_path,self.sub_folder)
+
+        if 'HR' in LR_or_HR:
+            folder_path = os.path.join(dataset_path,'HR')
+            self._gt_files = self.get_file_path(folder_path,self.sub_folder)
+
+        self.need_gt = need_gt
+
+    def get_file_path(self,dataset_path,sub_folders):
+        files = []
+        all_dirs = os.listdir(dataset_path)
+        
+        if not isinstance(sub_folders,list):
+            sub_folders = [sub_folders]
+
+        for sub_folder in sub_folders:
+            file_dir = os.path.join(dataset_path,sub_folder)
+            all_file_in_dir = os.listdir(file_dir)
+            all_file_in_dir.sort(key=lambda x:int(x.split('.')[0]))
+            if len(all_file_in_dir) > 100:
+                all_file_in_dir = all_file_in_dir[:100]
+            for name in all_file_in_dir:
+                file_path = os.path.join(file_dir,name)
+                if cv2.haveImageReader(file_path):
+                    files.append(file_path)
+        return files
+
+    def __len__(self):
+        """ Returns the length of the dataset """
+        return len(self._lq_files)//self._input_frames
+
+    
+    def getImage(self,filepath):
+        image = cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)
+        
+        input_image = np.transpose(image, (2, 0, 1))
+
+        return input_image
+
+
+    def __getitem__(self,index):
+        """ Returns frames by index in the NCHW layout """
+        if self._gt_files== None and self._lq_files == None:
+            return None,None
+        
+        image_list = []
+        gt_list = []
+        for id in range(self._input_frames):
+            image_index = index*self._input_frames + id
+            image_path = self._lq_files[image_index]
+            image = self.getImage(image_path)
+            image_list.append(image)
+        inputframes = np.array(image_list)
+        inputframes = np.expand_dims(inputframes,0)
+        inputframes = inputframes.astype(np.float32)/255.   # normalize
+
+        if self.need_gt == True:
+            for id in range(self._input_frames):
+                image_index = index*self._input_frames + id
+                image_path = self._gt_files[image_index]
+                image = self.getImage(image_path)
+                gt_list.append(image)
+            gtframes = np.array(gt_list)
+            gtframes = np.expand_dims(gtframes,0)
+            gtframes = gtframes.astype(np.float32)/255.
+            return inputframes,gtframes  
+        
+        return inputframes,None  
+
+class BasicvsrMetrics(Metric):
+    def __init__(self,metrics_name,quantization=True):
+        self._metrics = metrics_name
+        self._results = dict()
+        self.allowed_metrics = {'PSNR': psnr, 'SSIM': ssim}
+        self.for_accuracy_aware_quantization = quantization
+        for metric in self._metrics:
+            assert metric in self.allowed_metrics, (
+                f'The metric:{metric} not in supported metrics:{self._metrics}')
+            self._results[metric]= []
+    
+    @property
+    def value(self):
+        results = dict()
+        for metric in self._metrics:
+            values_num = len(self._results[metric])
+            results[metric] = self._results[metric][values_num-1]
+        return results
+
+    @property
+    def avg_value(self):
+        results = dict()
+        for key in self._results.keys():
+            logger.info(f'metric {key}: {self._results[key]}')
+            results[key] = np.mean(self._results[key])
+        return results
+
+
+    def update(self, output, target,crop_border=0,convert_to=None):
+        """Calculte psnr and ssim metrics
+
+        Refer to: basicvsr.py:evaluate()
+
+        """
+        if self.for_accuracy_aware_quantization:
+            output = torch.from_numpy(output[0])  
+            target = torch.from_numpy(target[0])
+        else:
+            output = torch.from_numpy(output)     
+            target = torch.from_numpy(target)
+        for metric in self._metrics:
+            assert metric in self.allowed_metrics, (
+                f'The metric:{metric} not in supported metrics:{self._metrics}')
+            
+            if output.ndim == 5:  # a sequence: (n, t, c, h, w)
+                avg = []
+                for i in range(0, output.size(1)):
+                    output_i = tensor2img(output[:, i, :, :, :])
+                    gt_i = tensor2img(target[:, i, :, :, :])
+                    avg.append(self.allowed_metrics[metric](output_i, gt_i, crop_border, convert_to=convert_to))
+                self._results[metric].append(avg)
+
+            elif output.ndim == 4: 
+                output_img = tensor2img(output)
+                gt_img = tensor2img(target)
+                value = self.allowed_metrics[metric](
+                    output_img, gt_img, crop_border, convert_to=convert_to)
+                self._results[metric].append(value)
+
+            else:
+                raise Exception('The metric cannot be calculated '
+                            'for a model with wrong output dims')
+        
+
+    def reset(self):
+        for metric in self._metrics:
+            assert metric in self.allowed_metrics, (
+                f'The metric:{metric} not in supported metrics:{self._metrics}')
+            self._results[metric]= []
+
+    def get_attributes(self):
+        return {'PSNR': {'direction': 'higher-better',
+                             'type': 'accuracy'},
+                'SSIM':{'direction':'higher-better',
+                             'type':'accuracy'}}
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Model Quantization')
+    parser.add_argument(
+        '--input_model',
+        default='',
+        type=str,
+        help='Need. Model needs to be quantized which in *.xml format') 
+    parser.add_argument(
+        '--output_dir',
+        default='./',
+        type=str,
+        help='Optional. Path to store quantized model.')
+    parser.add_argument(
+        '--dataset_path',
+        default='',
+        type=str,
+        help='Need. A representative calibration dataset representing a use case scenario')
+    parser.add_argument(
+        '--sub_folder',
+        default='',
+        type=str,
+        help='Need. Sub-folder of datasets for inference')
+    parser.add_argument(
+        '--nif',
+        default=3,
+        type=int,
+        help='Need. Number of input frames')  
+    parser.add_argument(
+        '--accuracy_aware_quantization',
+        action='store_true', 
+        help='Optional. Quantize by accuracy aware algorithm,otherwise by default algorithm')  
+    parser.add_argument(
+        '--extension',
+        default='',
+        type=str,
+        help='Optional. Extension path for custom operation')
+    args = parser.parse_args()
+    return args
+
+def main(args):
+    model_name = args.input_model.split('/')[-1].split('.xml')[0]
+    model_xml = args.input_model
+    model_bin = args.input_model.split('.xml')[0] + '.bin'
+    model_mapping = args.input_model.split('.xml')[0] + '.mapping'
+    # config settings
+    model_config = {
+        "model_name": "basicvsr",
+        "model": model_xml,
+        "weights": model_bin,
+    }
+    engine_config = {"device": "CPU",
+        'stat_requests_number':1,
+        'eval_requests_number':1
+    }
+    algorithms = []
+    name_prex = ""
+    # algorithm settings:
+    if args.accuracy_aware_quantization:
+        algorithms = [
+            {
+                "name": "AccuracyAwareQuantization",
+                "params": {
+                    "target_device": "CPU",
+                    "stat_subset_size": 10,  # for default algorithm
+                    "ranking_subset_size":10,
+                    "max_iter_num":100,
+                    "maximal_drop":0.01,
+                    "drop_type":"relative"
+                },
+            }
+        ]
+        name_prex = "optimized_acc_aware_"
+    else:
+        algorithms = [
+            {
+                "name": "DefaultQuantization",
+                "params": {
+                    "target_device": "CPU",
+                    "stat_subset_size": 10,  
+                },
+            }
+        ]
+        name_prex = "optimized_"
+    # metric setting
+    metrics = ['PSNR','SSIM']
+    
+    logger.info(f'Dataset path: {args.dataset_path} and folder: {args.sub_folder}')
+    logger.info(f'Model to be quantized: {args.input_model}')
+
+    # Step 1: Load model
+    model = load_model(model_config=model_config)
+
+    # Step 2: Initialize the engine
+    if args.accuracy_aware_quantization:
+        data_loader = QuantizationLoader(args.dataset_path,LR_or_HR=['LR','HR'],sub_folder=args.sub_folder,num_input_frames=args.nif,need_gt=True)  
+        # Implement Metric
+        metric = BasicvsrMetrics(metrics)
+        engine = IEEngine(config=engine_config, data_loader=data_loader,metric=metric) # accuarcy aware
+    else:
+        data_loader = QuantizationLoader(args.dataset_path,LR_or_HR=['LR'],sub_folder=args.sub_folder,num_input_frames=args.nif,need_gt=False)
+        engine = IEEngine(config=engine_config, data_loader=data_loader) # Default
+
+    # Optional load extension
+    if args.extension:
+        logger.info(f'Add custom layer extension:{args.extension}')
+        engine._ie.add_extension(args.extension)
+    
+    # Step 3: Create pipeline
+    logger.info('Create pipeline and run:')
+    pipeline = create_pipeline(algorithms,engine)
+    compressed_model = pipeline.run(model=model)
+
+    logger.info('Finished pipeline running, begin to compress model weights:')
+
+    # Step 4: Compress model weights to quantized precision
+    compress_model_weights(compressed_model)
+
+    # Step 5: Save the compressed model to desired path
+    logger.info('Finished model weights compress, begin to save model')
+    compressed_model_paths = save_model(model=compressed_model,
+                                    save_path = args.output_dir,
+                                        model_name = name_prex + model_name
+                                    )
+
+
+
+if __name__ == '__main__':
+    args = parse_args()
+    main(args)
diff --git a/vsr_opt/tools/requirements.txt b/vsr_opt/tools/requirements.txt
new file mode 100644
index 0000000000..ac988bdf84
--- /dev/null
+++ b/vsr_opt/tools/requirements.txt
@@ -0,0 +1,2 @@
+torch
+torchvision
-- 
2.25.1

