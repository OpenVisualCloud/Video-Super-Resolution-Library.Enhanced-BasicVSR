From 2e3c901f67f84fecdb6093fc1884b04da674c8a4 Mon Sep 17 00:00:00 2001
From: KunLong9 <105702843+KunLong9@users.noreply.github.com>
Date: Wed, 21 Sep 2022 14:29:55 +0800
Subject: [PATCH 04/17] flow_warp custom op ocl implementation and application
 in openvino tools (#6)

---
 .../custom_op/flow_warp.cpp                   |  6 +-
 flow_warp_custom_op/flow_warp.cl              | 88 +++++++++++++++++++
 flow_warp_custom_op/flow_warp.xml             | 14 +++
 samples/cpp/benchmark_app/main.cpp            |  2 +-
 .../src/graph/impls/ocl/custom_primitive.cpp  |  3 +
 .../openvino/tools/benchmark/main.py          |  3 +-
 .../cross_check_tool/cross_check_tool.py      | 11 ++-
 .../openvino/tools/cross_check_tool/utils.py  |  7 +-
 8 files changed, 124 insertions(+), 10 deletions(-)
 create mode 100644 flow_warp_custom_op/flow_warp.cl
 create mode 100644 flow_warp_custom_op/flow_warp.xml

diff --git a/docs/template_extension/custom_op/flow_warp.cpp b/docs/template_extension/custom_op/flow_warp.cpp
index e237b9d56d..443db48743 100644
--- a/docs/template_extension/custom_op/flow_warp.cpp
+++ b/docs/template_extension/custom_op/flow_warp.cpp
@@ -207,8 +207,8 @@ void flow_warp(const float*input_data,std::vector<size_t> input_dim,float*flow_d
                 if (interpolation_mode == GridSamplerInterpolation::Bilinear) {
                     // get corner pixel values from (x, y)
                     // for 4d, we use north-east-south-west
-                    int64_t ix_nw = static_cast<int64_t>(std::floor(ix));
-                    int64_t iy_nw = static_cast<int64_t>(std::floor(iy));
+                    int64_t ix_nw = static_cast<int64_t>(std::floor(ix + 1e-4));
+                    int64_t iy_nw = static_cast<int64_t>(std::floor(iy + 1e-4));
 
                     int64_t ix_ne = ix_nw + 1;
                     int64_t iy_ne = iy_nw;
@@ -225,7 +225,7 @@ void flow_warp(const float*input_data,std::vector<size_t> input_dim,float*flow_d
                     float sw = (ix_ne - ix) * (iy - iy_ne);
                     float se = (ix - ix_nw) * (iy - iy_nw);
 
-                    // calculate bilinear weighted pixel value and set output pixel
+		    // calculate bilinear weighted pixel value and set output pixel
                     const float *inp_ptr_NC = inp_ptr_N;
                     float *out_ptr_NCHW = out_ptr + n * out_sN + h * out_sH + w * out_sW;
                     for (int64_t c = 0; c < C; ++c, out_ptr_NCHW += out_sC, inp_ptr_NC += inp_sC) {
diff --git a/flow_warp_custom_op/flow_warp.cl b/flow_warp_custom_op/flow_warp.cl
new file mode 100644
index 0000000000..5972e85e22
--- /dev/null
+++ b/flow_warp_custom_op/flow_warp.cl
@@ -0,0 +1,88 @@
+//OCL custom layer implementation for flow_warp
+//Author: Renzhi.Jiang@Intel.com
+//currently only support {padding:0, Bilinear interpolation, align corners:true}
+//input0 format: bfyx, input1:bfyx, outpyt format:bfyx
+
+// #pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#pragma OPENCL EXTENSION cl_intel_printf : enable
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)   \
+    CAT(prefix, _OFFSET) +                   \
+    (x)*CAT(prefix, _PITCHES[3]) +           \
+    (y)*CAT(prefix, _PITCHES[2]) +           \
+    (f)*CAT(prefix, _PITCHES[1]) +           \
+    (b)*CAT(prefix, _PITCHES[0])
+
+__kernel void flow_warp(
+    const __global INPUT0_TYPE* input0,
+    const __global INPUT1_TYPE* input1,
+    __global OUTPUT0_TYPE* output)
+{
+    uint x = get_global_id(0);
+    uint y = get_global_id(1);
+    
+#if OUTPUT0_BATCH_NUM == 1
+    uint f = get_global_id(2);
+    uint b = 1;
+#else
+    uint f = get_global_id(2) % OUTPUT0_DIMS[1];
+    uint b = get_global_id(2) / OUTPUT0_DIMS[1];
+#endif //OUTPUT_BATCH_NUM > 1
+
+    //get flow value
+    uint ind_x = GET_DATA_INDEX(INPUT1, b , 2, y, x);
+    uint ind_y = ind_x + 1;
+    
+    // get input pixel
+    INPUT1_TYPE flow_x = input1[ind_x] + x;
+    INPUT1_TYPE flow_y = input1[ind_y] + y;
+
+    // get input pixel neighborhood
+    int coord_x = floor(flow_x + 1e-4);
+    int coord_y = floor(flow_y + 1e-4);
+    int coord_x_next = coord_x + 1;
+    int coord_y_next = coord_y + 1;
+
+    float4 weights, x4, y4;
+    x4[0] = flow_x - coord_x; y4[0] = coord_y_next - flow_y;   //ne
+    x4[1] = coord_x_next - flow_x; y4[1] = coord_y_next - flow_y; // nw
+    x4[2] = flow_x - coord_x; y4[2] = flow_y - coord_y; // se
+    x4[3] = coord_x_next - flow_x; y4[3] = flow_y - coord_y;   // sw
+    weights = mad(x4, y4, 0);
+
+    OUTPUT0_TYPE res = 0.0f;
+
+    //TODO: more efficency way to do this ???
+    if (coord_y >=0 && coord_y < INPUT0_DIMS[2]) {
+        if (coord_x_next >= 0 && coord_x_next < INPUT0_DIMS[3]) { //ne
+            uint index = GET_DATA_INDEX(INPUT0, b, f, coord_y, coord_x_next);
+            res += input0[index] * weights[0];
+        }
+        if (coord_x >= 0 && coord_x < INPUT0_DIMS[3]) { //nw
+            uint index = GET_DATA_INDEX(INPUT0, b, f, coord_y, coord_x);
+            res += input0[index] * weights[1];
+        }
+    }
+
+    if (coord_y_next >=0 && coord_y_next < INPUT0_DIMS[2]) {
+        if (coord_x_next >=0 && coord_x_next < INPUT0_DIMS[3]) { //se
+            uint index = GET_DATA_INDEX(INPUT0, b, f, coord_y_next, coord_x_next);
+            res += input0[index] * weights[2];
+        }
+        if (coord_x >= 0 && coord_x < INPUT0_DIMS[3]) { //sw
+            uint index = GET_DATA_INDEX(INPUT0, b, f, coord_y_next, coord_x);
+            res += input0[index] * weights[3];
+        }
+    }
+    
+    uint out_indx = GET_DATA_INDEX(OUTPUT0, b, f, y, x);
+    output[out_indx] = res;
+}
+
+
+
+
+
diff --git a/flow_warp_custom_op/flow_warp.xml b/flow_warp_custom_op/flow_warp.xml
new file mode 100644
index 0000000000..5c42599fbc
--- /dev/null
+++ b/flow_warp_custom_op/flow_warp.xml
@@ -0,0 +1,14 @@
+<!-- configuration file for flow warp kernel -->
+<CustomLayer name="flow_warp" type="SimpleGPU" version="1">
+  <Kernel entry="flow_warp">
+    <Source filename="flow_warp.cl"/>
+    <!-- <Define name="neg_slope" type="float" param="negative_slope" default="0.0"/> -->
+  </Kernel>
+  <Buffers>
+    <Tensor arg-index="0" type="input" port-index="0" format="BFYX"/>
+    <Tensor arg-index="1" type="input" port-index="1" format="BFYX"/>
+    <Tensor arg-index="2" type="output" port-index="0" format="BFYX"/>
+  </Buffers>
+  <CompilerOptions options="-cl-mad-enable"/>
+  <WorkSizes global="X,Y,B*F"/>
+</CustomLayer>
diff --git a/samples/cpp/benchmark_app/main.cpp b/samples/cpp/benchmark_app/main.cpp
index f6fded1b22..895eb15e2e 100644
--- a/samples/cpp/benchmark_app/main.cpp
+++ b/samples/cpp/benchmark_app/main.cpp
@@ -209,7 +209,7 @@ int main(int argc, char* argv[]) {
 
         ov::Core core;
 
-        if (FLAGS_d.find("CPU") != std::string::npos && !FLAGS_l.empty()) {
+        if (/*FLAGS_d.find("CPU") != std::string::npos && */!FLAGS_l.empty()) {
             // CPU (MKLDNN) extensions is loaded as a shared library
             core.add_extension(FLAGS_l);
             slog::info << "CPU (MKLDNN) extensions is loaded " << FLAGS_l << slog::endl;
diff --git a/src/plugins/intel_gpu/src/graph/impls/ocl/custom_primitive.cpp b/src/plugins/intel_gpu/src/graph/impls/ocl/custom_primitive.cpp
index 3b0d2161e4..e43b08c56c 100644
--- a/src/plugins/intel_gpu/src/graph/impls/ocl/custom_primitive.cpp
+++ b/src/plugins/intel_gpu/src/graph/impls/ocl/custom_primitive.cpp
@@ -127,6 +127,9 @@ static void add_layout_to_jit(kernel_selector::jit_constants& mem_consts, const
     mem_consts.AddConstant(
         kernel_selector::MakeJitConstant(name + "_FORMAT_" + kernel_selector::toString(to_data_layout(l.format)), ""));
 
+    mem_consts.AddConstant(
+	kernel_selector::MakeJitConstant(name + "_BATCH_NUM",  std::to_string(l.size.sizes(format::bfyx)[0])));
+    
     // Padding (in elements)
     // #define INPUT0_LOWER_PADDING (uint[]) { 0, 0, 0, 0 }
     // #define INPUT0_UPPER_PADDING (uint[]) { 0, 0, 0, 0 }
diff --git a/tools/benchmark_tool/openvino/tools/benchmark/main.py b/tools/benchmark_tool/openvino/tools/benchmark/main.py
index 78037e8ecd..fa0dcb605b 100644
--- a/tools/benchmark_tool/openvino/tools/benchmark/main.py
+++ b/tools/benchmark_tool/openvino/tools/benchmark/main.py
@@ -67,7 +67,8 @@ def run(args):
                               args.number_iterations, args.time, args.api_type, args.inference_only)
 
         ## CPU (MKLDNN) extensions
-        if CPU_DEVICE_NAME in device_name and args.path_to_extension:
+        #if CPU_DEVICE_NAME in device_name and args.path_to_extension:
+        if args.path_to_extension:
             benchmark.add_extension(path_to_extension=args.path_to_extension)
 
         ## GPU (clDNN) Extensions
diff --git a/tools/cross_check_tool/openvino/tools/cross_check_tool/cross_check_tool.py b/tools/cross_check_tool/openvino/tools/cross_check_tool/cross_check_tool.py
index 58542dcaa7..a8aa1fc6bb 100755
--- a/tools/cross_check_tool/openvino/tools/cross_check_tool/cross_check_tool.py
+++ b/tools/cross_check_tool/openvino/tools/cross_check_tool/cross_check_tool.py
@@ -38,12 +38,17 @@ def set_cpu_extensions(core: Core, cpu_ext: str):
     core.add_extension(cpu_ext)
 
 
-def get_plugin(device: str, cpu_ext: str = None, config: str = None):
+def get_plugin(device: str, cpu_ext: str = None, config: str = None, gpu_config:str =None):  # @longkun: , gpu_config:str =None
     core = Core()
     # log.info('{} plugin:\n          API version ............ {}'.format(device, plugin.version), extra={'no_lvl': True})
     set_plugin_config(core=core, device=device, config=config)
     if cpu_ext and 'CPU' in device:
         set_cpu_extensions(core=core, cpu_ext=cpu_ext)
+    
+    # GPU config for custom op  # @longkun:
+    if cpu_ext and 'GPU' in device:
+        core.add_extension(cpu_ext)
+        core.set_property('GPU', {'CONFIG_FILE': gpu_config})
     return core
 
 
@@ -161,7 +166,7 @@ def overall_accuracy_check(model: str, ref_model: str, out_ops: list, ref_out_op
 
 
 def one_ir_mode(args):
-    core = get_plugin(args.device, args.l, args.config)
+    core = get_plugin(args.device, args.l, args.config, args.path_to_cldnn_config) 
     model = get_model(model_path=args.model, core=core)
     model_ops, model_inputs, model_outputs = get_model_info(model)
     log.info(f'{args.device} vs {args.reference_device}')
@@ -169,7 +174,7 @@ def one_ir_mode(args):
     out_ops = get_ops_list(model_ops, model_outputs, args.layers)
     print_inputs(model_inputs)
     print_output_ops(out_ops)
-    ref_core = get_plugin(args.reference_device, args.l, args.reference_config)
+    ref_core = get_plugin(args.reference_device, args.l, args.reference_config, args.path_to_cldnn_config) 
     global_accuracy = []
     inputs = input_processing(model_path=args.model, model_inputs=model_inputs, input_file=args.input)
     global_times, ref_global_times = overall_accuracy_check(model=args.model, ref_model=args.model,
diff --git a/tools/cross_check_tool/openvino/tools/cross_check_tool/utils.py b/tools/cross_check_tool/openvino/tools/cross_check_tool/utils.py
index 29038b16b0..b00bdaa733 100755
--- a/tools/cross_check_tool/openvino/tools/cross_check_tool/utils.py
+++ b/tools/cross_check_tool/openvino/tools/cross_check_tool/utils.py
@@ -196,6 +196,9 @@ def build_parser():
     plugin.add_argument('-l', type=str, action=ExistingFileAction,
                         help='Required for MKLDNN (CPU)-targeted custom layers. Comma separated paths to a shared'
                              ' libraries with the kernels implementation.')
+    plugin.add_argument('--path_to_cldnn_config', '-c', type=str, action=ExistingFileAction,
+                        help='Required for GPU custom kernels. Absolute path to an .xml file with the '
+                             'kernels description.')  # @longkun:
 
     modes = parser.add_argument_group('CCT mode arguments')
     # TODO eps? nobody uses it
@@ -520,8 +523,8 @@ def print_all_over_the_net_metrics(global_accuracy: list, global_times: list = N
 def get_ops_list(all_ops: list, outputs: list, layers: str):
     if layers is not None and layers != 'None':
         if layers == 'all':
-            return [op for op in all_ops if op.get_type_name() not in ['Constant', 'Result', 'Parameter', 'Less', 'Greater', 'Equal', 'Convolution']]
-            # return [op for op in all_ops if op.get_type_name() in ['Concat']]
+            # return [op for op in all_ops if op.get_type_name() not in ['Constant', 'Result', 'Parameter', 'Less', 'Greater', 'Equal', 'Convolution']]
+            return [op for op in all_ops if op.get_type_name() in ['Gather']]  #@longkun:
         else:
             all_ops_map = {op.friendly_name: op for op in all_ops}
             user_ops = [layer.strip() for layer in layers.split(',')]
-- 
2.25.1

