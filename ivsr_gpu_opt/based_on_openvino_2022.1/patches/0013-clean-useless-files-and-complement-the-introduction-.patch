From ad6d86d3713dcad720de62be802d7fb4223d29e2 Mon Sep 17 00:00:00 2001
From: KunLong9 <105702843+KunLong9@users.noreply.github.com>
Date: Fri, 23 Sep 2022 09:59:14 +0800
Subject: [PATCH 13/17] clean useless files and complement the introduction
 docs (#15)

* modification of custom op for MO

* delete useless files

* complement for readme
---
 vsr_opt/Custom op from Pytorch to OpenVINO.md |  80 ++++-
 vsr_opt/VSR Introduction.md                   |  52 ++-
 vsr_opt/samples/ReadMe.md                     |  31 --
 vsr_opt/samples/basicvsr_infer.py             | 256 -------------
 vsr_opt/samples/basicvsr_metrics.py           | 327 -----------------
 vsr_opt/samples/crop_restore_image.py         | 335 ------------------
 vsr_opt/samples/quantization_basicvsr.py      | 149 --------
 vsr_opt/samples/quantization_dataloader.py    | 100 ------
 8 files changed, 110 insertions(+), 1220 deletions(-)
 delete mode 100755 vsr_opt/samples/ReadMe.md
 delete mode 100755 vsr_opt/samples/basicvsr_infer.py
 delete mode 100755 vsr_opt/samples/basicvsr_metrics.py
 delete mode 100755 vsr_opt/samples/crop_restore_image.py
 delete mode 100755 vsr_opt/samples/quantization_basicvsr.py
 delete mode 100755 vsr_opt/samples/quantization_dataloader.py

diff --git a/vsr_opt/Custom op from Pytorch to OpenVINO.md b/vsr_opt/Custom op from Pytorch to OpenVINO.md
index bdf4710e35..c0f6a418e1 100644
--- a/vsr_opt/Custom op from Pytorch to OpenVINO.md	
+++ b/vsr_opt/Custom op from Pytorch to OpenVINO.md	
@@ -1,14 +1,20 @@
-# Custom op registeration and implementation from Pytorch to OpenVINO
+# Custom op  registration and implementation from Pytorch to OpenVINO
+
+Here provides a simple introduction about how to add a self-defined operation to Pytorch, ONNX and OpenVINO.
 
 ## Create Pytorch Custom op
 
-1.Adding the custom operator implementation in C++ 
+There are three main steps to creating and applying a custom operation in Pytorch:
+
+1. Adding the custom operator implementation in C++ 
 
-2.Registering custom operator with TorchScript
+2. Registering custom operator with TorchScript
 
-3.Using the custom operator in programs
+3. Using the custom operator in programs
 
-If you have a custom operator that you need to register in TorchScript as a C++ extension, you need to implement the operator and build it with setuptools.
+If you want to create a custom operator, you need to implement the operator and register the operator in TorchScript as a C++ extension.
+
+As an example, let's create a `flow_warp` custom operator and show the sample code.
 
 Implement the custom operator `flow_warp` and register the custom operator in TorchScript by `torch::RegisterOperators` in `flow_warp.cpp`.
 ```c
@@ -22,9 +28,9 @@ torch::Tensor flow_warp(torch::Tensor X,torch::Tensor flow) {
 // register flow_warp op with TorchScript compiler 
 static auto registry = torch::RegisterOperators("custom_op_namespace::flow_warp", &flow_warp);
 ```
-Note that the first argument of `torch::RegisterOperators` is operator **namespace** and **name** separated by **::**. The next argument is a reference to your function.
+Note that the first argument of `torch::RegisterOperators` includes the **namespace** and **name** of the operator to be registered, which separated by **::**. The next argument is a reference to your function.
 
-Once you have your C++ function, you can build it using `setuptools.Extension`. Create a `setup.py` script in the same directory where you have your C++ code. `CppExtension.BuildExtension` takes care of the required compiler flags, such as required include paths and flags required during mixed C++/CUDA mixed compilation.
+Once you finish the C++ implementation of custom operator, you can build it using `setuptools.Extension`. Create a `setup.py` script in the same directory where you have your C++ code. `CppExtension.BuildExtension` takes care of the required compiler flags, such as required include paths and flags required during C++/CUDA mixed compilation.
 
 
 ```python
@@ -265,7 +271,7 @@ find_library(ONNXRUNTIME_LIBRARY onnxruntime HINTS "/home/media/xxx/basicvsr/cus
 target_link_libraries(FlowWarp_op PUBLIC ${ONNXRUNTIME_LIBRARY})
 ```
 
-Now that you have implmented the custom operator, you should be able to run your model and test it. Before running, you need to register the custom operator to onnxruntime sessions by the share library using `register_custom_ops_library`.
+Now that you have implemented the custom operator, you should be able to run your model and test it. Before running, you need to register the custom operator to onnxruntime sessions by the share library using `register_custom_ops_library`.
 
 ```py
 def inference_on_onnx(x,flow):
@@ -285,12 +291,12 @@ def inference_on_onnx(x,flow):
 ```
 The code fragment above can be added to the test file `flow_warp_test.py`.
 
- [Reference:How to export Pytorch model with custom op to ONNX and run it in ONNX Runtime](https://github.com/onnx/tutorials/tree/master/PyTorchCustomOperator)
+More details in [How to export Pytorch model with custom op to ONNX and run it in ONNX Runtime](https://github.com/onnx/tutorials/tree/master/PyTorchCustomOperator).
 
 
 ## Expert ONNX custom operator to OpenVINO IR and implement for OpenVINO runtime
 
-AS the OpenVINO offical documents [Custom Operation Support Overview](https://docs.openvino.ai/2021.4/openvino_docs_HOWTO_Custom_Layers_Guide.html#enabling-magnetic-resonance-image-reconstruction-model) informed, there are three steps to support inference of a model with custom operation(s):
+As the OpenVINO offical documents [Custom Operation Support Overview](https://docs.openvino.ai/2021.4/openvino_docs_HOWTO_Custom_Layers_Guide.html#enabling-magnetic-resonance-image-reconstruction-model) informed, there are three steps to support inference of a model with custom operation(s):
 
 Step1: Add support for custom operation in the Model Optimizer
 
@@ -301,7 +307,6 @@ Step3: Create the custom operation implementation for Inference Engine.
 
 
 ### Add support for custom in the Model Optimizer
-[Model Optimizer Extensibility](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html#model-optimizer-extensions)
 
 You may refer to the [Model Optimizer Workflow](https://docs.openvino.ai/2020.1/_docs_HOWTO_Custom_Layers_Guide.html) and you can find that there are two keys to extend the custom layers in Model Optimizer:
 - Custom Layer Extractor
@@ -352,6 +357,7 @@ class FlowWarp(Op):
 ```
 Theoretically, you can transform the ONNX model to OpenVINO IR successfully by the custom layer extractor and operation. However, in the more recently release OpenVINO source code, there would be nGraph validation in the MO process. That means the MO process would produce an temporary IR and try to build the model graph to validate the IR is workable. The validation process would meet some mistakes, you can find the solution latter.
 
+Refer to [Model Optimizer Extensibility](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html#model-optimizer-extensions) for more information.
 
 ### Create custom operation as nGraph operation
 There is the offical documents about create custom operation as nGraph operation
@@ -436,7 +442,7 @@ bool FlowWarp::has_evaluate() const {
 }
 //! [op:evaluate]
 ```
-Note that in the new custom extension method the `execute` function is the entrance for kernel to perform the custom operator's operation. Therefore, the Step 3 "Create the custom operation for Inference Engine" can be merged into Step 2.
+Note that in the new custom extension method the `evaluate` function is the entrance for `CPU` kernel to perform the custom operator's operation. Therefore, the Step 3 "Create the custom operation for Inference Engine" can be merged into Step 2.
 
 After defining and implmenting the custom operator, the next step is to register the custom operation. You may register the custom operation by create the custom operation as an extension. In our example, we create a script file `extension.cpp` to register custom operation as an extension. In the script you need to define two share pointer on behalf of the operation itself and the operation mapping.
 
@@ -487,24 +493,66 @@ target_compile_definitions(${TARGET_NAME} PRIVATE IMPLEMENT_OPENVINO_EXTENSION_A
 target_link_libraries(${TARGET_NAME} PRIVATE openvino::runtime)
 ```
 
-Since we have acqured the custom operation share library, we can add extension for nGraph validation in MO process. In our example, to address the nGraph validation problem, we make some modification in the `openvino/tools/mo/openvino/tools/mo/back/offline_transformations.py` by adding the custom extension by library path manually.
+Since we have acquired the custom operation share library, we can add extension for nGraph validation in MO process. In our example, to address the nGraph validation problem, we make some modification in the `openvino/tools/mo/openvino/tools/mo/back/offline_transformations.py` by adding the custom extension to library path manually.
 ```py
 # offline_transformations.py
 
     def read_model(path_to_xml):
         fe = fem.load_by_framework(framework="ir")
         # add custom extension for nGraph validation 
-        fe.add_extension('/home/media/xxx/openvino/src/my_custom_op/build/libcustom_extension.so')  # addin
+        fe.add_extension('xxx/openvino/bin/intel64/Release/lib/libcustom_extension.so')  # addin
         function = fe.convert(fe.load(path_to_xml))
         return function
 ```
 
-After obtaining the IR format in success, you can use the custom operation in your scene by loading the custom operation share library as extension. You can add the code fragments below in your right position.
+### Create the custom operation implementation for Inference Engine
+
+To enable the custom operation on GPU, you need to provide the kernel code in OpenCL C and an XML configuration file that connects the kernel and its parameters to the parameters of the operation.
+
+```c
+//flow_warp.cl
+
+//OCL custom layer implementation for flow_warp
+__kernel void flow_warp(
+    const __global INPUT0_TYPE* input0,
+    const __global INPUT1_TYPE* input1,
+    __global OUTPUT0_TYPE* output)
+{
+    // flow_warp custom operator ocl implementation 
+    ...
+}
+
+```
+
+```xml
+<!-- flow_warp.xml -->
+
+<!-- configuration file for flow warp kernel -->
+<CustomLayer name="flow_warp" type="SimpleGPU" version="1">
+  <Kernel entry="flow_warp">
+    <Source filename="flow_warp.cl"/>
+    <!-- <Define name="neg_slope" type="float" param="negative_slope" default="0.0"/> -->
+  </Kernel>
+  <Buffers>
+    <Tensor arg-index="0" type="input" port-index="0" format="BFYX"/>
+    <Tensor arg-index="1" type="input" port-index="1" format="BFYX"/>
+    <Tensor arg-index="2" type="output" port-index="0" format="BFYX"/>
+  </Buffers>
+  <CompilerOptions options="-cl-mad-enable"/>
+  <WorkSizes global="X,Y,B*F"/>
+</CustomLayer>
+
+```
+After obtaining the IR format in success, you can use the custom operation in your scene by loading the custom operation share library as extension. You can add the code fragments below in your right position. For more details you may refer to [How to Implement Custom GPU Operations](https://docs.openvino.ai/2021.4/openvino_docs_IE_DG_Extensibility_DG_GPU_Kernel.html).
 
 ```py
 from openvino.runtime import Core
 ie=Core()
-ie.add_extension('/home/media/xxx/openvino/bin/intel64/Release/lib/libcustom_extension.so') 
+# for CPU extension
+ie.add_extension('xxx/openvino/bin/intel64/lib/libcustom_extension.so') 
+# for GPU extension
+ie.add_extension('xxx/openvino/bin/intel64/lib/libcustom_extension.so')
+ie.set_property('GPU', {'CONFIG_FILE': 'xxx/openvino/flow_warp_custom_op_gpu/flow_warp.xml'})
 ```
 
 
diff --git a/vsr_opt/VSR Introduction.md b/vsr_opt/VSR Introduction.md
index d1aac95d38..f0a7d02d29 100644
--- a/vsr_opt/VSR Introduction.md	
+++ b/vsr_opt/VSR Introduction.md	
@@ -6,16 +6,34 @@ This tutorial demonstrates step-by-step instructions to perform Video Super Reso
 
 Firstly, the PyTorch model needs to be converted to ONNX, you may refer to [Convert PyTorch model to ONNX](https://docs.openvino.ai/latest/notebooks/102-pytorch-onnx-to-openvino-with-output.html#onnx-model-conversion) for details. There is an sample script `pytorch2onnx.py` implements the model conversion. You can perform the model conversion by command:
 ```bash
-python3 pytorch2onnx.py --input_model <Pytorch model> --output_dir <Output dir to store ONNX model>
+export PYTHONPATH=$PYTHONPATH:<PATH_TO_THIS_PROJECT>/vsr_opt/
+cd <PATH_TO_THIS_PROJECT>/vsr_opt
+# To get a model w/o custom op
+python tools/pytorch2onnx.py --input_model <Pytorch model> --output_dir <Output dir to store ONNX model> --nif <Number of input frames> --width <Width of input frames> --height <Height of input frames> 
+# To get a model w/ custom op
+python tools/pytorch2onnx.py --input_model <Pytorch model> --output_dir <Output dir to store ONNX model> --nif <Number of input frames> --width <Width of input frames> --height <Height of input frames> --custom_op
 ```
 
 ## ONNX to OpenVINO IR
 
-Then the ONNX model needs to be further converted to OpenVINO Intermediate Representation (IR) formats. Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR. There is a sample command to convert onnx model to OpenVINO IR:
+Then the ONNX model needs to be further converted to OpenVINO Intermediate Representation (IR) formats. Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR.
+First, install new mo and pot python package.
 ```bash
-mo --input_model <onnx model path> --output_dir <path to store openvino IR> 
+mkdir <PATH_TO_THIS_PROJECT>/wheels
+cd <PATH_TO_THIS_PROJECT>/tools/mo && python3 setup.py build && python3 setup.py bdist_wheel --dist-dir=<PATH_TO_THIS_PROJECT>/wheels
+cd <PATH_TO_THIS_PROJECT>/tools/pot && python3 setup.py build && python3 setup.py bdist_wheel --dist-dir=<PATH_TO_THIS_PROJECT>/wheels
+sudo python3 -m pip install <PATH_TO_THIS_PROJECT>/wheels/*
+rm -r <PATH_TO_THIS_PROJECT>/wheels
+```
+You could uninstall mo and pot with:
+```bash
+sudo python3 -m pip uninstall <PATH_TO_THIS_PROJECT>/wheels/*
+```
+Then, run `mo.py` with the following command:
+```bash
+cd <PATH_TO_THIS_PROJECT>
+python3 tools/mo/openvino/tools/mo/mo.py --input_model <ONNX model> --extensions tools/mo/openvino/tools/mo/custom_op_mo_extension/ --extension_for_ngraph_validation bin/intel64/lib/libcustom_extension.so --output_dir <Output dir to store OpenVINO IR>
 ```
-
 
  Refer to [Convert model with Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information. 
 
@@ -23,10 +41,32 @@ mo --input_model <onnx model path> --output_dir <path to store openvino IR>
 
 Since the precision of original Pytorch model may be FP32 or FP16, in order to cut down the memory cost and accelerate the inference process you may need to perform model quantization. OpenVINO provides a Post-training Optimization Tool (POT) which supports the uniform integer quantization method. There is a sample script `quantization.py` to perform INT8 quantization . You may perform INT8 quantization by command:
 ```bash
-python3 quantization.py --input_model <xml model to be quantized> --dataset_path <path of calibration dataset> --sub_folder <scenario sub-folder of calibration dataset> --nif <number of input frames>  
+python3 quantization.py --input_model <XML model to be quantized> --dataset_path <Path of calibration dataset> --sub_folder <Scenario sub-folder of calibration dataset> --nif <Number of input frames>  
 ```
 
 
 Refer to [Post-training Optimization Tool](https://docs.openvino.ai/latest/pot_introduction.html) for more information.
 
-## OpenVINO IR Inference
\ No newline at end of file
+## OpenVINO IR Inference
+
+Last but not least, you can perform the inference by OpenVINO IR. There is a python sample code `basicvsr_inference_sample.py` implements the model inference. You can start the inference by command:
+
+```bash
+cd <PATH_TO_THIS_PROJECT>
+
+# Perform inference w/o patch evaluation
+python3 vsr_opt/samples/basicvsr_inference_sample.py --model <XML model> --input <Input frames> --number_input_frames 3 --device GPU --extensions bin/intel64/lib/libcustom_extension.so --path_to_cldnn_config flow_warp_custom_op/flow_warp.xml --save_path <Output dir to store predictions>
+
+# Perform inference w/ patch evaluation
+python3 vsr_opt/samples/basicvsr_inference_sample.py --model <XML model> --input <Input frames> --number_input_frames 3 --device GPU --extensions bin/intel64/lib/libcustom_extension.so --path_to_cldnn_config flow_warp_custom_op/flow_warp.xml --save_path <Output dir to store predictions> --patch_evaluation
+```
+
+Also, there is a C++ sample code. You can run `<PATH_TO_THIS_PROJECT>/bin/intel64/basicvsr -h` to get help messages and see the default settings of parameters. The followings are some examples to perform BasicVSR inference.
+
+```bash
+
+# Run the inference w/o patch evaluation on CPU
+<PATH_TO_THIS_PROJECT>/bin/intel64/basicvsr_sample -model_path=<IR model path(.xml)> -extension=<PATH_TO_THIS_PROJECT>/bin/intel64/lib/libcustom_extension.so -data_path=<Directory path including input frames> -nif=<Number of input frames> -save_predictions -save_path=<Directory path to save results> -cldnn_config=<PATH_TO_THIS_PROJECT>/flow_warp_custom_op/flow_warp.xml
+
+# Run the inference w/ patch evaluation on GPU
+<PATH_TO_THIS_PROJECT>/bin/intel64/basicvsr_sample -model_path=<IR model path(.xml)> -extension=<PATH_TO_THIS_PROJECT>/bin/intel64/lib/libcustom_extension.so -data_path=<Directory path including input frames> -nif=<Number of input frames> -patch_evalution -device=GPU -save_predictions -save_path=<Directory path to save results> -cldnn_config=<PATH_TO_THIS_PROJECT>/flow_warp_custom_op/flow_warp.xml
\ No newline at end of file
diff --git a/vsr_opt/samples/ReadMe.md b/vsr_opt/samples/ReadMe.md
deleted file mode 100755
index d964487314..0000000000
--- a/vsr_opt/samples/ReadMe.md
+++ /dev/null
@@ -1,31 +0,0 @@
-# BasicVSR Sample
-
-There are several python scripts about basicvsr model inference and quantization.
-
-## Inference
-You can perform basicvsr inference by running `basicvsr_infer.py`. Before inference, you need to prepare the model and data. The model can be either in *.onnx or *.xml format. For the inference data, you need to organize the data with low resolution in `LR` directory and with high resolution in `HR` directory. You can provide the data path before `LR` and `HR` as a parameter.
-
-Below is an example command to perform model inference:
-```bash
-python3 basicvsr_infer.py --ir_model_path ./models/input_1_3_3_1080_1920.xml --data_path ./dataset/cuc/ --data_subfolder 000 --nif 3 --save_predictions
-```
-In the example, the data is organized as:
-```text
-basicvsr_sample
-    |---dataset
-        |---cuc
-            |---LR
-                |--000
-            |---HR
-                |---000 
-```
-
-## Quantization
-
-The model quantization workflow is defined in `quantization_basicvsr.py`. You can choose either default quantization algorithm or accuarcy aware quantization algorithm. You need to provide the model to be quantized and the dataset for validation.
-
-Below is an example command to perform default quantization:
-```bash
-python3 quantization_basicvsr.py --model_path ./models --model_name input_1_5_3_1080_1920 --dataset_path ./dataset/cuc/ --sub_folders 000 --nif 3
-```
-
diff --git a/vsr_opt/samples/basicvsr_infer.py b/vsr_opt/samples/basicvsr_infer.py
deleted file mode 100755
index cac30c9555..0000000000
--- a/vsr_opt/samples/basicvsr_infer.py
+++ /dev/null
@@ -1,256 +0,0 @@
-#!/usr/bin/env python
-# coding=utf-8
-
-from openvino.runtime import Core
-import torch
-import numpy as np
-import cv2
-import mmcv
-from tqdm import tqdm
-
-from basicvsr_metrics import BasicvsrMetrics
-from quantization_dataloader import FramesLoader
-import argparse
-import os
-from torchvision.utils import make_grid
-import math
-
-from crop_restore_image import restore_crop_from_patch,crop_images_to_patch,crop_images_to_random_patch,restore_random_crop_from_patch
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(description='Model Inference')
-    parser.add_argument(
-        '--ir_model_path',
-        default='',
-        type=str,
-        help='Optional. Path to store openvino ir model')
-    parser.add_argument(
-        '--onnx_model_path',
-        default='',
-        type=str,
-        help='Optional. Path to store onnx model') 
-    parser.add_argument(
-        '--data_path',
-        default='./dataset/cuc',
-        type=str,
-        help='Need. Dataset path ') 
-    parser.add_argument(
-        '--data_subfolder',
-        default='000',  # cuc:029 || reds:'000', '011', '015', '020' || new cuc: 000 001 004 006 008
-        type=str,
-        help='Need. Dataset subfolder ') 
-    parser.add_argument(
-        '--nif',
-        default=3,
-        type=int,
-        help='Need. Number of input frames')
-    parser.add_argument(
-        '--extension',
-        default='',
-        type=str,
-        help='Optional. Extension (.so or .dll) path ') 
-    parser.add_argument(
-        '--device',
-        default='CPU',
-        type=str,
-        help='Optional. CPU or GPU ') 
-    parser.add_argument('--save_predictions', 
-        action='store_true', 
-        help='Optional. Save the model predictions as images')   
-    parser.add_argument('--patch_evalution', 
-        action='store_true', 
-        help='Optional. Evalute by image patches')  
-    parser.add_argument(
-        '--save_path',
-        default='./outputs/',
-        type=str,
-        help='Optional.Path to save predictions')
-    args = parser.parse_args()
-    return args
-
-def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):
-    """Convert torch Tensors into image numpy arrays.
-
-    After clamping to (min, max), image values will be normalized to [0, 1].
-
-    For differnet tensor shapes, this function will have different behaviors:
-
-        1. 4D mini-batch Tensor of shape (N x 3/1 x H x W):
-            Use `make_grid` to stitch images in the batch dimension, and then
-            convert it to numpy array.
-        2. 3D Tensor of shape (3/1 x H x W) and 2D Tensor of shape (H x W):
-            Directly change to numpy array.
-
-    Note that the image channel in input tensors should be RGB order. This
-    function will convert it to cv2 convention, i.e., (H x W x C) with BGR
-    order.
-
-    Args:
-        tensor (Tensor | list[Tensor]): Input tensors.
-        out_type (numpy type): Output types. If ``np.uint8``, transform outputs
-            to uint8 type with range [0, 255]; otherwise, float type with
-            range [0, 1]. Default: ``np.uint8``.
-        min_max (tuple): min and max values for clamp.
-
-    Returns:
-        (Tensor | list[Tensor]): 3D ndarray of shape (H x W x C) or 2D ndarray
-        of shape (H x W).
-    """
-    if not (torch.is_tensor(tensor) or
-            (isinstance(tensor, list)
-             and all(torch.is_tensor(t) for t in tensor))):
-        raise TypeError(
-            f'tensor or list of tensors expected, got {type(tensor)}')
-
-    if torch.is_tensor(tensor):
-        tensor = [tensor]
-    result = []
-    for _tensor in tensor:
-        # Squeeze two times so that:
-        # 1. (1, 1, h, w) -> (h, w) or
-        # 3. (1, 3, h, w) -> (3, h, w) or
-        # 2. (n>1, 3/1, h, w) -> (n>1, 3/1, h, w)
-        _tensor = _tensor.squeeze(0).squeeze(0)
-        _tensor = _tensor.float().detach().cpu().clamp_(*min_max)
-        _tensor = (_tensor - min_max[0]) / (min_max[1] - min_max[0])
-        n_dim = _tensor.dim()
-        if n_dim == 4:
-            img_np = make_grid(
-                _tensor, nrow=int(math.sqrt(_tensor.size(0))),
-                normalize=False).numpy()
-            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
-        elif n_dim == 3:
-            img_np = _tensor.numpy()
-            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
-        elif n_dim == 2:
-            img_np = _tensor.numpy()
-        else:
-            raise ValueError('Only support 4D, 3D or 2D tensor. '
-                             f'But received with dimension: {n_dim}')
-        if out_type == np.uint8:
-            img_np = (img_np * 255.0).round()
-
-        img_np.astype(out_type)
-        result.append(img_np)
-    result = result[0] if len(result) == 1 else result
-    return result
-
-def model_inference(model_path,input):
-    ie = Core()
-    input_model=ie.read_model(model=model_path)
-    compiled_model = ie.compile_model(model=input_model,device_name="CPU")
-    
-    input_layer = next(iter(compiled_model.inputs))
-    output_layer = next(iter(compiled_model.outputs))
-
-    res = compiled_model(inputs=[input])[output_layer]
-
-    return res
-
-def inference(compiled_model,output_layer,dataloader,metrics,patch_evalution=False):
-    results = []
-    acc_psnr = []
-    acc_ssim = []
-    for idx,(input,target) in tqdm(enumerate(dataloader),total = len(dataloader)):
-        if patch_evalution:
-            input_patch_list = crop_images_to_random_patch(input,[190,340], [2,2], overlapp_size=[0, 0], scale=1)
-            pred_patch_list = []
-            for idx in range(len(input_patch_list)):
-                input_patch = input_patch_list[idx]
-                pred_patch = compiled_model(inputs=[input_patch])[output_layer]
-                pred_patch_list.append(pred_patch)
-            pred = restore_random_crop_from_patch(pred_patch_list, ori_size=[720,1280], crop_size=[380,680], blocks=[2,2], padding=0, scales=1)
-        else:
-            pred = compiled_model(inputs=[input])[output_layer]
-        results.append(pred)
-        metrics.update(pred,target)
-        acc_psnr.append(np.array(metrics.value['PSNR']).mean())
-        acc_ssim.append(np.array(metrics.value['SSIM']).mean())
-    
-    print(f'PSNR:{acc_psnr}')
-    print(f'SSIM:{acc_ssim}')
-    print(f'Avg PSNR:{np.array(acc_psnr).mean()}, Avg SSIM:{np.array(acc_ssim).mean()}')
-    return metrics.avg_value,results
-
-def accuracy_drop(ir_acc,onnx_acc):
-    drop_acc = {}
-    drop_percentage = {}
-    for key in ir_acc.keys():
-        key_drop = onnx_acc[key] - ir_acc[key] 
-        drop_acc[key] = key_drop
-        drop_percentage[key] = key_drop/onnx_acc[key]
-    return drop_acc,drop_percentage
-
-
-def save_outputs(predictions,save_path,type,data_file_list):
-    save_file_path = os.path.join(save_path,type)
-    for id in range(len(predictions)):
-        output = predictions[id]
-        output = torch.from_numpy(output)
-        if output.ndim==5:
-            for i in range(0,output.shape[1]):
-                output_i = tensor2img(output[:,i,:,:,:])
-                file_dir = data_file_list[id*output.shape[1]+i].split('/')[-2]
-                file_name = data_file_list[id*output.shape[1]+i].split('/')[-1]
-                rela_file_path = os.path.join(file_dir,file_name)
-                res = mmcv.imwrite(output_i,os.path.join(save_file_path,rela_file_path))
-        elif output.ndim == 4:  
-            output_i = tensor2img(output)
-            file_path = data_file_list[id].split('/')[-2]
-            file_name = data_file_list[id].split('/')[-1]
-            res = mmcv.imwrite(output_i,os.path.join(save_file_path,os.path.join(file_path,file_name)))
-
-        else:
-            raise Exception('The output cannot be saved '
-                            'for a model with wrong output dims')
-            
-        if res == False:
-            print("Failed to save image")
-
-
-
-def dataset_main():
-    args = parse_args()
-    dataloader = FramesLoader(dataset_path=args.data_path,LR_or_HR=['LR','HR'],sub_folder=args.data_subfolder,num_input_frames=args.nif)
-    # metrics
-    ir_metric = BasicvsrMetrics(['PSNR','SSIM'],False)
-    onnx_metric = BasicvsrMetrics(['PSNR','SSIM'],False)
-
-    ie = Core()
-    # add extension: flow_warp_op
-    if args.extension != '':
-        # ie.add_extension('../openvino/bin/intel64/Release/lib/libcustom_extension.so') 
-        ie.add_extension(args.extension) 
-    if args.ir_model_path != '':
-        ir_input_model=ie.read_model(model=args.ir_model_path)
-        ir_compiled_model = ie.compile_model(model=ir_input_model,device_name=args.device)
-        ir_input_layer = next(iter(ir_compiled_model.inputs))
-        ir_output_layer = next(iter(ir_compiled_model.outputs))
-        ir_acc,ir_results = inference(ir_compiled_model,ir_output_layer,dataloader,ir_metric,args.patch_evalution)
-        print(f'Quantized model metrics: {ir_acc}')
-
-    # onnx
-    if args.onnx_model_path != '':
-        onnx_input_model=ie.read_model(model=args.onnx_model_path)
-        onnx_compiled_model = ie.compile_model(model=onnx_input_model,device_name=args.device)
-        onnx_input_layer = next(iter(onnx_compiled_model.inputs))
-        onnx_output_layer = next(iter(onnx_compiled_model.outputs))
-        onnx_acc,onnx_results = inference(onnx_compiled_model,onnx_output_layer,dataloader,onnx_metric,args.patch_evalution)
-        print(f'Original model metrics: {onnx_acc}')
-
-    if args.ir_model_path != '' and args.onnx_model_path != '':
-        acc_drop,drop_percentage = accuracy_drop(ir_acc,onnx_acc)
-        print(f'Drop accuracy:{acc_drop}')
-        print(f'Drop percentage:{drop_percentage}')
-
-    if args.save_predictions:
-        if args.ir_model_path != '':
-            save_outputs(ir_results,args.save_path,'ir',dataloader._lq_files)
-        if args.onnx_model_path != '':
-            save_outputs(onnx_results,args.save_path,'onnx',dataloader._lq_files)
-        print('Finished image saving')
-
-
-if __name__=='__main__':
-    dataset_main()
\ No newline at end of file
diff --git a/vsr_opt/samples/basicvsr_metrics.py b/vsr_opt/samples/basicvsr_metrics.py
deleted file mode 100755
index b211f872b9..0000000000
--- a/vsr_opt/samples/basicvsr_metrics.py
+++ /dev/null
@@ -1,327 +0,0 @@
-#!/usr/bin/env python
-# coding=utf-8
-import numpy as np
-import torch
-import cv2
-
-import mmcv
-from openvino.tools.pot import Metric
-from torchvision.utils import make_grid
-import math
-from openvino.tools.pot.utils.logger import get_logger
-logger = get_logger(__name__)
-
-def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):
-    """Convert torch Tensors into image numpy arrays. from: mmedit/core/misc.py
-
-    After clamping to (min, max), image values will be normalized to [0, 1].
-
-    For differnet tensor shapes, this function will have different behaviors:
-
-        1. 4D mini-batch Tensor of shape (N x 3/1 x H x W):
-            Use `make_grid` to stitch images in the batch dimension, and then
-            convert it to numpy array.
-        2. 3D Tensor of shape (3/1 x H x W) and 2D Tensor of shape (H x W):
-            Directly change to numpy array.
-
-    Note that the image channel in input tensors should be RGB order. This
-    function will convert it to cv2 convention, i.e., (H x W x C) with BGR
-    order.
-
-    Args:
-        tensor (Tensor | list[Tensor]): Input tensors.
-        out_type (numpy type): Output types. If ``np.uint8``, transform outputs
-            to uint8 type with range [0, 255]; otherwise, float type with
-            range [0, 1]. Default: ``np.uint8``.
-        min_max (tuple): min and max values for clamp.
-
-    Returns:
-        (Tensor | list[Tensor]): 3D ndarray of shape (H x W x C) or 2D ndarray
-        of shape (H x W).
-    """
-    if not (torch.is_tensor(tensor) or
-            (isinstance(tensor, list)
-             and all(torch.is_tensor(t) for t in tensor))):
-        raise TypeError(
-            f'tensor or list of tensors expected, got {type(tensor)}')
-
-    if torch.is_tensor(tensor):
-        tensor = [tensor]
-    result = []
-    for _tensor in tensor:
-        # Squeeze two times so that:
-        # 1. (1, 1, h, w) -> (h, w) or
-        # 3. (1, 3, h, w) -> (3, h, w) or
-        # 2. (n>1, 3/1, h, w) -> (n>1, 3/1, h, w)
-        _tensor = _tensor.squeeze(0).squeeze(0)
-        _tensor = _tensor.float().detach().cpu().clamp_(*min_max)
-        _tensor = (_tensor - min_max[0]) / (min_max[1] - min_max[0])
-        n_dim = _tensor.dim()
-        if n_dim == 4:
-            img_np = make_grid(
-                _tensor, nrow=int(math.sqrt(_tensor.size(0))),
-                normalize=False).numpy()
-            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
-        elif n_dim == 3:
-            img_np = _tensor.numpy()
-            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
-        elif n_dim == 2:
-            img_np = _tensor.numpy()
-        else:
-            raise ValueError('Only support 4D, 3D or 2D tensor. '
-                             f'But received with dimension: {n_dim}')
-        if out_type == np.uint8:
-            # Unlike MATLAB, numpy.unit8() WILL NOT round by default.
-            img_np = (img_np * 255.0).round()
-        img_np.astype(out_type)
-        result.append(img_np)
-    result = result[0] if len(result) == 1 else result
-    return result
-
-def reorder_image(img, input_order='HWC'):
-    """Reorder images to 'HWC' order.
-
-    If the input_order is (h, w), return (h, w, 1);
-    If the input_order is (c, h, w), return (h, w, c);
-    If the input_order is (h, w, c), return as it is.
-
-    Args:
-        img (ndarray): Input image.
-        input_order (str): Whether the input order is 'HWC' or 'CHW'.
-            If the input image shape is (h, w), input_order will not have
-            effects. Default: 'HWC'.
-
-    Returns:
-        ndarray: reordered image.
-    """
-
-    if input_order not in ['HWC', 'CHW']:
-        raise ValueError(
-            f'Wrong input_order {input_order}. Supported input_orders are '
-            '"HWC" and "CHW"')
-    if len(img.shape) == 2:
-        img = img[..., None]
-        return img
-    if input_order == 'CHW':
-        img = img.transpose(1, 2, 0)
-    return img
-
-def psnr(img1, img2, crop_border=0, input_order='HWC', convert_to=None):
-    """Calculate PSNR (Peak Signal-to-Noise Ratio).
-
-    Ref: https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio
-
-    Args:
-        img1 (ndarray): Images with range [0, 255].
-        img2 (ndarray): Images with range [0, 255].
-        crop_border (int): Cropped pixels in each edges of an image. These
-            pixels are not involved in the PSNR calculation. Default: 0.
-        input_order (str): Whether the input order is 'HWC' or 'CHW'.
-            Default: 'HWC'.
-        convert_to (str): Whether to convert the images to other color models.
-            If None, the images are not altered. When computing for 'Y',
-            the images are assumed to be in BGR order. Options are 'Y' and
-            None. Default: None.
-
-    Returns:
-        float: psnr result.
-    """
-
-    assert img1.shape == img2.shape, (
-        f'Image shapes are differnet: {img1.shape}, {img2.shape}.')
-    if input_order not in ['HWC', 'CHW']:
-        raise ValueError(
-            f'Wrong input_order {input_order}. Supported input_orders are '
-            '"HWC" and "CHW"')
-    img1 = reorder_image(img1, input_order=input_order)
-    img2 = reorder_image(img2, input_order=input_order)
-
-    img1, img2 = img1.astype(np.float32), img2.astype(np.float32)
-    if isinstance(convert_to, str) and convert_to.lower() == 'y':
-        img1 = mmcv.bgr2ycbcr(img1 / 255., y_only=True) * 255.
-        img2 = mmcv.bgr2ycbcr(img2 / 255., y_only=True) * 255.
-    elif convert_to is not None:
-        raise ValueError('Wrong color model. Supported values are '
-                         '"Y" and None.')
-
-    if crop_border != 0:
-        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, None]
-        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, None]
-
-    mse_value = np.mean((img1 - img2)**2)
-    if mse_value == 0:
-        # return float('inf')
-        return float(10e5)
-    return 20. * np.log10(255. / np.sqrt(mse_value))
-
-def _ssim(img1, img2):
-    """Calculate SSIM (structural similarity) for one channel images.
-
-    It is called by func:`calculate_ssim`.
-
-    Args:
-        img1, img2 (ndarray): Images with range [0, 255] with order 'HWC'.
-
-    Returns:
-        float: ssim result.
-    """
-
-    C1 = (0.01 * 255)**2
-    C2 = (0.03 * 255)**2
-
-    img1 = img1.astype(np.float64)
-    img2 = img2.astype(np.float64)
-    kernel = cv2.getGaussianKernel(11, 1.5)
-    window = np.outer(kernel, kernel.transpose())
-
-    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]
-    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]
-    mu1_sq = mu1**2
-    mu2_sq = mu2**2
-    mu1_mu2 = mu1 * mu2
-    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq
-    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq
-    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2
-
-    ssim_map = ((2 * mu1_mu2 + C1) *
-                (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *
-                                       (sigma1_sq + sigma2_sq + C2))
-    return ssim_map.mean()
-
-
-def ssim(img1, img2, crop_border=0, input_order='HWC', convert_to=None):
-    """Calculate SSIM (structural similarity).
-
-    Ref:
-    Image quality assessment: From error visibility to structural similarity
-
-    The results are the same as that of the official released MATLAB code in
-    https://ece.uwaterloo.ca/~z70wang/research/ssim/.
-
-    For three-channel images, SSIM is calculated for each channel and then
-    averaged.
-
-    Args:
-        img1 (ndarray): Images with range [0, 255].
-        img2 (ndarray): Images with range [0, 255].
-        crop_border (int): Cropped pixels in each edges of an image. These
-            pixels are not involved in the SSIM calculation. Default: 0.
-        input_order (str): Whether the input order is 'HWC' or 'CHW'.
-            Default: 'HWC'.
-        convert_to (str): Whether to convert the images to other color models.
-            If None, the images are not altered. When computing for 'Y',
-            the images are assumed to be in BGR order. Options are 'Y' and
-            None. Default: None.
-
-    Returns:
-        float: ssim result.
-    """
-
-    assert img1.shape == img2.shape, (
-        f'Image shapes are differnet: {img1.shape}, {img2.shape}.')
-    if input_order not in ['HWC', 'CHW']:
-        raise ValueError(
-            f'Wrong input_order {input_order}. Supported input_orders are '
-            '"HWC" and "CHW"')
-    img1 = reorder_image(img1, input_order=input_order)
-    img2 = reorder_image(img2, input_order=input_order)
-
-    if isinstance(convert_to, str) and convert_to.lower() == 'y':
-        img1, img2 = img1.astype(np.float32), img2.astype(np.float32)
-        img1 = mmcv.bgr2ycbcr(img1 / 255., y_only=True) * 255.
-        img2 = mmcv.bgr2ycbcr(img2 / 255., y_only=True) * 255.
-        img1 = np.expand_dims(img1, axis=2)
-        img2 = np.expand_dims(img2, axis=2)
-    elif convert_to is not None:
-        raise ValueError('Wrong color model. Supported values are '
-                         '"Y" and None')
-
-    if crop_border != 0:
-        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, None]
-        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, None]
-
-    ssims = []
-    for i in range(img1.shape[2]):
-        ssims.append(_ssim(img1[..., i], img2[..., i]))
-    return np.array(ssims).mean()
-
-
-class BasicvsrMetrics(Metric):
-    def __init__(self,metrics_name,quantization=True):
-        self._metrics = metrics_name
-        self._results = dict()
-        self.allowed_metrics = {'PSNR': psnr, 'SSIM': ssim}
-        self.for_accuracy_aware_quantization = quantization
-        for metric in self._metrics:
-            assert metric in self.allowed_metrics, (
-                f'The metric:{metric} not in supported metrics:{self._metrics}')
-            self._results[metric]= []
-    
-    @property
-    def value(self):
-        results = dict()
-        for metric in self._metrics:
-            values_num = len(self._results[metric])
-            results[metric] = self._results[metric][values_num-1]
-        return results
-
-    @property
-    def avg_value(self):
-        results = dict()
-        for key in self._results.keys():
-            logger.info(f'metric {key}: {self._results[key]}')
-            results[key] = np.mean(self._results[key])
-        return results
-
-
-    def update(self, output, target,crop_border=0,convert_to=None):
-        """Calculte psnr and ssim metrics
-
-        Refer to: basicvsr.py:evaluate()
-
-        """
-        if self.for_accuracy_aware_quantization:
-            output = torch.from_numpy(output[0])  # dim=4: (n,c,h,w) # for accuracy-aware quantization
-            target = torch.from_numpy(target[0])
-        else:
-            output = torch.from_numpy(output)     # dim=5: (b,n,c,h,w) # for basicVSR_infer
-            target = torch.from_numpy(target)
-        for metric in self._metrics:
-            assert metric in self.allowed_metrics, (
-                f'The metric:{metric} not in supported metrics:{self._metrics}')
-            
-            if output.ndim == 5:  # a sequence: (n, t, c, h, w)
-                avg = []
-                for i in range(0, output.size(1)):
-                    output_i = tensor2img(output[:, i, :, :, :])
-                    gt_i = tensor2img(target[:, i, :, :, :])
-                    avg.append(self.allowed_metrics[metric](output_i, gt_i, crop_border, convert_to=convert_to))
-                self._results[metric].append(avg)
-
-            elif output.ndim == 4:  # an image: (n, c, h, w), for Vimeo-90K-T
-                output_img = tensor2img(output)
-                gt_img = tensor2img(target)
-                value = self.allowed_metrics[metric](
-                    output_img, gt_img, crop_border, convert_to=convert_to)
-                # self._results[metric] = value
-                self._results[metric].append(value)
-
-            else:
-                raise Exception('The metric cannot be calculated '
-                            'for a model with wrong output dims')
-        
-
-    def reset(self):
-        for metric in self._metrics:
-            assert metric in self.allowed_metrics, (
-                f'The metric:{metric} not in supported metrics:{self._metrics}')
-            self._results[metric]= []
-
-    def get_attributes(self):
-        return {'PSNR': {'direction': 'higher-better',
-                             'type': 'accuracy'},
-                'SSIM':{'direction':'higher-better',
-                             'type':'accuracy'}}
-
-    
-
diff --git a/vsr_opt/samples/crop_restore_image.py b/vsr_opt/samples/crop_restore_image.py
deleted file mode 100755
index 35a8e9aa41..0000000000
--- a/vsr_opt/samples/crop_restore_image.py
+++ /dev/null
@@ -1,335 +0,0 @@
-import os
-from tqdm import tqdm
-import json
-import shutil
-from PIL import Image
-from joblib import Parallel, delayed
-import numpy as np
-import cv2 as cv
-def check_dir(path):
-    if not os.path.exists(path):
-        os.makedirs(path)
-from copy import deepcopy
-
-def is_image_file(filename):
-    return any(filename.endswith(extension) for extension in ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', ])
-
-
-def caulate_random_crop_coordinate(ori_size, crop_size, blocks):
-    [width, height] = ori_size   
-    [crop_width,crop_height] = crop_size
-    inter_width = (crop_width * blocks[0] - width) // (blocks[0] - 1)
-    last_fill_width = (crop_width * blocks[0] - width) % (blocks[0] - 1)
-    inter_height = (crop_height * blocks[1] - height) // (blocks[1] - 1)
-    last_fill_height = (crop_height * blocks[1] - height) % (blocks[1] - 1)
-    crop_coordinate_list = []
-    for i_ in range(blocks[0]):
-        for j_ in range(blocks[1]):
-            x1 = (crop_width - inter_width) * i_
-            y1 = (crop_height - inter_height) * j_
-            if i_ == blocks[0] - 1:
-                x1 = x1 - last_fill_width
-            if j_ == blocks[1] - 1:
-                y1 = y1 - last_fill_height
-            crop_coordinate_list.append((x1, y1, x1 + crop_width, y1 + crop_height))
-   return crop_coordinate_list,blocks
-
-
-def caulate_crop_coordinate(ori_size, crop_size, blocks):
-    [width, height] = ori_size 
-    inter_width = (crop_size * blocks[0] - width) // (blocks[0] - 1)
-    last_fill_width = (crop_size * blocks[0] - width) % (blocks[0] - 1)
-    inter_height = (crop_size * blocks[1] - height) // (blocks[1] - 1)
-    last_fill_height = (crop_size * blocks[1] - height) % (blocks[1] - 1)
-    crop_coordinate_list = []
-    for i_ in range(blocks[0]):
-        for j_ in range(blocks[1]):
-            x1 = (crop_size - inter_width) * i_
-            y1 = (crop_size - inter_height) * j_
-            if i_ == blocks[0] - 1:
-                x1 = x1 - last_fill_width
-            if j_ == blocks[1] - 1:
-                y1 = y1 - last_fill_height
-            crop_coordinate_list.append((x1, y1, x1 + crop_size, y1 + crop_size))
-    return crop_coordinate_list,blocks
-
-
-def caulate_crop_coordinate_padding(ori_size: list, crop_size: int, overlapp_size: list):
-    [width, height] = ori_size 
-    [overlapp_w, overlapp_h] = overlapp_size
-    block_w = (width - crop_size) / (crop_size - overlapp_w)
-    if block_w % 1 != 0:
-        block_w = int(block_w) + 2
-    else:
-        block_w = int(block_w) + 1
-    block_h = (height - crop_size) / (crop_size - overlapp_h)
-    if block_h % 1 != 0:
-        block_h = int(block_h) + 2
-    else:
-        block_h = int(block_h) + 1
-
-    if (height) <= crop_size:
-        block_h = 1
-    crop_coordinate_list = []
-    for i_ in range(block_w):
-        for j_ in range(block_h):
-            x1 = (crop_size - overlapp_w) * i_
-            y1 = (crop_size - overlapp_h) * j_
-            if (height) <= crop_size:
-                crop_coordinate_list.append((x1, 0, x1 + crop_size, height))
-            else:
-                crop_coordinate_list.append((x1, y1, x1 + crop_size, y1 + crop_size))
-
-    return crop_coordinate_list, [block_w, block_h]
-
-def crop_img(img_path, save_path, crop_size, blocks, overlapp_size=[0, 0], scale=1):
-    images_list = []
-
-    for img in os.listdir(img_path):
-        path = os.path.join(img_path, img)
-        images_list.append(path)
-
-    ori_size = Image.open(images_list[0]).size
-    ori_size = list(int(a / scale) for a in ori_size)
-
-    if overlapp_size[0] == 0:
-        crop_coordinate_list, cal_blocks = caulate_crop_coordinate(ori_size, crop_size, blocks)
-    else:
-        crop_coordinate_list, cal_blocks = caulate_crop_coordinate_padding(ori_size, crop_size, overlapp_size)
-
-    if overlapp_size[0] != 0:
-        save_path = os.path.join(save_path,
-                                 str(crop_size).zfill(3) + '_' + str(overlapp_size[0]) + '_' + str(overlapp_size[1]))
-    else:
-        save_path = os.path.join(save_path, str(crop_size).zfill(3) + '_' + str(blocks[0]) + '_' + str(blocks[1]))
-
-    check_dir(save_path)
-    crop_coordinate_list = [tuple(coo * scale for coo in corps) for corps in crop_coordinate_list]
-
-    for sequ in os.listdir(img_path):
-        for idx in range(len(crop_coordinate_list)):
-            check_dir(os.path.join(save_path, str(idx + 1).zfill(3), sequ))
-
-    def crop_img(croped_img_path):
-        img = Image.open(croped_img_path)
-        img_old_path, img_name = os.path.split(croped_img_path)
-        img_sequence = os.path.split(img_old_path)[-1]
-        for idx, coordinate in enumerate(crop_coordinate_list):
-            crooped = img.crop(coordinate)
-            save_img_path = os.path.join(save_path, str(idx + 1).zfill(3), img_sequence)
-            check_dir(save_img_path)
-            crooped.save(os.path.join(save_img_path, img_name))
-
-    for img_path in tqdm(images_list):
-       crop_img(img_path)
-
-
-def crop_images_to_patch(images,crop_size, blocks, overlapp_size=[0, 0], scale=1):
-    b,n,c,h,w = images.shape
-    ori_size = [h,w]
-    scale_size = list(int(a / scale) for a in ori_size)
-    if overlapp_size[0] == 0:
-        crop_coordinate_list, cal_blocks = caulate_crop_coordinate(scale_size, crop_size, blocks)
-    else:
-        crop_coordinate_list, cal_blocks = caulate_crop_coordinate_padding(scale_size, crop_size, overlapp_size)
-    crop_coordinate_list = [tuple(coo * scale for coo in corps) for corps in crop_coordinate_list]
-    def crop_img(image):
-        croped_list =[]
-        for idx, coordinate in enumerate(crop_coordinate_list):
-            cropped_img = images[:,:,:,coordinate[0]:coordinate[2],coordinate[1]:coordinate[3]]
-            _,_,_,c_h,c_w = cropped_img.shape
-            if c_h < crop_size or c_w < crop_size:
-                cropped = np.zeros((b,n,c,crop_size,crop_size))
-                cropped[:,:,:,:c_h,:c_w] = cropped[:,:,:,:c_h,:c_w] + cropped_img
-                cropped_img = cropped
-            croped_list.append(cropped_img)
-
-        return croped_list
-    
-    crop_list = crop_img(images)
-    return crop_list
-
-def crop_images_to_random_patch(images,crop_size, blocks, overlapp_size=[0, 0], scale=1):
-    b,n,c,h,w = images.shape
-    ori_size = [h,w]
-    scale_size = list(int(a / scale) for a in ori_size)
-    if overlapp_size[0] == 0:
-        crop_coordinate_list, cal_blocks = caulate_random_crop_coordinate(scale_size, crop_size, blocks)
-
-    else:
-        crop_coordinate_list, cal_blocks = caulate_crop_coordinate_padding(scale_size, crop_size, overlapp_size)
-    crop_coordinate_list = [tuple(coo * scale for coo in corps) for corps in crop_coordinate_list]
-    def crop_img(image):
-        croped_list =[]
-        for idx, coordinate in enumerate(crop_coordinate_list):
-            cropped_img = images[:,:,:,coordinate[0]:coordinate[2],coordinate[1]:coordinate[3]]
-            _,_,_,c_h,c_w = cropped_img.shape
-            if c_h < crop_size[0] or c_w < crop_size[1]:
-                cropped = np.zeros((b,n,c,crop_size[0],crop_size[1]))
-                cropped[:,:,:,:c_h,:c_w] = cropped[:,:,:,:c_h,:c_w] + cropped_img
-                cropped_img = cropped
-            croped_list.append(cropped_img)
-
-        return croped_list
-    
-    crop_list = crop_img(images)
-    return crop_list
-
-
-def restore_crop_(img_path, save_path, ori_size: list, crop_size: int, blocks, padding=0, scales=1):
-    import pdb;pdb.set_trace()
-    if padding == 0:
-        crop_coordinate_list = caulate_crop_coordinate(ori_size, crop_size, blocks)
-    else:
-        padding = [padding,padding] 
-        crop_coordinate_list,cal_blocks = caulate_crop_coordinate_padding(ori_size, crop_size, padding)
-    crop_coordinate_list = [tuple(coo * scales for coo in corps) for corps in crop_coordinate_list]
-
-    print(crop_coordinate_list)
-
-    img_crop_sequ = os.listdir(img_path)
-    img_crop_sequ.sort(key=lambda x: int(x))
-    img_folder_sequ = os.listdir(os.path.join(img_path, img_crop_sequ[0]))
-    img_folder_sequ = [os.path.join(img_crop_path, img_folder_sequ[0]) for img_crop_path in img_crop_sequ]  
-    for folder in img_folder_sequ:
-        save_sequ_path = os.path.join(save_path, folder)
-        check_dir(save_sequ_path)
-
-    def restore_crop_img(img_restore_list, save_img_path):
-        resotre_size_w = ori_size[0] * scales
-        resotre_size_h = ori_size[1] * scales
-        output_img = np.zeros((resotre_size_h, resotre_size_w, 3), dtype=int)
-        add_nums_np = np.zeros((resotre_size_h, resotre_size_w, 3), dtype=int)
-        crop_img_size = crop_size * scales
-        for img_idx, img_crop_path in enumerate(img_restore_list):
-            teplate_add = np.ones((crop_img_size, crop_img_size, 3), dtype=int)
-            coodinate = crop_coordinate_list[img_idx]
-            img = Image.open(img_crop_path)
-            img_np = np.array(img)
-            print(f'Image patch size:{img_np.shape}')
-            if coodinate[2] > resotre_size_w:
-                img_np = img_np[:, :-(coodinate[2] - resotre_size_w), :]
-                teplate_add = teplate_add[:, :-(coodinate[2] - resotre_size_w), :]
-            if coodinate[3] > resotre_size_h:
-                img_np = img_np[:-(coodinate[3] - resotre_size_h), :, :]
-                teplate_add = teplate_add[:-(coodinate[3] - resotre_size_h), :, :]
-
-            output_img[coodinate[1]:coodinate[3], coodinate[0]:coodinate[2], :] += img_np
-            add_nums_np[coodinate[1]:coodinate[3], coodinate[0]:coodinate[2], :] += teplate_add
-
-        img_result = np.divide(output_img, add_nums_np)
-
-        img = Image.fromarray(img_result.astype('uint8')).convert('RGB')
-        
-        img.save(save_img_path)
-
-    fname_imgs_list = []
-
-    for root, _, fnames in os.walk(os.path.join(img_path, img_folder_sequ[0])):
-        for fname in fnames:
-            if 'png' in fname:
-                fname_imgs_list.append(os.path.join(root, fname))
-
-    def parllel_crop(single_img_path):
-        root, fname = os.path.split(single_img_path)
-        sequ_name = os.path.split(root)[-1]
-        save_sequ_path = os.path.join(save_path, sequ_name)
-        check_dir(save_sequ_path) 
-        img_restore_list = []
-        for crop_sequ in img_crop_sequ:
-            img_restore_list.append(os.path.join(img_path, crop_sequ, sequ_name, fname))
-        restore_crop_img(img_restore_list, os.path.join(save_sequ_path, fname))
-
-    
-    for file_ in tqdm(fname_imgs_list):
-        parllel_crop(file_)
-   
-
-def restore_crop_from_patch(patch_list, ori_size: list, crop_size: int, blocks, padding=0, scales=1):
-    if padding == 0:
-        crop_coordinate_list,cal_blocks = caulate_crop_coordinate(ori_size, crop_size, blocks)
-    else:
-        padding = [padding,padding] 
-        crop_coordinate_list,cal_blocks = caulate_crop_coordinate_padding(ori_size, crop_size, padding)
-    crop_coordinate_list = [tuple(coo * scales for coo in corps) for corps in crop_coordinate_list]
-    def restore_crop_img(img_restore_list):
-        b,n,c,_,_= img_restore_list[0].shape
-        restore_size_h = ori_size[0] * scales 
-        restore_size_w = ori_size[1] * scales  
-        output_img = np.zeros((b,n,c,restore_size_h, restore_size_w), dtype=np.float32)
-        add_nums_np = np.zeros((b,n,c,restore_size_h, restore_size_w), dtype=np.float32)
-        crop_img_size = crop_size * scales
-        for img_idx, img_crop in enumerate(img_restore_list):
-            teplate_add = np.ones((b,n,c,crop_img_size, crop_img_size), dtype=np.float32)
-            coordinate = crop_coordinate_list[img_idx]
-            if coordinate[3] > restore_size_w:
-                img_crop = img_crop[:,:,:, :,:-(coordinate[3] - restore_size_w)]
-                teplate_add = teplate_add[:,:,:,:,:-(coordinate[3] - restore_size_w)]
-            if coordinate[2] > restore_size_h:
-                img_crop = img_crop[:,:,:,:-(coordinate[2] - restore_size_h),:]
-                teplate_add = teplate_add[:,:,:,:-(coordinate[2] - restore_size_h),:]
-
-            output_img[:,:,:,coordinate[0]:coordinate[2], coordinate[1]:coordinate[3]] += img_crop
-            add_nums_np[:,:,:,coordinate[0]:coordinate[2], coordinate[1]:coordinate[3]] += teplate_add
-
-        np.place(add_nums_np,add_nums_np==0,[1])
-        img_result = np.divide(output_img, add_nums_np)
-        return img_result
-
-def restore_random_crop_from_patch(patch_list, ori_size: list, crop_size: int, blocks, padding=0, scales=1):
-    if padding == 0:
-        crop_coordinate_list,cal_blocks = caulate_random_crop_coordinate(ori_size, crop_size, blocks)
-    else:
-        padding = [padding,padding] 
-        crop_coordinate_list,cal_blocks = caulate_crop_coordinate_padding(ori_size, crop_size, padding)
-    crop_coordinate_list = [tuple(coo * scales for coo in corps) for corps in crop_coordinate_list]
-
-    def restore_crop_img(img_restore_list):
-        b,n,c,_,_= img_restore_list[0].shape
-        restore_size_h = ori_size[0] * scales  
-        restore_size_w = ori_size[1] * scales  
-        output_img = np.zeros((b,n,c,restore_size_h, restore_size_w), dtype=np.float32)
-        add_nums_np = np.zeros((b,n,c,restore_size_h, restore_size_w), dtype=np.float32)
-        crop_img_size = crop_size * scales
-        for img_idx, img_crop in enumerate(img_restore_list):
-            teplate_add = np.ones((b,n,c,crop_img_size[0], crop_img_size[1]), dtype=np.float32)
-            coordinate = crop_coordinate_list[img_idx]
-            if coordinate[3] > restore_size_w:
-                img_crop = img_crop[:,:,:, :,:-(coordinate[3] - restore_size_w)]
-                teplate_add = teplate_add[:,:,:,:,:-(coordinate[3] - restore_size_w)]
-            if coordinate[2] > restore_size_h:
-                img_crop = img_crop[:,:,:,:-(coordinate[2] - restore_size_h),:]
-                teplate_add = teplate_add[:,:,:,:-(coordinate[2] - restore_size_h),:]
-
-            output_img[:,:,:,coordinate[0]:coordinate[2], coordinate[1]:coordinate[3]] += img_crop
-            add_nums_np[:,:,:,coordinate[0]:coordinate[2], coordinate[1]:coordinate[3]] += teplate_add
-
-        np.place(add_nums_np,add_nums_np==0,[1])
-        img_result = np.divide(output_img, add_nums_np)
-        return img_result
-
-    restored_results = restore_crop_img(patch_list)
-    return restored_results
-
-    
-
-from basicvsr_metrics import psnr,ssim
-if __name__ == '__main__':
-    test_img = './dataset/reds/LR/000/00000000.png'
-    image = cv.cvtColor(cv.imread(test_img), cv.COLOR_BGR2RGB)
-    input_image = np.transpose(image, (2, 0, 1))
-    input_image = np.expand_dims(input_image,0)
-    input_image = np.expand_dims(input_image,0)
-    crop_list = crop_images_to_random_patch(input_image,[190,340], [2,2], overlapp_size=[0, 0], scale=1)
-    output = restore_random_crop_from_patch(crop_list, ori_size=[360,640], crop_size=[190,340], blocks=[2,2], padding=0, scales=1)
-    output_img = output.squeeze().squeeze()
-    output_img = np.transpose(output_img, (1, 2, 0))
-    psnr_ = psnr(image,output_img)
-    ssim_ = ssim(image,output_img)
-    print(f'psnr:{psnr_}, ssim:{ssim_}')
-    output_img = cv.cvtColor(output_img,cv.COLOR_RGB2BGR)
-
-    cv.imwrite('../test_LR_0.png',output_img)
-
-    
-    
\ No newline at end of file
diff --git a/vsr_opt/samples/quantization_basicvsr.py b/vsr_opt/samples/quantization_basicvsr.py
deleted file mode 100755
index ef5b085398..0000000000
--- a/vsr_opt/samples/quantization_basicvsr.py
+++ /dev/null
@@ -1,149 +0,0 @@
-#!/usr/bin/env python
-# coding=utf-8
-import os
-import sys
-from openvino.tools.pot import IEEngine
-from openvino.tools.pot import load_model,save_model
-from openvino.tools.pot import compress_model_weights
-from openvino.tools.pot import create_pipeline
-from basicvsr_metrics import BasicvsrMetrics
-import argparse
-
-from openvino.tools.pot.utils.logger import init_logger,get_logger
-init_logger(level='INFO')
-logger = get_logger(__name__)
-
-from quantization_dataloader import FramesLoader
-
-def parse_args():
-    parser = argparse.ArgumentParser(description='Model Inference')
-    parser.add_argument(
-        '--model_path',
-        default='./models',
-        type=str,
-        help='Need.= Path to store onnx model') 
-    parser.add_argument(
-        '--model_name',
-        default='input_1_5_3_1080_1920',
-        type=str,
-        help='Need. Model name')
-    parser.add_argument(
-        '--dataset_path',
-        default='./dataset/cuc/',
-        type=str,
-        help='Need. Dataset path')
-    parser.add_argument(
-        '--sub_folders',
-        default='000',
-        type=str,
-        help='Need. Sub-folder of datasets for inference')
-    parser.add_argument(
-        '--nif',
-        default=3,
-        type=int,
-        help='Need. Number of input frames')  
-    parser.add_argument(
-        '--accuracy_aware_quantization',
-        action='store_true', 
-        help='Optional. Quantize by accuracy aware algorithm,otherwise by default algorithm')  
-    parser.add_argument(
-        '--extension',
-        default='',
-        type=str,
-        help='Optional. Extension path for custom operation')
-    args = parser.parse_args()
-    return args
-
-def main():
-    args = parse_args()
-    dataset_path = args.dataset_path
-    sub_folders = args.sub_folders
-    model_path = args.model_path
-    xml_model = args.model_name + '.xml'
-    bin_model = args.model_name + '.bin'
-    num_input_frames = args.nif
-    model_config = {
-        "model_name": "basicvsr",
-        "model": os.path.join(model_path,xml_model),
-        "weights": os.path.join(model_path,bin_model),
-
-    }
-
-    engine_config = {"device": "CPU",
-        'stat_requests_number':1,
-        'eval_requests_number':1
-    }
-    algorithms = []
-    name_prex = ""
-    # AccuracyAwareQuantization
-    if args.accuracy_aware_quantization:
-        algorithms = [
-            {
-                "name": "AccuracyAwareQuantization",
-                "params": {
-                    "target_device": "CPU",
-                    "stat_subset_size": 10,  # for default algorithm
-                    "ranking_subset_size":10,
-                    "max_iter_num":100,
-                    "maximal_drop":0.01,
-                    "drop_type":"relative"
-                },
-            }
-        ]
-        name_prex = "optimized_acc_aware_"
-
-    # DefaultQuantization
-    else:
-        algorithms = [
-            {
-                "name": "DefaultQuantization",
-                "params": {
-                    "target_device": "CPU",
-                    "stat_subset_size": 10,  # for default algorithm
-                },
-            }
-        ]
-        name_prex = "optimized_"
-
-    metrics = ['PSNR','SSIM']
-    # Step 1: Implement data loader
-    data_loader = FramesLoader(dataset_path,sub_folder=sub_folders,num_input_frames=num_input_frames,need_gt=args.accuracy_aware_quantization)  
-    logger.info(f'Dataset path{dataset_path}')
-    logger.info(f'Model to be quantized:{xml_model}')
-
-    # Step 2: Load model
-    model = load_model(model_config=model_config)
-
-    # Step 3: Initialize the engine
-    if args.accuracy_aware_quantization:
-        # Implement Metric
-        metric = BasicvsrMetrics(metrics)
-        engine = IEEngine(config=engine_config, data_loader=data_loader,metric=metric) # accuarcy aware
-    else:
-        engine = IEEngine(config=engine_config, data_loader=data_loader) # Default
-
-    # load extension
-    if args.extension:
-        logger.info(f'Add custom layer extension:{args.extension}')
-        engine._ie.add_extension(args.extension)
-
-    # Step 4: Create pipeline
-    logger.info('Create pipeline and run:')
-    pipeline = create_pipeline(algorithms,engine)
-    compressed_model = pipeline.run(model=model)
-
-    logger.info('Finished pipeline running, begin to compress model weights:')
-
-    # Step 5: Compress model weights to quantized precision
-    compress_model_weights(compressed_model)
-
-    # Step 6: Save the compressed model to desired path
-    logger.info('Finished model weoghts compress, begin to save model')
-    compressed_model_paths = save_model(model=compressed_model,
-                                    save_path = model_path,
-                                        model_name = name_prex + args.model_name
-                                    )
-
-if __name__ == '__main__':
-    main()
-
diff --git a/vsr_opt/samples/quantization_dataloader.py b/vsr_opt/samples/quantization_dataloader.py
deleted file mode 100755
index 8f4c811a4e..0000000000
--- a/vsr_opt/samples/quantization_dataloader.py
+++ /dev/null
@@ -1,100 +0,0 @@
-#!/usr/bin/env python
-# coding=utf-8
-import os
-import numpy as np
-import cv2 as cv
-
-from openvino.tools.pot import DataLoader
-
-class FramesLoader(DataLoader):
-    """ Loads images from a folder """
-    def __init__(self,dataset_path,LR_or_HR=['LR','HR'],sub_folder='',num_input_frames =3,need_gt=True):
-        self._lq_files=[]
-        self._gt_files=[]
-        self._input_frames = num_input_frames
-        self.sub_folder = sub_folder.split(',')
-        if 'LR' in LR_or_HR:
-            folder_path = os.path.join(dataset_path,'LR')
-            self._lq_files = self.get_file_path(folder_path,self.sub_folder)
-
-        if 'HR' in LR_or_HR:
-            folder_path = os.path.join(dataset_path,'HR')
-            self._gt_files = self.get_file_path(folder_path,self.sub_folder)
-
-        self.need_gt = need_gt
-
-    def get_file_path(self,dataset_path,sub_folders):
-        files = []
-        all_dirs = os.listdir(dataset_path)
-        
-        if not isinstance(sub_folders,list):
-            sub_folders = [sub_folders]
-
-        for sub_folder in sub_folders:
-            file_dir = os.path.join(dataset_path,sub_folder)
-            all_file_in_dir = os.listdir(file_dir)
-            all_file_in_dir.sort(key=lambda x:int(x.split('.')[0]))
-            if len(all_file_in_dir) > 100:
-                all_file_in_dir = all_file_in_dir[:100]
-            for name in all_file_in_dir:
-                file_path = os.path.join(file_dir,name)
-                if cv.haveImageReader(file_path):
-                    files.append(file_path)
-        return files
-
-    def __len__(self):
-        """ Returns the length of the dataset """
-        return len(self._lq_files)//self._input_frames
-
-    
-    def getImage(self,filepath):
-        image = cv.cvtColor(cv.imread(filepath), cv.COLOR_BGR2RGB)
-        
-        input_image = np.transpose(image, (2, 0, 1))
-
-        return input_image
-
-
-    def __getitem__(self,index):
-        """ Returns frames by index in the NCHW layout """
-        if self._gt_files== None and self._lq_files == None:
-            return None,None
-        
-        image_list = []
-        gt_list = []
-        for id in range(self._input_frames):
-            image_index = index*self._input_frames + id
-            image_path = self._lq_files[image_index]
-            image = self.getImage(image_path)
-            image_list.append(image)
-        inputframes = np.array(image_list)
-        inputframes = np.expand_dims(inputframes,0)
-        inputframes = inputframes.astype(np.float32)/255.   # normalize
-
-        if self.need_gt == True:
-            for id in range(self._input_frames):
-                image_index = index*self._input_frames + id
-                image_path = self._gt_files[image_index]
-                image = self.getImage(image_path)
-                gt_list.append(image)
-            gtframes = np.array(gt_list)
-            gtframes = np.expand_dims(gtframes,0)
-            gtframes = gtframes.astype(np.float32)/255.
-            return inputframes,gtframes  # AccuracyawareQuantization, cross check
-        
-        return inputframes,None  # DefaultQuantization
-
-
-
-if __name__ == '__main__':
-    dataset_path = './basicvsr/dataset/reds/'
-    data_folder = ['000','011','015','020']   
-    import pdb;pdb.set_trace()
-    dataloader = FramesLoader(dataset_path,['LR','HR'],data_folder)
-    data,gt = dataloader.__getitem__(1)
-    print(f'input data shape:{data.shape}')
-    print(f'gt data shape:{gt.shape}')
-
-
-
-
-- 
2.25.1

