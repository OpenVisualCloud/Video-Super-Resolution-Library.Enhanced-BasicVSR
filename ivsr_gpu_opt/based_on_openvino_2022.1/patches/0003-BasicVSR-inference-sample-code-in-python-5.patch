From a3bf3ce384a6c15b6e600c85c4c3c4283cb96e40 Mon Sep 17 00:00:00 2001
From: KunLong9 <105702843+KunLong9@users.noreply.github.com>
Date: Mon, 19 Sep 2022 18:43:39 +0800
Subject: [PATCH 03/17] BasicVSR inference sample code in python (#5)

* fix gather_elements implementation error

* add INT8 support for gather_elements output

* fix the interpolate calc error between CPU/GPU

* change configuration for cross check tool

* optimize memory reuse mechanism

* custom op implementation

* POT patch for memory issue

* MO patch for custom op extension

* VSR Introduction

* rename and add in python infer samples

* BasicVSR inference sample code in python

Co-authored-by: Jiang, Renzhi <Renzhi.Jiang@intel.com>
---
 src/plugins/intel_cpu/thirdparty/onednn      |   1 +
 vsr_opt/samples/basicvsr_inference_sample.py | 204 +++++++++++++++++++
 vsr_opt/samples/patch_utils.py               | 115 +++++++++++
 3 files changed, 320 insertions(+)
 create mode 160000 src/plugins/intel_cpu/thirdparty/onednn
 create mode 100755 vsr_opt/samples/basicvsr_inference_sample.py
 create mode 100755 vsr_opt/samples/patch_utils.py

diff --git a/src/plugins/intel_cpu/thirdparty/onednn b/src/plugins/intel_cpu/thirdparty/onednn
new file mode 160000
index 0000000000..2a749c577f
--- /dev/null
+++ b/src/plugins/intel_cpu/thirdparty/onednn
@@ -0,0 +1 @@
+Subproject commit 2a749c577f8a841a396d4bd46eaf311b7e7dc089
diff --git a/vsr_opt/samples/basicvsr_inference_sample.py b/vsr_opt/samples/basicvsr_inference_sample.py
new file mode 100755
index 0000000000..3c944443f7
--- /dev/null
+++ b/vsr_opt/samples/basicvsr_inference_sample.py
@@ -0,0 +1,204 @@
+#!/usr/bin/env python
+# coding=utf-8
+import argparse
+import os
+import math
+
+import torch
+import numpy as np
+import cv2
+from math import ceil
+from torchvision.utils import make_grid
+from openvino.runtime import Core
+from patch_utils import restore_random_crop_from_patch,crop_images_to_random_patch
+import logging 
+logging.basicConfig(level = logging.INFO)
+logger = logging.getLogger(__name__)
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Model Inference')
+    parser.add_argument(
+        '--model',
+        '-m',
+        type=str,
+        help='Need. Openvino IR model for inference')
+    parser.add_argument(
+        '--input',
+        '-i',
+        type=str,
+        help='Need. Input data for inference, multi-input please separate by comma') 
+    parser.add_argument(
+        '--number_input_frames',
+        '-nif',
+        type=int,
+        help='Need. Number of input frames. Please keep in line with the model')
+    parser.add_argument(
+        '--device',
+        '-d',
+        default='CPU',
+        type=str,
+        help='Optional. Device to performance inference, default CPU') 
+    parser.add_argument(
+        '--save_path',
+        '-s',
+        default='./',
+        type=str,
+        help='Optional. Path to save inference predictions')
+    parser.add_argument(
+        '--patch_evaluation', 
+        '-pe',
+        action='store_true', 
+        help='Optional. Evaluate by image patches. Supported patch size is [360,640],'
+            ' make sure model input consistent with the supported patch size')
+    parser.add_argument(
+        '--extensions',
+        '-e',
+        type=str,
+        help='Optional. Required for CPU custom layers. '
+                'Absolute path to a shared library with the kernels implementations.')
+    parser.add_argument(
+        '--path_to_cldnn_config',
+        '-pcc',
+        type=str,
+        help='Optional. Required for GPU custom kernels. Absolute path to an .xml file with the '
+                    'kernels description.')
+    args = parser.parse_args()
+    return args
+
+
+def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):
+    """Convert torch Tensors into image numpy arrays.
+
+    Note that the image channel in input tensors should be RGB order. This
+    function will convert it to cv2 convention, i.e., (H x W x C) with BGR
+    order.
+
+    Args:
+        tensor (Tensor | list[Tensor]): Input tensors.
+        out_type (numpy type): Output types. If ``np.uint8``, transform outputs
+            to uint8 type with range [0, 255]; otherwise, float type with
+            range [0, 1]. Default: ``np.uint8``.
+        min_max (tuple): min and max values for clamp.
+
+    Returns:
+        (Tensor | list[Tensor]): 3D ndarray of shape (H x W x C) or 2D ndarray
+        of shape (H x W).
+    """
+    if not (torch.is_tensor(tensor) or
+            (isinstance(tensor, list)
+             and all(torch.is_tensor(t) for t in tensor))):
+        raise TypeError(
+            f'tensor or list of tensors expected, got {type(tensor)}')
+
+    if torch.is_tensor(tensor):
+        tensor = [tensor]
+    result = []
+    for _tensor in tensor:
+        _tensor = _tensor.squeeze(0).squeeze(0)
+        _tensor = _tensor.float().detach().cpu().clamp_(*min_max)
+        _tensor = (_tensor - min_max[0]) / (min_max[1] - min_max[0])
+        n_dim = _tensor.dim()
+        if n_dim == 4:
+            img_np = make_grid(
+                _tensor, nrow=int(math.sqrt(_tensor.size(0))),
+                normalize=False).numpy()
+            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
+        elif n_dim == 3:
+            img_np = _tensor.numpy()
+            img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))
+        elif n_dim == 2:
+            img_np = _tensor.numpy()
+        else:
+            raise ValueError('Only support 4D, 3D or 2D tensor. '
+                             f'But received with dimension: {n_dim}')
+        if out_type == np.uint8:
+            img_np = (img_np * 255.0).round()
+
+        img_np.astype(out_type)
+        result.append(img_np)
+    result = result[0] if len(result) == 1 else result
+    return result
+
+
+def read_input_data(path,nif):
+    input_path_list = path.split(',')
+    if len(input_path_list) < nif:
+        raise ValueError(f'BasicVSR inference needs {nif} input frames. But received {len(input_patch_list)}')
+    if len(input_path_list) > nif:
+        logger.warning('Too much input data. Only previous "{}" images will be used for inference'.format(nif))
+        input_path_list = input_path_list[:nif]
+    input_list = []
+    file_name_list = []
+    for input_path in input_path_list:
+        file_name_list.append(input_path.split('/')[-1])
+        image = cv2.cvtColor(cv2.imread(input_path), cv2.COLOR_BGR2RGB)
+        input_image = np.transpose(image, (2, 0, 1))
+        input_list.append(input_image)
+    input_array = np.array(input_list)
+    input_array = np.expand_dims(input_array,0)
+    input_array = input_array.astype(np.float32)/255.
+    return input_array,file_name_list
+
+
+def save_predictions(predictions,save_path,file_name_list):
+    for id in range(len(file_name_list)):
+        pred = predictions[:,id,:,:,:]
+        name = file_name_list[id]
+        output = torch.from_numpy(pred)
+        output_img = tensor2img(output)
+        cv2.imwrite(os.path.join(save_path,name),output_img)
+    logger.info('Succeed in saving inference results.')
+
+
+def basicvsr_inference(args):
+    ie = Core()
+    # CPU extension
+    if args.device == 'CPU' and args.extensions:
+        ie.add_extension(args.extensions) 
+        logger.info(f'CPU extensions is loaded {path_to_extension}')
+    # GPU extension
+    if args.device == 'GPU' and args.extensions and args.path_to_cldnn_config:
+        ie.add_extension(args.extensions)
+        ie.set_property('GPU', {'CONFIG_FILE': args.path_to_cldnn_config})
+        logger.info(f'GPU extensions is loaded {args.path_to_cldnn_config}')
+        
+    input_data,data_names = read_input_data(args.input,args.number_input_frames)
+    _,t,c,h,w = input_data.shape
+
+    ir_model=ie.read_model(model=args.model)
+    ir_compiled_model = ie.compile_model(model=ir_model,device_name=args.device)
+    ir_input_layer = next(iter(ir_compiled_model.inputs))
+    ir_output_layer = next(iter(ir_compiled_model.outputs))
+    _,input_t,input_c,input_h,input_w = ir_input_layer.shape
+    _,target_t,target_c,target_h,target_w = ir_output_layer.shape
+
+    logger.info(f'Start inference on {args.model}:')
+    if args.patch_evaluation:
+        if h < 360 or w < 640:
+            raise ValueError(f'Patch generation failure. Input data size [{h,w}] smaller than patch size [{360,640}]') 
+        if input_h != 360 or input_w != 640:
+            raise ValueError(f'Patch evaluation failure. Model input shape [{input_c,input_h}] is incompatible with patch size [360,640]') 
+        blocks = [ceil(h/360),ceil(w/640)]
+        input_patch_list = crop_images_to_random_patch(input_data,[360,640], blocks)
+        pred_patch_list = []
+        for idx in range(len(input_patch_list)):
+            input_patch = input_patch_list[idx]
+            pred_patch = ir_compiled_model(inputs=[input_patch])[ir_output_layer]
+            pred_patch_list.append(pred_patch)
+        scale = target_h // input_h
+        restore_size = [input_data.shape[-2]*scale,input_data.shape[-1]*scale]
+        pred_size = pred_patch_list[0].shape[-2:]
+        preds = restore_random_crop_from_patch(pred_patch_list, restore_size, pred_size, blocks)
+    else:
+        for (x,y) in zip(ir_input_layer.shape,input_data.shape):
+            if x != y :
+                raise Exception(f'Model input {ir_input_layer.shape} and given input data {input_data.shape} are incompatible')
+        preds = ir_compiled_model(inputs=[input_data])[ir_output_layer]
+    
+    save_predictions(preds,args.save_path,data_names)
+    logger.info(f'BasicVSR inference on {args.input} has finished.')
+
+
+if __name__=='__main__':
+    args = parse_args()
+    basicvsr_inference(args)
\ No newline at end of file
diff --git a/vsr_opt/samples/patch_utils.py b/vsr_opt/samples/patch_utils.py
new file mode 100755
index 0000000000..44c6a5d3df
--- /dev/null
+++ b/vsr_opt/samples/patch_utils.py
@@ -0,0 +1,115 @@
+import numpy as np
+
+def caulate_random_crop_coordinate(ori_size, crop_size, blocks):
+    [width, height] = ori_size   
+    [crop_width,crop_height] = crop_size
+    inter_width = (crop_width * blocks[0] - width) // (blocks[0] - 1)
+    last_fill_width = (crop_width * blocks[0] - width) % (blocks[0] - 1)
+    inter_height = (crop_height * blocks[1] - height) // (blocks[1] - 1)
+    last_fill_height = (crop_height * blocks[1] - height) % (blocks[1] - 1)
+    crop_coordinate_list = []
+    for i_ in range(blocks[0]):
+        for j_ in range(blocks[1]):
+            x1 = (crop_width - inter_width) * i_
+            y1 = (crop_height - inter_height) * j_
+            if i_ == blocks[0] - 1:
+                x1 = x1 - last_fill_width
+            if j_ == blocks[1] - 1:
+                y1 = y1 - last_fill_height
+            crop_coordinate_list.append((x1, y1, x1 + crop_width, y1 + crop_height))
+    return crop_coordinate_list,blocks
+
+
+def caulate_crop_coordinate_padding(ori_size: list, crop_size: int, overlapp_size: list):
+    [width, height] = ori_size 
+    [overlapp_w, overlapp_h] = overlapp_size
+    block_w = (width - crop_size) / (crop_size - overlapp_w)
+    if block_w % 1 != 0:
+        block_w = int(block_w) + 2
+    else:
+        block_w = int(block_w) + 1
+    block_h = (height - crop_size) / (crop_size - overlapp_h)
+    if block_h % 1 != 0:
+        block_h = int(block_h) + 2
+    else:
+        block_h = int(block_h) + 1
+
+    if (height) <= crop_size:
+        block_h = 1
+    crop_coordinate_list = []
+    for i_ in range(block_w):
+        for j_ in range(block_h):
+            x1 = (crop_size - overlapp_w) * i_
+            y1 = (crop_size - overlapp_h) * j_
+            if (height) <= crop_size:
+                crop_coordinate_list.append((x1, 0, x1 + crop_size, height))
+            else:
+                crop_coordinate_list.append((x1, y1, x1 + crop_size, y1 + crop_size))
+
+    return crop_coordinate_list, [block_w, block_h]
+
+
+def crop_images_to_random_patch(images,crop_size, blocks, overlapp_size=[0, 0], scale=1):
+    b,n,c,h,w = images.shape
+    ori_size = [h,w]
+    scale_size = list(int(a / scale) for a in ori_size)
+    if overlapp_size[0] == 0:
+        crop_coordinate_list, cal_blocks = caulate_random_crop_coordinate(scale_size, crop_size, blocks)
+    else:
+        crop_coordinate_list, cal_blocks = caulate_crop_coordinate_padding(scale_size, crop_size, overlapp_size)
+    crop_coordinate_list = [tuple(coo * scale for coo in corps) for corps in crop_coordinate_list]
+    def crop_img(image):
+        croped_list =[]
+        for idx, coordinate in enumerate(crop_coordinate_list):
+            cropped_img = image[:,:,:,coordinate[0]:coordinate[2],coordinate[1]:coordinate[3]]
+            _,_,_,c_h,c_w = cropped_img.shape
+            if c_h < crop_size[0] or c_w < crop_size[1]:
+                cropped = np.zeros((b,n,c,crop_size[0],crop_size[1]))
+                cropped[:,:,:,:c_h,:c_w] = cropped[:,:,:,:c_h,:c_w] + cropped_img
+                cropped_img = cropped
+            croped_list.append(cropped_img)
+
+        return croped_list
+    
+    crop_list = crop_img(images)
+    return crop_list
+
+
+def restore_random_crop_from_patch(patch_list, ori_size: list, crop_size: int, blocks, padding=0, scales=1):
+    if padding == 0:
+        crop_coordinate_list,cal_blocks = caulate_random_crop_coordinate(ori_size, crop_size, blocks)
+    else:
+        padding = [padding,padding] 
+        crop_coordinate_list,cal_blocks = caulate_crop_coordinate_padding(ori_size, crop_size, padding)
+    crop_coordinate_list = [tuple(coo * scales for coo in corps) for corps in crop_coordinate_list]
+
+    def restore_crop_img(img_restore_list):
+        b,n,c,_,_= img_restore_list[0].shape
+        restore_size_h = ori_size[0] * scales  
+        restore_size_w = ori_size[1] * scales  
+        output_img = np.zeros((b,n,c,restore_size_h, restore_size_w), dtype=np.float32)
+        add_nums_np = np.zeros((b,n,c,restore_size_h, restore_size_w), dtype=np.float32)
+        crop_img_size = crop_size * scales
+        for img_idx, img_crop in enumerate(img_restore_list):
+            teplate_add = np.ones((b,n,c,crop_img_size[0], crop_img_size[1]), dtype=np.float32)
+            coordinate = crop_coordinate_list[img_idx]
+            if coordinate[3] > restore_size_w:
+                img_crop = img_crop[:,:,:, :,:-(coordinate[3] - restore_size_w)]
+                teplate_add = teplate_add[:,:,:,:,:-(coordinate[3] - restore_size_w)]
+            if coordinate[2] > restore_size_h:
+                img_crop = img_crop[:,:,:,:-(coordinate[2] - restore_size_h),:]
+                teplate_add = teplate_add[:,:,:,:-(coordinate[2] - restore_size_h),:]
+
+            output_img[:,:,:,coordinate[0]:coordinate[2], coordinate[1]:coordinate[3]] += img_crop
+            add_nums_np[:,:,:,coordinate[0]:coordinate[2], coordinate[1]:coordinate[3]] += teplate_add
+
+        np.place(add_nums_np,add_nums_np==0,[1])
+        img_result = np.divide(output_img, add_nums_np)
+        return img_result
+
+    restored_results = restore_crop_img(patch_list)
+    return restored_results
+
+
+    
+    
\ No newline at end of file
-- 
2.25.1

